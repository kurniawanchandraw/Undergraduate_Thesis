{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GTWR + GNN for Spatiotemporal Socioeconomic Forecasting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Abstract.** We study a hybrid approach that couples Geographically and Temporally Weighted Regression (GTWR) with a learnable Graph Neural Network (GNN) prior to forecast regional socioeconomic indicators. This notebook documents the full pipeline in a paper-like narrative: we motivate the method, introduce the mathematical formulations, and accompany each section with executable code. The workflow follows the experimental flow previously prototyped in `Untitled-1-.ipynb`, but all supporting utilities are reproduced inline so the analysis is self-contained.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction\n",
        "Spatiotemporal panels often exhibit location-dependent dependencies that evolve across time. GTWR extends classical regression by assigning kernel weights that decay with spatial and temporal distance, yielding localized parameter estimates. Recent advances in graph representation learning suggest replacing fixed kernels with learnable adjacency structures. We therefore blend a GTWR-style local Weighted Least Squares (WLS) solver with a neural encoder that adapts the weight matrix from data, enabling flexible propagation of information across space-time while retaining interpretability.\n",
        "\n",
        "This notebook formalizes the approach, details the dataset, and benchmarks several training configurations along with out-of-sample (OOS) evaluation protocols.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Description and Panel Construction\n",
        "We work with the `Data BPS Laporan KP - Coded.xlsx` panel spanning yearly observations. Each record contains latitude (`lat`), longitude (`lon`), a time index (`Tahun`), the target response (`y`), and eight covariates (`X1`--`X8`). Our goal is to model 2019--2022 (train/validation/test split) and reserve 2023 for OOS evaluation. Before touching the data we discuss how balanced panels are formed.\n",
        "\n",
        "### Theory: Balanced Spatiotemporal Panel\n",
        "For a set of times $\\mathcal{T}$ and locations $\\mathcal{L}$ we require that each $(t, \\ell) \\in \\mathcal{T} \\times \\mathcal{L}$ appears exactly once. Raw administrative data may include missing locations per year; naively truncating to the first $N$ rows per year (as our earlier prototype did) silently misaligns coordinates. Instead we intersect the coordinate keys across all years to guarantee consistent ordering:\n",
        "\n",
        "1. Sort within each year by latitude and longitude.\n",
        "2. Compute the intersection of location keys across times.\n",
        "3. Align rows by the joint key so that feature and target arrays line up across years.\n",
        "\n",
        "This ensures that downstream kernels and GNN weights operate on coherent spatial indices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bffabe4c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device in use: cpu\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from IPython.display import display\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "warnings_enabled = False\n",
        "if not warnings_enabled:\n",
        "    import warnings\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device in use: {device}\")\n",
        "BASE_DIR = Path(\".\").resolve()\n",
        "DATA_PATH = BASE_DIR / \"Data BPS Laporan KP - Coded.xlsx\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b3c68ea",
      "metadata": {},
      "source": [
        "### Theory: Safe Invocation & Evaluation Metrics\n",
        "Reusable experimentation benefits from helper routines. `safe_call` filters keyword arguments based on function signatures, protecting against API drift. We also define tensor-to-NumPy conversion and standard regression metrics (RMSE, MAE, $R^2$) for consistent reporting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a7aaeb5d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import inspect\n",
        "\n",
        "def safe_call(fn, /, *args, **kwargs):\n",
        "    \"\"\"Call `fn` while discarding unexpected keyword arguments.\"\"\"\n",
        "    sig = inspect.signature(fn)\n",
        "    allowed = {k: v for k, v in kwargs.items() if k in sig.parameters}\n",
        "    return fn(*args, **allowed)\n",
        "\n",
        "def to_numpy(x):\n",
        "    if torch.is_tensor(x):\n",
        "        return x.detach().cpu().numpy()\n",
        "    return np.asarray(x)\n",
        "\n",
        "def regression_metrics(y_true, y_pred):\n",
        "    if y_true is None or y_pred is None or len(y_true) == 0:\n",
        "        return math.nan, math.nan, math.nan\n",
        "    return (\n",
        "        float(np.sqrt(mean_squared_error(y_true, y_pred))),\n",
        "        float(mean_absolute_error(y_true, y_pred)),\n",
        "        float(r2_score(y_true, y_pred)),\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb965c9d",
      "metadata": {},
      "source": [
        "### Theory: Data Loading and Panel Balancing\n",
        "We encapsulate data ingestion in two steps:\n",
        "\n",
        "- `load_panel_xlsx` reads the Excel file, selects relevant columns, and drops incomplete rows.\n",
        "- `build_panel_arrays` aligns coordinates across years. Let $\\mathbf{X}_t \\in \\mathbb{R}^{N \\times p}$, $\\mathbf{y}_t\\in\\mathbb{R}^N$, and $\\mathbf{c}_t \\in \\mathbb{R}^{N \\times 2}$ denote features, targets, and coordinates for year $t$. The function returns stacked arrays $\\mathbf{X} \\in \\mathbb{R}^{TN \\times p}$ and $\\mathbf{y} \\in \\mathbb{R}^{TN}$ alongside per-year blocks.\n",
        "\n",
        "Algorithm highlights:\n",
        "1. Compute location keys $(lat, lon)$ for every record.\n",
        "2. Build the intersection of keys across all requested years.\n",
        "3. Reindex each yearly block using the shared key ordering.\n",
        "4. Stack the balanced blocks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "97c4d5be",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_panel_xlsx(path_xlsx, lat_col, lon_col, time_col, target_col, feature_cols):\n",
        "    df = pd.read_excel(path_xlsx)\n",
        "    cols = [lat_col, lon_col, time_col, target_col] + list(feature_cols)\n",
        "    df = df[cols].dropna().copy()\n",
        "    return df\n",
        "\n",
        "def build_panel_arrays(df, time_col, target_col, feature_cols, lat_col, lon_col, times_sorted=None, atol=1e-6):\n",
        "    if times_sorted is None:\n",
        "        times_sorted = sorted(df[time_col].unique())\n",
        "\n",
        "    df_sorted = df.sort_values([time_col, lat_col, lon_col]).reset_index(drop=True)\n",
        "    df_sorted[\"coord_key\"] = list(zip(df_sorted[lat_col], df_sorted[lon_col]))\n",
        "\n",
        "    agg_spec = {target_col: \"mean\", lat_col: \"first\", lon_col: \"first\"}\n",
        "    agg_spec.update({feat: \"mean\" for feat in feature_cols})\n",
        "\n",
        "    blocks_by_time = {}\n",
        "    keys_per_time = {}\n",
        "    for t in times_sorted:\n",
        "        block = df_sorted[df_sorted[time_col] == t].copy()\n",
        "        block = block.groupby(\"coord_key\", sort=True).agg(agg_spec)\n",
        "        block.reset_index(drop=False, inplace=True)\n",
        "        if block[\"coord_key\"].duplicated().any():\n",
        "            raise ValueError(f\"Duplicate coordinate keys remain for year {t}; check data quality.\")\n",
        "        blocks_by_time[t] = block\n",
        "        keys_per_time[t] = set(block[\"coord_key\"])\n",
        "\n",
        "    shared_keys = set.intersection(*keys_per_time.values()) if keys_per_time else set()\n",
        "    if len(shared_keys) == 0:\n",
        "        raise ValueError(\"No common coordinates found across all time periods; cannot build balanced panel.\")\n",
        "\n",
        "    key_list = sorted(shared_keys, key=lambda xy: (round(xy[0] / atol) * atol, round(xy[1] / atol) * atol))\n",
        "\n",
        "    X_blocks, y_blocks, C_blocks = [], [], []\n",
        "    for t in times_sorted:\n",
        "        block = blocks_by_time[t].set_index(\"coord_key\").reindex(key_list)\n",
        "        block.reset_index(drop=False, inplace=True)\n",
        "        X_blocks.append(block[feature_cols].values.astype(np.float32))\n",
        "        y_blocks.append(block[target_col].values.astype(np.float32))\n",
        "        C_blocks.append(block[[lat_col, lon_col]].values.astype(np.float32))\n",
        "\n",
        "    X_all = np.vstack(X_blocks)\n",
        "    y_all = np.concatenate(y_blocks)\n",
        "    coords_all = np.vstack(C_blocks)\n",
        "\n",
        "    return {\n",
        "        \"X_all\": X_all,\n",
        "        \"y_all\": y_all,\n",
        "        \"coords_all\": coords_all,\n",
        "        \"coords_blocks\": C_blocks,\n",
        "        \"times\": np.array(times_sorted),\n",
        "        \"N_per_year\": X_blocks[0].shape[0],\n",
        "    }\n",
        "\n",
        "def year_rows(times_sorted, N_per_year, target_year):\n",
        "    offsets = []\n",
        "    cursor = 0\n",
        "    for t in times_sorted:\n",
        "        if t == target_year:\n",
        "            offsets.extend(range(cursor, cursor + N_per_year))\n",
        "        cursor += N_per_year\n",
        "    return np.array(offsets, dtype=int)\n",
        "\n",
        "def split_train_val_test(times_sorted, N_per_year, use_val=True):\n",
        "    test_year = times_sorted[-1]\n",
        "    val_year = times_sorted[-2] if use_val else None\n",
        "    train_years = times_sorted[:-2] if use_val else times_sorted[:-1]\n",
        "    train_rows = np.concatenate([year_rows(times_sorted, N_per_year, y) for y in train_years])\n",
        "    val_rows = year_rows(times_sorted, N_per_year, val_year) if use_val else np.array([], dtype=int)\n",
        "    test_rows = year_rows(times_sorted, N_per_year, test_year)\n",
        "    return {\n",
        "        \"train_rows\": train_rows,\n",
        "        \"val_rows\": val_rows,\n",
        "        \"test_rows\": test_rows,\n",
        "        \"train_years\": train_years,\n",
        "        \"val_year\": val_year,\n",
        "        \"test_year\": test_year,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59cc5dbe",
      "metadata": {},
      "source": [
        "## 3. Spatiotemporal Kernel Prior\n",
        "GTWR uses distance-based kernels to assign weights. We adopt a separable Gaussian kernel across space and time:\n",
        "\n",
        "$$K_{ij} = \\exp\\left(-\\tfrac{1}{2}\\left(\\frac{d^{\\text{geo}}_{ij}}{h_S}\\right)^2\\right) \\cdot \\exp\\left(-\\tfrac{1}{2}\\left(\\frac{|t_i - t_j|}{h_T}\\right)^2\\right),$$\n",
        "\n",
        "where $h_S$ and $h_T$ are bandwidths. We estimate $h_S$ from the median pairwise great-circle distance (haversine) and $h_T$ from temporal gaps. To encourage sparsity we keep the top-$k$ neighbors per row and renormalize to obtain a row-stochastic prior matrix $A_{prior}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "45201d91",
      "metadata": {},
      "outputs": [],
      "source": [
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371.0\n",
        "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
        "    dlat, dlon = lat2 - lat1, lon2 - lon1\n",
        "    a = np.sin(dlat / 2) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) ** 2\n",
        "    return 2 * R * np.arcsin(np.sqrt(a))\n",
        "\n",
        "def _pairwise_block(coords_a, coords_b):\n",
        "    n_a, n_b = len(coords_a), len(coords_b)\n",
        "    out = np.zeros((n_a, n_b), dtype=np.float64)\n",
        "    for i in range(n_a):\n",
        "        lat1, lon1 = coords_a[i]\n",
        "        for j in range(n_b):\n",
        "            lat2, lon2 = coords_b[j]\n",
        "            out[i, j] = haversine(lat1, lon1, lat2, lon2)\n",
        "    return out\n",
        "\n",
        "def _sparsify_knn(W, k_neighbors, self_weight):\n",
        "    n = W.shape[0]\n",
        "    if n <= 1:\n",
        "        return np.eye(n) * self_weight\n",
        "    k_eff = max(1, min(k_neighbors, n - 1))\n",
        "    W_sparse = np.zeros_like(W)\n",
        "    for i in range(n):\n",
        "        row = W[i].copy()\n",
        "        row[i] = -np.inf\n",
        "        idx = np.argpartition(-row, kth=k_eff - 1)[:k_eff]\n",
        "        W_sparse[i, idx] = W[i, idx]\n",
        "    np.fill_diagonal(W_sparse, self_weight)\n",
        "    row_sum = W_sparse.sum(axis=1, keepdims=True)\n",
        "    return W_sparse / np.where(row_sum > 0, row_sum, 1.0)\n",
        "\n",
        "def build_spatiotemporal_kernel(coords_blocks, times, tau_s=1.0, tau_t=1.0, k_neighbors=8, prior_self_weight=1.0, verbose=False):\n",
        "    times = np.array(times, dtype=float)\n",
        "    bandwidth_samples = []\n",
        "    for coords in coords_blocks:\n",
        "        if len(coords) > 1:\n",
        "            D = _pairwise_block(coords, coords)\n",
        "            bandwidth_samples.append(D[D > 0])\n",
        "    if bandwidth_samples:\n",
        "        hS = np.median(np.concatenate(bandwidth_samples))\n",
        "    else:\n",
        "        hS = 1.0\n",
        "    hS = max(hS / max(tau_s, 1e-6), 1e-6)\n",
        "\n",
        "    Dt = np.abs(times[:, None] - times[None, :])\n",
        "    hT = np.median(Dt[Dt > 0]) if (Dt > 0).any() else 1.0\n",
        "    hT = max(hT / max(tau_t, 1e-6), 1e-6)\n",
        "\n",
        "    N_per_year = coords_blocks[0].shape[0]\n",
        "    total = N_per_year * len(coords_blocks)\n",
        "    W_full = np.zeros((total, total), dtype=np.float64)\n",
        "    row_offset = 0\n",
        "    for i, (Ci, ti) in enumerate(zip(coords_blocks, times)):\n",
        "        col_offset = 0\n",
        "        for j, (Cj, tj) in enumerate(zip(coords_blocks, times)):\n",
        "            Dij = _pairwise_block(Ci, Cj)\n",
        "            Ks = np.exp(-0.5 * (Dij / hS) ** 2)\n",
        "            Kt = math.exp(-0.5 * ((abs(ti - tj) / hT) ** 2))\n",
        "            W_full[row_offset:row_offset + N_per_year, col_offset:col_offset + N_per_year] = Ks * Kt\n",
        "            col_offset += N_per_year\n",
        "        row_offset += N_per_year\n",
        "\n",
        "    W_sparse = _sparsify_knn(W_full, k_neighbors, prior_self_weight)\n",
        "    if verbose:\n",
        "        sparsity = np.mean(W_sparse == 0)\n",
        "        print(f\"Kernel constructed with sparsity {sparsity:.3f}\")\n",
        "    return W_sparse\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "930c45a8",
      "metadata": {},
      "source": [
        "## 4. Local Weighted Least Squares (WLS)\n",
        "Given a weight matrix $W \\in \\mathbb{R}^{N \\times N}$ and covariates $X$, GTWR solves\n",
        "$$\\hat{\\beta}_i = \\arg\\min_\\beta \\sum_j W_{ij} (y_j - x_j^\\top \\beta)^2 + \\lambda \\|\\beta\\|_2^2,$$\n",
        "which yields local predictions $\\hat{y}_i = x_i^\\top \\hat{\\beta}_i$. We implement ridge and iteratively reweighted Huber variants, exposed via `solve_local_wls`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "45270c0e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def local_wls_ridge(X, y, W, ridge=5.0, return_betas=True):\n",
        "    N, p = X.shape\n",
        "    device = X.device\n",
        "    I = ridge * torch.eye(p, device=device)\n",
        "    y_hat = torch.zeros(N, device=device)\n",
        "    betas = torch.zeros(N, p, device=device) if return_betas else None\n",
        "    for i in range(N):\n",
        "        w = W[i]\n",
        "        ws = torch.sqrt(w + 1e-12)\n",
        "        Xw = X * ws.unsqueeze(1)\n",
        "        yw = y * ws\n",
        "        XtWX = Xw.t() @ Xw + I\n",
        "        XtWy = Xw.t() @ yw\n",
        "        try:\n",
        "            beta = torch.linalg.solve(XtWX, XtWy)\n",
        "        except RuntimeError:\n",
        "            beta = torch.linalg.lstsq(XtWX, XtWy.unsqueeze(1)).solution.squeeze()\n",
        "        y_hat[i] = X[i] @ beta\n",
        "        if return_betas:\n",
        "            betas[i] = beta\n",
        "    return (y_hat, betas) if return_betas else y_hat\n",
        "\n",
        "def local_wls_huber(X, y, W, ridge=5.0, delta=1.0, iters=3, return_betas=True):\n",
        "    N, p = X.shape\n",
        "    device = X.device\n",
        "    I = ridge * torch.eye(p, device=device)\n",
        "    y_hat = torch.zeros(N, device=device)\n",
        "    betas = torch.zeros(N, p, device=device) if return_betas else None\n",
        "    for i in range(N):\n",
        "        w = W[i].clone()\n",
        "        beta = None\n",
        "        for _ in range(iters):\n",
        "            ws = torch.sqrt(w + 1e-12)\n",
        "            Xw = X * ws.unsqueeze(1)\n",
        "            yw = y * ws\n",
        "            XtWX = Xw.t() @ Xw + I\n",
        "            XtWy = Xw.t() @ yw\n",
        "            try:\n",
        "                beta = torch.linalg.solve(XtWX, XtWy)\n",
        "            except RuntimeError:\n",
        "                beta = torch.linalg.lstsq(XtWX, XtWy.unsqueeze(1)).solution.squeeze()\n",
        "            residual = y - X @ beta\n",
        "            abs_res = torch.abs(residual) + 1e-12\n",
        "            w = W[i] * torch.where(abs_res <= delta, torch.ones_like(abs_res), (delta / abs_res))\n",
        "        y_hat[i] = X[i] @ beta\n",
        "        if return_betas:\n",
        "            betas[i] = beta\n",
        "    return (y_hat, betas) if return_betas else y_hat\n",
        "\n",
        "def solve_local_wls(X, y, W, kind=\"ridge\", ridge=5.0, huber_delta=1.0, huber_iters=3, return_betas=True):\n",
        "    if kind == \"ridge\":\n",
        "        return local_wls_ridge(X, y, W, ridge=ridge, return_betas=return_betas)\n",
        "    if kind == \"huber\":\n",
        "        return local_wls_huber(X, y, W, ridge=ridge, delta=huber_delta, iters=huber_iters, return_betas=return_betas)\n",
        "    raise ValueError(f\"Unknown WLS kind: {kind}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f891f016",
      "metadata": {},
      "source": [
        "## 5. GNN Weight Generator\n",
        "We parameterize the adaptive weight matrix via `MathematicallyCorrectGNNWeightNet`, a lightweight encoder producing embeddings $H = f_\\theta(X)$. Cosine similarity combined with a learnable temperature $\\tau$ yields logits, which are blended with the log prior using a mixing coefficient $\\alpha$ (constrained to $(0,1)$ via a sigmoid). A row-wise softmax enforces stochasticity.\n",
        "\n",
        "This design is inspired by attention mechanisms yet keeps the prior in log-space to avoid degenerate weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "224b8749",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MathematicallyCorrectGNNWeightNet(nn.Module):\n",
        "    def __init__(self, d_in, spa_hid=32, emb=16, tau=1.2, alpha_init=0.30):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(d_in, spa_hid),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(spa_hid, emb),\n",
        "        )\n",
        "        self.log_tau = nn.Parameter(torch.log(torch.tensor(float(tau))))\n",
        "        self.raw_alpha = nn.Parameter(torch.tensor(math.log(alpha_init / (1.0 - alpha_init))))\n",
        "        nn.init.kaiming_uniform_(self.encoder[0].weight, a=math.sqrt(5))\n",
        "        nn.init.zeros_(self.encoder[0].bias)\n",
        "        nn.init.xavier_uniform_(self.encoder[2].weight)\n",
        "        nn.init.zeros_(self.encoder[2].bias)\n",
        "\n",
        "    @property\n",
        "    def tau(self):\n",
        "        return torch.exp(self.log_tau).clamp(min=0.1, max=10.0)\n",
        "\n",
        "    @property\n",
        "    def alpha(self):\n",
        "        return torch.sigmoid(self.raw_alpha)\n",
        "\n",
        "    def forward(self, X, A_prior):\n",
        "        H = self.encoder(X)\n",
        "        H_norm = F.normalize(H, p=2, dim=1)\n",
        "        S = H_norm @ H_norm.t()\n",
        "        logits = S / self.tau\n",
        "        log_prior = torch.log(A_prior + 1e-12)\n",
        "        log_blend = self.alpha * log_prior + (1.0 - self.alpha) * logits\n",
        "        W = F.softmax(log_blend, dim=1)\n",
        "        return W, H\n",
        "\n",
        "def _row_normalize(W, eps=1e-12):\n",
        "    row_sum = W.sum(dim=1, keepdim=True)\n",
        "    return W / (row_sum + eps)\n",
        "\n",
        "def topk_rows(W, k):\n",
        "    if k is None:\n",
        "        return W\n",
        "    n = W.shape[0]\n",
        "    k_eff = max(1, min(k, n))\n",
        "    values, indices = torch.topk(W, k_eff, dim=1)\n",
        "    mask = torch.zeros_like(W)\n",
        "    mask.scatter_(1, indices, 1.0)\n",
        "    W_pruned = W * mask\n",
        "    return _row_normalize(W_pruned)\n",
        "\n",
        "def symmetrize_rows(W):\n",
        "    W_sym = 0.5 * (W + W.t())\n",
        "    W_sym = torch.clamp(W_sym, min=0.0)\n",
        "    return _row_normalize(W_sym)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b31c072d",
      "metadata": {},
      "source": [
        "## 6. Training Objective\n",
        "We minimize a composite loss over training rows:\n",
        "\n",
        "1. **Supervised MSE** between predictions and labels on train indices.\n",
        "2. **Entropy regularizer** encouraging diffuse weights (controlled by `ent_w`).\n",
        "3. **Spatial smoothness** on per-year regression coefficients: for year $t$,\n",
        "   $$\\mathcal{L}_{smooth} = \\sum_{i,j} A^{(t)}_{ij} \\|\\beta^{(t)}_i - \\beta^{(t)}_j\\|^2,$$\n",
        "   scaled by `smooth_w`.\n",
        "\n",
        "Early stopping monitors validation RMSE (or training RMSE when validation is absent). We expose `graph_topk` and `graph_symmetrize` to post-process the learned adjacency.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "fe3e0d5b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(\n",
        "    model,\n",
        "    X_all,\n",
        "    y_all,\n",
        "    A_prior,\n",
        "    train_rows,\n",
        "    val_rows=None,\n",
        "    test_rows=None,\n",
        "    epochs=200,\n",
        "    lr=1e-3,\n",
        "    ridge_lambda=5.0,\n",
        "    ent_w=5e-3,\n",
        "    smooth_w=1e-3,\n",
        "    N_per_year=None,\n",
        "    times=None,\n",
        "    print_every=25,\n",
        "    early_stop=True,\n",
        "    es_patience=80,\n",
        "    wls_kind=\"ridge\",\n",
        "    huber_delta=1.0,\n",
        "    huber_iters=3,\n",
        "    graph_topk=None,\n",
        "    graph_symmetrize=False,\n",
        "    device=None,\n",
        "):\n",
        "    device = device or next(model.parameters()).device\n",
        "    X_t = torch.tensor(X_all, dtype=torch.float32, device=device)\n",
        "    y_t = torch.tensor(y_all, dtype=torch.float32, device=device)\n",
        "    A_t = torch.tensor(A_prior, dtype=torch.float32, device=device)\n",
        "\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    best_val, best_state, patience = float(\"inf\"), None, 0\n",
        "    history = []\n",
        "    T = len(times) if times is not None else None\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        opt.zero_grad()\n",
        "        W, _ = model(X_t, A_t)\n",
        "        if graph_topk is not None:\n",
        "            W = topk_rows(W, graph_topk)\n",
        "            if graph_symmetrize:\n",
        "                W = symmetrize_rows(W)\n",
        "\n",
        "        y_hat, betas = solve_local_wls(\n",
        "            X_t,\n",
        "            y_t,\n",
        "            W,\n",
        "            kind=wls_kind,\n",
        "            ridge=ridge_lambda,\n",
        "            huber_delta=huber_delta,\n",
        "            huber_iters=huber_iters,\n",
        "            return_betas=True,\n",
        "        )\n",
        "\n",
        "        sup_loss = F.mse_loss(y_hat[train_rows], y_t[train_rows])\n",
        "        Wn = _row_normalize(W)\n",
        "        ent = -torch.sum(Wn * torch.log(Wn + 1e-12), dim=1).mean()\n",
        "        ent_loss = -ent_w * ent\n",
        "\n",
        "        if T is not None and N_per_year is not None:\n",
        "            smooth = 0.0\n",
        "            beta_mat = betas.reshape(T, N_per_year, -1)\n",
        "            for t_idx in range(T):\n",
        "                slice_start = t_idx * N_per_year\n",
        "                slice_end = (t_idx + 1) * N_per_year\n",
        "                W_spatial = A_t[slice_start:slice_end, slice_start:slice_end]\n",
        "                bt = beta_mat[t_idx]\n",
        "                diff = bt.unsqueeze(1) - bt.unsqueeze(0)\n",
        "                smooth = smooth + torch.sum(W_spatial.unsqueeze(-1) * diff.pow(2))\n",
        "            spatial_loss = smooth_w * smooth\n",
        "        else:\n",
        "            spatial_loss = 0.0\n",
        "\n",
        "        total_loss = sup_loss + ent_loss + spatial_loss\n",
        "        total_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        opt.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            W_eval, _ = model(X_t, A_t)\n",
        "            if graph_topk is not None:\n",
        "                W_eval = topk_rows(W_eval, graph_topk)\n",
        "                if graph_symmetrize:\n",
        "                    W_eval = symmetrize_rows(W_eval)\n",
        "            y_eval = solve_local_wls(\n",
        "                X_t,\n",
        "                y_t,\n",
        "                W_eval,\n",
        "                kind=wls_kind,\n",
        "                ridge=ridge_lambda,\n",
        "                return_betas=False,\n",
        "            )\n",
        "            y_eval_np = y_eval.detach().cpu().numpy()\n",
        "            rmse_tr = float(np.sqrt(mean_squared_error(y_all[train_rows], y_eval_np[train_rows])))\n",
        "            if val_rows is not None and len(val_rows) > 0:\n",
        "                rmse_va = float(np.sqrt(mean_squared_error(y_all[val_rows], y_eval_np[val_rows])))\n",
        "            else:\n",
        "                rmse_va = float(\"inf\")\n",
        "            if test_rows is not None and len(test_rows) > 0:\n",
        "                rmse_te = float(np.sqrt(mean_squared_error(y_all[test_rows], y_eval_np[test_rows])))\n",
        "            else:\n",
        "                rmse_te = math.nan\n",
        "\n",
        "        history.append(\n",
        "            {\n",
        "                \"epoch\": ep,\n",
        "                \"loss\": float(total_loss.item()),\n",
        "                \"rmse_tr\": rmse_tr,\n",
        "                \"rmse_va\": rmse_va,\n",
        "                \"rmse_te\": rmse_te,\n",
        "                \"alpha\": float(model.alpha.item()),\n",
        "                \"tau\": float(model.tau.item()),\n",
        "            }\n",
        "        )\n",
        "        if (ep % print_every == 0) or (ep == 1):\n",
        "            print(\n",
        "                f\"Epoch {ep:03d} | Loss {total_loss.item():.4f} | RMSE train {rmse_tr:.3f} | val {rmse_va:.3f} | alpha {model.alpha.item():.3f} | tau {model.tau.item():.3f}\"\n",
        "            )\n",
        "\n",
        "        score = rmse_va if val_rows is not None and len(val_rows) > 0 else rmse_tr\n",
        "        if score < best_val - 1e-6:\n",
        "            best_val = score\n",
        "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "            patience = 0\n",
        "        else:\n",
        "            patience += 1\n",
        "            if early_stop and patience >= es_patience:\n",
        "                print(f\"Early stopping at epoch {ep}\")\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        W_final, _ = model(X_t, A_t)\n",
        "        if graph_topk is not None:\n",
        "            W_final = topk_rows(W_final, graph_topk)\n",
        "            if graph_symmetrize:\n",
        "                W_final = symmetrize_rows(W_final)\n",
        "        y_final, betas_final = solve_local_wls(\n",
        "            X_t,\n",
        "            y_t,\n",
        "            W_final,\n",
        "            kind=wls_kind,\n",
        "            ridge=ridge_lambda,\n",
        "            return_betas=True,\n",
        "        )\n",
        "\n",
        "    return {\n",
        "        \"model\": model,\n",
        "        \"W\": W_final,\n",
        "        \"y_hat\": y_final,\n",
        "        \"betas\": betas_final,\n",
        "        \"history\": history,\n",
        "        \"best_state\": best_state,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ae24a8b",
      "metadata": {},
      "source": [
        "### Theory: Transductive Fine-Tuning with Future Nodes\n",
        "To adapt the trained model when new-year nodes become available (without leaking their labels into the loss), we fine-tune on the expanded panel. The graph prior is rebuilt on 2019--2023, but the loss masks future rows while still allowing them to receive messages during forward passes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8de45395",
      "metadata": {},
      "outputs": [],
      "source": [
        "def finetune_transductive_with_future(\n",
        "    model,\n",
        "    X_all_full,\n",
        "    y_all_full,\n",
        "    coords_blocks_full,\n",
        "    times_full,\n",
        "    train_rows,\n",
        "    val_rows,\n",
        "    future_rows,\n",
        "    lr=1e-4,\n",
        "    epochs=150,\n",
        "    ridge_lambda=5.0,\n",
        "    ent_w=5e-3,\n",
        "    smooth_w=1e-3,\n",
        "    knn_k=8,\n",
        "    tau_s=1.0,\n",
        "    tau_t=1.0,\n",
        "    prior_self_weight=1.0,\n",
        "    N_per_year=None,\n",
        "    print_every=25,\n",
        "    patience=40,\n",
        "    wls_kind=\"ridge\",\n",
        "    huber_delta=1.0,\n",
        "    huber_iters=3,\n",
        "    graph_topk=None,\n",
        "    graph_symmetrize=False,\n",
        "    device=None,\n",
        "):\n",
        "    device = device or next(model.parameters()).device\n",
        "    A_prior_np = build_spatiotemporal_kernel(\n",
        "        coords_blocks_full,\n",
        "        times_full,\n",
        "        tau_s=tau_s,\n",
        "        tau_t=tau_t,\n",
        "        k_neighbors=knn_k,\n",
        "        prior_self_weight=prior_self_weight,\n",
        "        verbose=False,\n",
        "    )\n",
        "    A = torch.tensor(A_prior_np, dtype=torch.float32, device=device)\n",
        "    X = torch.tensor(X_all_full, dtype=torch.float32, device=device)\n",
        "    y = torch.tensor(y_all_full, dtype=torch.float32, device=device)\n",
        "\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    best_val, best_state, patience_ctr = float(\"inf\"), None, 0\n",
        "    T = len(times_full)\n",
        "\n",
        "    mask_train = torch.zeros(X.shape[0], dtype=torch.bool, device=device)\n",
        "    mask_train[train_rows] = True\n",
        "    mask_val = torch.zeros_like(mask_train)\n",
        "    mask_val[val_rows] = True\n",
        "    mask_future = torch.zeros_like(mask_train)\n",
        "    mask_future[future_rows] = True\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        opt.zero_grad()\n",
        "        W, _ = model(X, A)\n",
        "        if graph_topk is not None:\n",
        "            W = topk_rows(W, graph_topk)\n",
        "            if graph_symmetrize:\n",
        "                W = symmetrize_rows(W)\n",
        "\n",
        "        y_hat, betas = solve_local_wls(\n",
        "            X,\n",
        "            y,\n",
        "            W,\n",
        "            kind=wls_kind,\n",
        "            ridge=ridge_lambda,\n",
        "            huber_delta=huber_delta,\n",
        "            huber_iters=huber_iters,\n",
        "            return_betas=True,\n",
        "        )\n",
        "        sup_loss = F.mse_loss(y_hat[mask_train], y[mask_train])\n",
        "        Wn = _row_normalize(W)\n",
        "        ent = -torch.sum(Wn * torch.log(Wn + 1e-12), dim=1).mean()\n",
        "        ent_loss = -ent_w * ent\n",
        "\n",
        "        smooth = 0.0\n",
        "        if N_per_year is not None and T is not None:\n",
        "            beta_mat = betas.reshape(T, N_per_year, -1)\n",
        "            for t_idx in range(T):\n",
        "                start = t_idx * N_per_year\n",
        "                end = (t_idx + 1) * N_per_year\n",
        "                W_block = A[start:end, start:end]\n",
        "                bt = beta_mat[t_idx]\n",
        "                diff = bt.unsqueeze(1) - bt.unsqueeze(0)\n",
        "                smooth = smooth + torch.sum(W_block.unsqueeze(-1) * diff.pow(2))\n",
        "        spatial_loss = smooth_w * smooth\n",
        "\n",
        "        total_loss = sup_loss + ent_loss + spatial_loss\n",
        "        total_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        opt.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            W_eval, _ = model(X, A)\n",
        "            if graph_topk is not None:\n",
        "                W_eval = topk_rows(W_eval, graph_topk)\n",
        "                if graph_symmetrize:\n",
        "                    W_eval = symmetrize_rows(W_eval)\n",
        "            y_eval = solve_local_wls(\n",
        "                X,\n",
        "                y,\n",
        "                W_eval,\n",
        "                kind=wls_kind,\n",
        "                ridge=ridge_lambda,\n",
        "                return_betas=False,\n",
        "            )\n",
        "            y_eval_np = y_eval.detach().cpu().numpy()\n",
        "            rmse_tr = float(np.sqrt(mean_squared_error(y_all_full[train_rows], y_eval_np[train_rows])))\n",
        "            rmse_va = float(np.sqrt(mean_squared_error(y_all_full[val_rows], y_eval_np[val_rows])))\n",
        "            rmse_fu = float(np.sqrt(mean_squared_error(y_all_full[future_rows], y_eval_np[future_rows])))\n",
        "\n",
        "        if (ep % print_every == 0) or (ep == 1):\n",
        "            print(\n",
        "                f\"Fine-tune Ep {ep:03d} | Loss {total_loss.item():.4f} | RMSE train {rmse_tr:.3f} | val {rmse_va:.3f} | future {rmse_fu:.3f}\"\n",
        "            )\n",
        "\n",
        "        if rmse_va < best_val - 1e-6:\n",
        "            best_val = rmse_va\n",
        "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "            patience_ctr = 0\n",
        "        else:\n",
        "            patience_ctr += 1\n",
        "            if patience_ctr >= patience:\n",
        "                print(f\"Fine-tune early stop at epoch {ep}\")\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        W_fin, _ = model(X, A)\n",
        "        if graph_topk is not None:\n",
        "            W_fin = topk_rows(W_fin, graph_topk)\n",
        "            if graph_symmetrize:\n",
        "                W_fin = symmetrize_rows(W_fin)\n",
        "        y_fin, betas_fin = solve_local_wls(\n",
        "            X,\n",
        "            y,\n",
        "            W_fin,\n",
        "            kind=wls_kind,\n",
        "            ridge=ridge_lambda,\n",
        "            return_betas=True,\n",
        "        )\n",
        "\n",
        "    return {\n",
        "        \"model\": model,\n",
        "        \"W\": W_fin,\n",
        "        \"y_hat\": y_fin,\n",
        "        \"betas\": betas_fin,\n",
        "        \"best_state\": best_state,\n",
        "        \"A_prior\": A,\n",
        "        \"X\": X,\n",
        "        \"y\": y,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f29234a",
      "metadata": {},
      "source": [
        "## 7. Inference Strategies\n",
        "We evaluate three deployment settings:\n",
        "\n",
        "1. **OOS-Transductive**: freeze old rows, allow new nodes to attend to old ones only.\n",
        "2. **OOS-Fullgraph**: augment the graph with new nodes and run a single forward pass.\n",
        "3. **Prior-only**: use the kernel prior without the GNN (sanity baseline).\n",
        "\n",
        "All strategies ultimately solve a local WLS system mixing old targets with new covariates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1db8ae54",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _estimate_bandwidths(coords_blocks_old, times_old, tau_s=1.0, tau_t=1.0):\n",
        "    samples = []\n",
        "    for C in coords_blocks_old:\n",
        "        n = len(C)\n",
        "        for i in range(n):\n",
        "            for j in range(i + 1, n):\n",
        "                samples.append(haversine(*C[i], *C[j]))\n",
        "    hS = np.median(samples) if samples else 100.0\n",
        "    hS = max(hS / max(tau_s, 1e-6), 1e-6)\n",
        "    t_unique = np.sort(np.unique(times_old))\n",
        "    Dt = np.abs(t_unique[:, None] - t_unique[None, :])\n",
        "    hT = np.median(Dt[Dt > 0]) if (Dt > 0).any() else 1.0\n",
        "    hT = max(hT / max(tau_t, 1e-6), 1e-6)\n",
        "    return hS, hT\n",
        "\n",
        "def predict_new_fullgraph(\n",
        "    model,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    coords_train,\n",
        "    times_train,\n",
        "    new_df,\n",
        "    feature_cols,\n",
        "    time_col,\n",
        "    lat_col,\n",
        "    lon_col,\n",
        "    tau_s=1.0,\n",
        "    tau_t=1.0,\n",
        "    knn_k=8,\n",
        "    prior_self_weight=1.0,\n",
        "    wls_kind=\"ridge\",\n",
        "    ridge_lambda=5.0,\n",
        "    huber_delta=1.0,\n",
        "    huber_iters=3,\n",
        "    graph_topk=None,\n",
        "    graph_symmetrize=False,\n",
        "    device=None,\n",
        "):\n",
        "    device = device or next(model.parameters()).device\n",
        "    X_new = new_df[feature_cols].values.astype(np.float32)\n",
        "    coords_new = new_df[[lat_col, lon_col]].values.astype(np.float32)\n",
        "    times_new = new_df[time_col].values.astype(float)\n",
        "    X_comb = np.vstack([X_train, X_new]).astype(np.float32)\n",
        "    coords_comb = np.vstack([coords_train, coords_new]).astype(np.float32)\n",
        "    times_comb = np.concatenate([times_train, times_new]).astype(float)\n",
        "\n",
        "    unique_times = np.sort(np.unique(times_comb))\n",
        "    coords_blocks = [coords_comb[times_comb == t] for t in unique_times]\n",
        "    A_prior_ext = build_spatiotemporal_kernel(\n",
        "        coords_blocks,\n",
        "        unique_times,\n",
        "        tau_s=tau_s,\n",
        "        tau_t=tau_t,\n",
        "        k_neighbors=knn_k,\n",
        "        prior_self_weight=prior_self_weight,\n",
        "        verbose=False,\n",
        "    )\n",
        "    X_comb_t = torch.tensor(X_comb, dtype=torch.float32, device=device)\n",
        "    A_prior_ext_t = torch.tensor(A_prior_ext, dtype=torch.float32, device=device)\n",
        "    n_old = len(X_train)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        W_learned, _ = model(X_comb_t, A_prior_ext_t)\n",
        "        if graph_topk is not None:\n",
        "            W_learned = topk_rows(W_learned, graph_topk)\n",
        "            if graph_symmetrize:\n",
        "                W_learned = symmetrize_rows(W_learned)\n",
        "        y_stub = torch.tensor(\n",
        "            np.concatenate([y_train, np.zeros(len(X_new), dtype=np.float32)]),\n",
        "            dtype=torch.float32,\n",
        "            device=device,\n",
        "        )\n",
        "        y_hat = solve_local_wls(\n",
        "            X_comb_t,\n",
        "            y_stub,\n",
        "            W_learned,\n",
        "            kind=wls_kind,\n",
        "            ridge=ridge_lambda,\n",
        "            huber_delta=huber_delta,\n",
        "            huber_iters=huber_iters,\n",
        "            return_betas=False,\n",
        "        )\n",
        "    return y_hat[n_old:].cpu().numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ef11686e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_new_oos_transductive(\n",
        "    model,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    coords_train,\n",
        "    times_train,\n",
        "    new_df,\n",
        "    feature_cols,\n",
        "    time_col,\n",
        "    lat_col,\n",
        "    lon_col,\n",
        "    tau_s=1.0,\n",
        "    tau_t=1.0,\n",
        "    knn_k=8,\n",
        "    prior_self_weight=1.0,\n",
        "    lambda_blend=0.8,\n",
        "    wls_kind=\"ridge\",\n",
        "    ridge_lambda=5.0,\n",
        "    huber_delta=1.0,\n",
        "    huber_iters=3,\n",
        "    graph_topk=None,\n",
        "    graph_symmetrize=False,\n",
        "    device=None,\n",
        "    cross_topk=None,\n",
        "    new_self_weight=0.0,\n",
        "):\n",
        "    device = device or next(model.parameters()).device\n",
        "    X_new = new_df[feature_cols].values.astype(np.float32)\n",
        "    coords_new = new_df[[lat_col, lon_col]].values.astype(np.float32)\n",
        "    times_new = new_df[time_col].values.astype(float)\n",
        "    n_old, n_new = len(X_train), len(X_new)\n",
        "\n",
        "    unique_times_old = np.sort(np.unique(times_train))\n",
        "    coords_blocks_old = [coords_train[times_train == t] for t in unique_times_old]\n",
        "    A_prior_old_np = build_spatiotemporal_kernel(\n",
        "        coords_blocks_old,\n",
        "        unique_times_old,\n",
        "        tau_s=tau_s,\n",
        "        tau_t=tau_t,\n",
        "        knn_k=knn_k,\n",
        "        prior_self_weight=prior_self_weight,\n",
        "        verbose=False,\n",
        "    )\n",
        "    A_prior_old = torch.tensor(A_prior_old_np, dtype=torch.float32, device=device)\n",
        "    X_old_t = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        W_old, H_old = model(X_old_t, A_prior_old)\n",
        "        if graph_topk is not None:\n",
        "            W_old = topk_rows(W_old, graph_topk)\n",
        "            if graph_symmetrize:\n",
        "                W_old = symmetrize_rows(W_old)\n",
        "\n",
        "    hS, hT = _estimate_bandwidths(coords_blocks_old, times_train, tau_s, tau_t)\n",
        "    A_cross = np.zeros((n_new, n_old), dtype=np.float32)\n",
        "    for i in range(n_new):\n",
        "        lat_i, lon_i, t_i = coords_new[i, 0], coords_new[i, 1], times_new[i]\n",
        "        for j in range(n_old):\n",
        "            lat_j, lon_j, t_j = coords_train[j, 0], coords_train[j, 1], times_train[j]\n",
        "            d_spa = haversine(lat_i, lon_i, lat_j, lon_j)\n",
        "            d_tmp = abs(t_i - t_j)\n",
        "            A_cross[i, j] = math.exp(-0.5 * (d_spa / hS) ** 2) * math.exp(-0.5 * (d_tmp / hT) ** 2)\n",
        "\n",
        "    if cross_topk is not None and cross_topk > 0:\n",
        "        k_eff = min(cross_topk, max(1, A_cross.shape[1] - 1))\n",
        "        idx = np.argpartition(-A_cross, kth=k_eff - 1, axis=1)[:, :k_eff]\n",
        "        mask = np.zeros_like(A_cross)\n",
        "        rows = np.arange(n_new)[:, None]\n",
        "        mask[rows, idx] = 1.0\n",
        "        A_cross = A_cross * mask\n",
        "\n",
        "    A_cross = A_cross / (A_cross.sum(axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        H_new = model.encoder(torch.tensor(X_new, dtype=torch.float32, device=device))\n",
        "        H_old_n = F.normalize(H_old, p=2, dim=1)\n",
        "        H_new_n = F.normalize(H_new, p=2, dim=1)\n",
        "        S = H_new_n @ H_old_n.t()\n",
        "        logits = S / model.tau\n",
        "        log_prior_cross = torch.log(torch.tensor(A_cross, dtype=torch.float32, device=device) + 1e-12)\n",
        "        alpha = model.alpha\n",
        "        log_blend = alpha * log_prior_cross + (1 - alpha) * logits\n",
        "        W_new2old_gnn = F.softmax(log_blend, dim=1)\n",
        "        if lambda_blend is not None and 0.0 <= lambda_blend <= 1.0:\n",
        "            W_new2old = lambda_blend * W_new2old_gnn + (1 - lambda_blend) * torch.tensor(A_cross, dtype=torch.float32, device=device)\n",
        "        else:\n",
        "            W_new2old = W_new2old_gnn\n",
        "\n",
        "    if new_self_weight and new_self_weight > 0:\n",
        "        W_new2new = torch.eye(n_new, device=device) * float(new_self_weight)\n",
        "    else:\n",
        "        W_new2new = torch.zeros((n_new, n_new), device=device)\n",
        "\n",
        "    W_full = torch.zeros((n_old + n_new, n_old + n_new), device=device)\n",
        "    W_full[:n_old, :n_old] = W_old\n",
        "    W_new_row = torch.cat([W_new2old, W_new2new], dim=1)\n",
        "    W_new_row = W_new_row / (W_new_row.sum(dim=1, keepdim=True) + 1e-12)\n",
        "    W_full[n_old:, :] = W_new_row\n",
        "\n",
        "    X_comb = np.vstack([X_train, X_new]).astype(np.float32)\n",
        "    X_comb_t = torch.tensor(X_comb, dtype=torch.float32, device=device)\n",
        "    y_stub = torch.tensor(\n",
        "        np.concatenate([y_train, np.zeros(n_new, dtype=np.float32)]),\n",
        "        dtype=torch.float32,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        y_hat = solve_local_wls(\n",
        "            X_comb_t,\n",
        "            y_stub,\n",
        "            W_full,\n",
        "            kind=wls_kind,\n",
        "            ridge=ridge_lambda,\n",
        "            huber_delta=huber_delta,\n",
        "            huber_iters=huber_iters,\n",
        "            return_betas=False,\n",
        "        )\n",
        "    return y_hat[n_old:].cpu().numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4927b859",
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_new_prior_only(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    coords_train,\n",
        "    times_train,\n",
        "    new_df,\n",
        "    feature_cols,\n",
        "    time_col,\n",
        "    lat_col,\n",
        "    lon_col,\n",
        "    tau_s=1.0,\n",
        "    tau_t=1.0,\n",
        "    knn_k=8,\n",
        "    wls_kind=\"ridge\",\n",
        "    ridge_lambda=5.0,\n",
        "    huber_delta=1.0,\n",
        "    huber_iters=3,\n",
        "    cross_topk=None,\n",
        "    new_self_weight=0.0,\n",
        "    device=None,\n",
        "):\n",
        "    device = device or torch.device(\"cpu\")\n",
        "    X_new = new_df[feature_cols].values.astype(np.float32)\n",
        "    coords_new = new_df[[lat_col, lon_col]].values.astype(np.float32)\n",
        "    times_new = new_df[time_col].values.astype(float)\n",
        "    n_old, n_new = len(X_train), len(X_new)\n",
        "\n",
        "    unique_times_old = np.sort(np.unique(times_train))\n",
        "    coords_blocks_old = [coords_train[times_train == t] for t in unique_times_old]\n",
        "    hS, hT = _estimate_bandwidths(coords_blocks_old, times_train, tau_s, tau_t)\n",
        "\n",
        "    A_cross = np.zeros((n_new, n_old), dtype=np.float32)\n",
        "    for i in range(n_new):\n",
        "        lat_i, lon_i, t_i = coords_new[i, 0], coords_new[i, 1], times_new[i]\n",
        "        for j in range(n_old):\n",
        "            lat_j, lon_j, t_j = coords_train[j, 0], coords_train[j, 1], times_train[j]\n",
        "            d_spa = haversine(lat_i, lon_i, lat_j, lon_j)\n",
        "            d_tmp = abs(t_i - t_j)\n",
        "            A_cross[i, j] = math.exp(-0.5 * (d_spa / hS) ** 2) * math.exp(-0.5 * (d_tmp / hT) ** 2)\n",
        "\n",
        "    if cross_topk is not None and cross_topk > 0:\n",
        "        k_eff = min(cross_topk, max(1, A_cross.shape[1] - 1))\n",
        "        idx = np.argpartition(-A_cross, kth=k_eff - 1, axis=1)[:, :k_eff]\n",
        "        mask = np.zeros_like(A_cross)\n",
        "        rows = np.arange(n_new)[:, None]\n",
        "        mask[rows, idx] = 1.0\n",
        "        A_cross = A_cross * mask\n",
        "\n",
        "    A_cross = A_cross / (A_cross.sum(axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "    if new_self_weight and new_self_weight > 0:\n",
        "        W_new2new = torch.eye(n_new) * float(new_self_weight)\n",
        "    else:\n",
        "        W_new2new = torch.zeros((n_new, n_new))\n",
        "\n",
        "    W_full = torch.zeros((n_old + n_new, n_old + n_new), dtype=torch.float32)\n",
        "    W_full[:n_old, :n_old] = torch.eye(n_old)\n",
        "    W_full[n_old:, :n_old] = torch.tensor(A_cross, dtype=torch.float32)\n",
        "    W_full[n_old:, n_old:] = W_new2new\n",
        "    W_full[n_old:, :] = W_full[n_old:, :] / (W_full[n_old:, :].sum(dim=1, keepdim=True) + 1e-12)\n",
        "\n",
        "    X_comb = np.vstack([X_train, X_new]).astype(np.float32)\n",
        "    X_comb_t = torch.tensor(X_comb, dtype=torch.float32, device=device)\n",
        "    y_stub = torch.tensor(np.concatenate([y_train, np.zeros(n_new, dtype=np.float32)]), dtype=torch.float32, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        y_hat = solve_local_wls(\n",
        "            X_comb_t,\n",
        "            y_stub,\n",
        "            torch.tensor(W_full, dtype=torch.float32, device=device),\n",
        "            kind=wls_kind,\n",
        "            ridge=ridge_lambda,\n",
        "            huber_delta=huber_delta,\n",
        "            huber_iters=huber_iters,\n",
        "            return_betas=False,\n",
        "        )\n",
        "    return y_hat[len(X_train):].cpu().numpy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68f7c3f5",
      "metadata": {},
      "source": [
        "## 8. Experimental Protocol\n",
        "We now instantiate the pipeline:\n",
        "\n",
        "1. Load the full panel and split 2019--2022 into train/validation/test years.\n",
        "2. Build the GTWR prior and tensor representations.\n",
        "3. Establish a GTWR-only baseline.\n",
        "4. Train multiple GNN configurations.\n",
        "5. Evaluate OOS strategies on 2023 (transductive, full-graph, fine-tuned).\n",
        "\n",
        "We reuse the configuration logic from the earlier prototype and wrap it here to keep the notebook self-contained.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a790ef9f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 595 records with columns: ['lat', 'lon', 'Tahun', 'y', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8']\n",
            "Years covered: [2019 2020 2021 2022] | locations per year: 104 | X shape: (416, 8)\n",
            "Split sizes -> train: 208, val: 104, test: 104\n",
            "Kernel constructed with sparsity 0.978\n"
          ]
        }
      ],
      "source": [
        "LAT_COL, LON_COL = \"lat\", \"lon\"\n",
        "TIME_COL, TARGET_COL = \"Tahun\", \"y\"\n",
        "FEATURE_COLS = [\"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X7\", \"X8\"]\n",
        "\n",
        "df_full = load_panel_xlsx(DATA_PATH, LAT_COL, LON_COL, TIME_COL, TARGET_COL, FEATURE_COLS)\n",
        "print(f\"Loaded {len(df_full)} records with columns: {df_full.columns.tolist()}\")\n",
        "\n",
        "mask_pre2023 = df_full[TIME_COL] < 2023\n",
        "mask_2023 = df_full[TIME_COL] == 2023\n",
        "\n",
        "df_2019_2022 = df_full[mask_pre2023].copy()\n",
        "df_2023 = df_full[mask_2023].copy().sort_values([LAT_COL, LON_COL]).reset_index(drop=True)\n",
        "\n",
        "times_2019_2022 = sorted(df_2019_2022[TIME_COL].unique())\n",
        "P = build_panel_arrays(\n",
        "    df_2019_2022,\n",
        "    TIME_COL,\n",
        "    TARGET_COL,\n",
        "    FEATURE_COLS,\n",
        "    LAT_COL,\n",
        "    LON_COL,\n",
        "    times_2019_2022,\n",
        ")\n",
        "X_all, y_all = P[\"X_all\"], P[\"y_all\"]\n",
        "coords_blocks, times, N_per_year = P[\"coords_blocks\"], P[\"times\"], P[\"N_per_year\"]\n",
        "coords_all = np.vstack(coords_blocks)\n",
        "\n",
        "print(f\"Years covered: {times} | locations per year: {N_per_year} | X shape: {X_all.shape}\")\n",
        "\n",
        "split = split_train_val_test(times, N_per_year, use_val=True)\n",
        "train_rows, val_rows, test_rows = split[\"train_rows\"], split[\"val_rows\"], split[\"test_rows\"]\n",
        "print(f\"Split sizes -> train: {len(train_rows)}, val: {len(val_rows)}, test: {len(test_rows)}\")\n",
        "\n",
        "W_prior = build_spatiotemporal_kernel(\n",
        "    coords_blocks,\n",
        "    times,\n",
        "    tau_s=1.0,\n",
        "    tau_t=1.0,\n",
        "    k_neighbors=8,\n",
        "    prior_self_weight=1.0,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "X_t = torch.tensor(X_all, dtype=torch.float32, device=device)\n",
        "y_t = torch.tensor(y_all, dtype=torch.float32, device=device)\n",
        "A_t = torch.tensor(W_prior, dtype=torch.float32, device=device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GTWR Baseline (Theory)\n",
        "Setting the learned weights aside, the GTWR baseline solves the local ridge WLS system using only the prior matrix $A_{prior}$. This quantifies the contribution of distance-based smoothing alone.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In-sample baseline metrics (2019-2022):\n"
          ]
        },
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "name",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "rmse_tr",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "mae_tr",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "r2_tr",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "rmse_va",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "mae_va",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "r2_va",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "rmse_te",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "mae_te",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "r2_te",
                  "rawType": "float64",
                  "type": "float"
                }
              ],
              "ref": "39888794-8240-4908-9056-2dd072b78a09",
              "rows": [
                [
                  "0",
                  "GTWR (prior only)",
                  "0.8238350938020212",
                  "0.6300923228263855",
                  "0.9035200476646423",
                  "0.9400717746441194",
                  "0.7431483864784241",
                  "0.8677577376365662",
                  "1.0414430123871046",
                  "0.8213625550270081",
                  "0.7783491015434265"
                ]
              ],
              "shape": {
                "columns": 10,
                "rows": 1
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>rmse_tr</th>\n",
              "      <th>mae_tr</th>\n",
              "      <th>r2_tr</th>\n",
              "      <th>rmse_va</th>\n",
              "      <th>mae_va</th>\n",
              "      <th>r2_va</th>\n",
              "      <th>rmse_te</th>\n",
              "      <th>mae_te</th>\n",
              "      <th>r2_te</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>GTWR (prior only)</td>\n",
              "      <td>0.823835</td>\n",
              "      <td>0.630092</td>\n",
              "      <td>0.90352</td>\n",
              "      <td>0.940072</td>\n",
              "      <td>0.743148</td>\n",
              "      <td>0.867758</td>\n",
              "      <td>1.041443</td>\n",
              "      <td>0.821363</td>\n",
              "      <td>0.778349</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                name   rmse_tr    mae_tr    r2_tr   rmse_va    mae_va  \\\n",
              "0  GTWR (prior only)  0.823835  0.630092  0.90352  0.940072  0.743148   \n",
              "\n",
              "      r2_va   rmse_te    mae_te     r2_te  \n",
              "0  0.867758  1.041443  0.821363  0.778349  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "y_gtwr = local_wls_ridge(X_t, y_t, A_t, ridge=10.0, return_betas=False).detach().cpu().numpy()\n",
        "\n",
        "def summarize_split(pred_np, rows):\n",
        "    if rows is None or len(rows) == 0:\n",
        "        return (math.nan, math.nan, math.nan)\n",
        "    return regression_metrics(y_all[rows], pred_np[rows])\n",
        "\n",
        "rmse_tr, mae_tr, r2_tr = summarize_split(y_gtwr, train_rows)\n",
        "rmse_va, mae_va, r2_va = summarize_split(y_gtwr, val_rows)\n",
        "rmse_te, mae_te, r2_te = summarize_split(y_gtwr, test_rows)\n",
        "\n",
        "baseline_summary = pd.DataFrame([\n",
        "    {\n",
        "        \"name\": \"GTWR (prior only)\",\n",
        "        \"rmse_tr\": rmse_tr,\n",
        "        \"mae_tr\": mae_tr,\n",
        "        \"r2_tr\": r2_tr,\n",
        "        \"rmse_va\": rmse_va,\n",
        "        \"mae_va\": mae_va,\n",
        "        \"r2_va\": r2_va,\n",
        "        \"rmse_te\": rmse_te,\n",
        "        \"mae_te\": mae_te,\n",
        "        \"r2_te\": r2_te,\n",
        "    }\n",
        "])\n",
        "print(\"In-sample baseline metrics (2019-2022):\")\n",
        "display(baseline_summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GNN Configuration Logic\n",
        "We define a helper to train multiple settings while insulating against return-type variations. Each configuration mirrors the experiments from the prototype notebook, showcasing different WLS solvers, graph truncation, and regularization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 001 | Loss 0.6859 | RMSE train 0.832 | val 0.949 | alpha 0.300 | tau 1.199\n",
            "Epoch 025 | Loss 0.6856 | RMSE train 0.832 | val 0.949 | alpha 0.305 | tau 1.171\n",
            "Epoch 050 | Loss 0.6853 | RMSE train 0.832 | val 0.949 | alpha 0.311 | tau 1.144\n",
            "Epoch 075 | Loss 0.6853 | RMSE train 0.832 | val 0.949 | alpha 0.316 | tau 1.119\n",
            "Epoch 100 | Loss 0.6850 | RMSE train 0.832 | val 0.949 | alpha 0.321 | tau 1.098\n",
            "Epoch 125 | Loss 0.6850 | RMSE train 0.832 | val 0.949 | alpha 0.327 | tau 1.079\n",
            "Epoch 150 | Loss 0.6847 | RMSE train 0.832 | val 0.949 | alpha 0.332 | tau 1.063\n",
            "Epoch 175 | Loss 0.6845 | RMSE train 0.832 | val 0.948 | alpha 0.338 | tau 1.049\n",
            "Epoch 200 | Loss 0.6843 | RMSE train 0.831 | val 0.948 | alpha 0.343 | tau 1.038\n",
            "Epoch 001 | Loss 0.7672 | RMSE train 0.870 | val 0.978 | alpha 0.300 | tau 1.199\n",
            "Epoch 025 | Loss 0.7672 | RMSE train 0.869 | val 0.978 | alpha 0.305 | tau 1.171\n",
            "Epoch 050 | Loss 0.7670 | RMSE train 0.869 | val 0.978 | alpha 0.311 | tau 1.144\n",
            "Epoch 075 | Loss 0.7668 | RMSE train 0.869 | val 0.978 | alpha 0.316 | tau 1.119\n",
            "Epoch 100 | Loss 0.7666 | RMSE train 0.869 | val 0.978 | alpha 0.321 | tau 1.098\n",
            "Epoch 125 | Loss 0.7665 | RMSE train 0.869 | val 0.978 | alpha 0.327 | tau 1.079\n",
            "Epoch 150 | Loss 0.7663 | RMSE train 0.869 | val 0.977 | alpha 0.332 | tau 1.063\n",
            "Epoch 175 | Loss 0.7662 | RMSE train 0.869 | val 0.977 | alpha 0.337 | tau 1.049\n",
            "Epoch 200 | Loss 0.7661 | RMSE train 0.869 | val 0.977 | alpha 0.343 | tau 1.038\n",
            "Epoch 001 | Loss 0.6212 | RMSE train 0.786 | val 0.903 | alpha 0.300 | tau 1.199\n",
            "Epoch 025 | Loss 0.6216 | RMSE train 0.786 | val 0.903 | alpha 0.305 | tau 1.171\n",
            "Epoch 050 | Loss 0.6214 | RMSE train 0.786 | val 0.904 | alpha 0.311 | tau 1.144\n",
            "Epoch 075 | Loss 0.6211 | RMSE train 0.785 | val 0.904 | alpha 0.316 | tau 1.119\n",
            "Epoch 100 | Loss 0.6209 | RMSE train 0.785 | val 0.902 | alpha 0.321 | tau 1.098\n",
            "Epoch 125 | Loss 0.6207 | RMSE train 0.785 | val 0.904 | alpha 0.327 | tau 1.079\n",
            "Epoch 150 | Loss 0.6205 | RMSE train 0.785 | val 0.903 | alpha 0.332 | tau 1.062\n",
            "Epoch 175 | Loss 0.6203 | RMSE train 0.785 | val 0.903 | alpha 0.338 | tau 1.049\n",
            "Epoch 200 | Loss 0.6201 | RMSE train 0.785 | val 0.903 | alpha 0.343 | tau 1.037\n",
            "Epoch 001 | Loss 0.3928 | RMSE train 0.626 | val 0.766 | alpha 0.300 | tau 1.199\n",
            "Epoch 025 | Loss 0.3926 | RMSE train 0.628 | val 0.766 | alpha 0.305 | tau 1.171\n",
            "Epoch 050 | Loss 0.3925 | RMSE train 0.626 | val 0.765 | alpha 0.311 | tau 1.144\n",
            "Epoch 075 | Loss 0.3924 | RMSE train 0.628 | val 0.765 | alpha 0.316 | tau 1.119\n",
            "Epoch 100 | Loss 0.3922 | RMSE train 0.627 | val 0.765 | alpha 0.321 | tau 1.098\n",
            "Epoch 125 | Loss 0.3921 | RMSE train 0.627 | val 0.765 | alpha 0.327 | tau 1.079\n",
            "Epoch 150 | Loss 0.3920 | RMSE train 0.627 | val 0.765 | alpha 0.332 | tau 1.063\n",
            "Epoch 175 | Loss 0.3918 | RMSE train 0.627 | val 0.765 | alpha 0.338 | tau 1.049\n",
            "Epoch 200 | Loss 0.3895 | RMSE train 0.625 | val 0.765 | alpha 0.343 | tau 1.038\n",
            "In-sample comparison across configurations:\n"
          ]
        },
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "name",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "rmse_tr",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "mae_tr",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "r2_tr",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "rmse_va",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "mae_va",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "r2_va",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "rmse_te",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "mae_te",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "r2_te",
                  "rawType": "float64",
                  "type": "float"
                }
              ],
              "ref": "6cf048bc-5165-4d2f-8315-0f33388b6917",
              "rows": [
                [
                  "0",
                  "GTWR (prior only)",
                  "0.8238350938020212",
                  "0.6300923228263855",
                  "0.9035200476646423",
                  "0.9400717746441194",
                  "0.7431483864784241",
                  "0.8677577376365662",
                  "1.0414430123871046",
                  "0.8213625550270081",
                  "0.7783491015434265"
                ],
                [
                  "1",
                  "GNN + Ridge + Entropy (TopK=12)",
                  "0.8314314995429942",
                  "0.6387324929237366",
                  "0.9017326235771179",
                  "0.9482962753003112",
                  "0.7518225908279419",
                  "0.8654336929321289",
                  "1.0484339365777802",
                  "0.8273142576217651",
                  "0.7753633260726929"
                ],
                [
                  "2",
                  "GNN + Huber + TopK=12 + Sym",
                  "0.8689504351028695",
                  "0.6291641592979431",
                  "0.8926637768745422",
                  "0.97724676954624",
                  "0.7242899537086487",
                  "0.8570919036865234",
                  "1.075299582765481",
                  "0.8267048597335815",
                  "0.763703465461731"
                ],
                [
                  "3",
                  "GNN + Ridge + TopK=8 (KL placeholder)",
                  "0.784879075655962",
                  "0.5999507904052734",
                  "0.9124286770820618",
                  "0.9020756575072101",
                  "0.7314327955245972",
                  "0.8782317042350769",
                  "0.9667098497468435",
                  "0.763701319694519",
                  "0.8090187311172485"
                ],
                [
                  "4",
                  "GNN + Ridge + TopK=6",
                  "0.6252567717480237",
                  "0.47210633754730225",
                  "0.9444258213043213",
                  "0.7647822666101445",
                  "0.5885192155838013",
                  "0.912476658821106",
                  "0.8419939416748973",
                  "0.627065896987915",
                  "0.8551173210144043"
                ]
              ],
              "shape": {
                "columns": 10,
                "rows": 5
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>rmse_tr</th>\n",
              "      <th>mae_tr</th>\n",
              "      <th>r2_tr</th>\n",
              "      <th>rmse_va</th>\n",
              "      <th>mae_va</th>\n",
              "      <th>r2_va</th>\n",
              "      <th>rmse_te</th>\n",
              "      <th>mae_te</th>\n",
              "      <th>r2_te</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>GTWR (prior only)</td>\n",
              "      <td>0.823835</td>\n",
              "      <td>0.630092</td>\n",
              "      <td>0.903520</td>\n",
              "      <td>0.940072</td>\n",
              "      <td>0.743148</td>\n",
              "      <td>0.867758</td>\n",
              "      <td>1.041443</td>\n",
              "      <td>0.821363</td>\n",
              "      <td>0.778349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GNN + Ridge + Entropy (TopK=12)</td>\n",
              "      <td>0.831431</td>\n",
              "      <td>0.638732</td>\n",
              "      <td>0.901733</td>\n",
              "      <td>0.948296</td>\n",
              "      <td>0.751823</td>\n",
              "      <td>0.865434</td>\n",
              "      <td>1.048434</td>\n",
              "      <td>0.827314</td>\n",
              "      <td>0.775363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>GNN + Huber + TopK=12 + Sym</td>\n",
              "      <td>0.868950</td>\n",
              "      <td>0.629164</td>\n",
              "      <td>0.892664</td>\n",
              "      <td>0.977247</td>\n",
              "      <td>0.724290</td>\n",
              "      <td>0.857092</td>\n",
              "      <td>1.075300</td>\n",
              "      <td>0.826705</td>\n",
              "      <td>0.763703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>GNN + Ridge + TopK=8 (KL placeholder)</td>\n",
              "      <td>0.784879</td>\n",
              "      <td>0.599951</td>\n",
              "      <td>0.912429</td>\n",
              "      <td>0.902076</td>\n",
              "      <td>0.731433</td>\n",
              "      <td>0.878232</td>\n",
              "      <td>0.966710</td>\n",
              "      <td>0.763701</td>\n",
              "      <td>0.809019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>GNN + Ridge + TopK=6</td>\n",
              "      <td>0.625257</td>\n",
              "      <td>0.472106</td>\n",
              "      <td>0.944426</td>\n",
              "      <td>0.764782</td>\n",
              "      <td>0.588519</td>\n",
              "      <td>0.912477</td>\n",
              "      <td>0.841994</td>\n",
              "      <td>0.627066</td>\n",
              "      <td>0.855117</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    name   rmse_tr    mae_tr     r2_tr  \\\n",
              "0                      GTWR (prior only)  0.823835  0.630092  0.903520   \n",
              "1        GNN + Ridge + Entropy (TopK=12)  0.831431  0.638732  0.901733   \n",
              "2            GNN + Huber + TopK=12 + Sym  0.868950  0.629164  0.892664   \n",
              "3  GNN + Ridge + TopK=8 (KL placeholder)  0.784879  0.599951  0.912429   \n",
              "4                   GNN + Ridge + TopK=6  0.625257  0.472106  0.944426   \n",
              "\n",
              "    rmse_va    mae_va     r2_va   rmse_te    mae_te     r2_te  \n",
              "0  0.940072  0.743148  0.867758  1.041443  0.821363  0.778349  \n",
              "1  0.948296  0.751823  0.865434  1.048434  0.827314  0.775363  \n",
              "2  0.977247  0.724290  0.857092  1.075300  0.826705  0.763703  \n",
              "3  0.902076  0.731433  0.878232  0.966710  0.763701  0.809019  \n",
              "4  0.764782  0.588519  0.912477  0.841994  0.627066  0.855117  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def unwrap_train_output(res, model_seed, ridge_lambda=10.0):\n",
        "    model_tr = model_seed\n",
        "    y_hat_t = None\n",
        "    history = []\n",
        "    if isinstance(res, dict):\n",
        "        model_tr = res.get(\"model\", model_seed)\n",
        "        y_hat_t = res.get(\"y_hat\", None)\n",
        "        history = res.get(\"history\", [])\n",
        "    elif isinstance(res, (tuple, list)):\n",
        "        if len(res) >= 1 and hasattr(res[0], \"state_dict\"):\n",
        "            model_tr = res[0]\n",
        "        if len(res) >= 2:\n",
        "            history = res[1]\n",
        "        if len(res) >= 3:\n",
        "            pack = res[2]\n",
        "            if torch.is_tensor(pack) and pack.shape[0] == X_t.shape[0]:\n",
        "                y_hat_t = pack\n",
        "            elif isinstance(pack, (tuple, list)):\n",
        "                for item in pack:\n",
        "                    if torch.is_tensor(item) and item.shape[0] == X_t.shape[0]:\n",
        "                        y_hat_t = item\n",
        "                        break\n",
        "    if y_hat_t is None:\n",
        "        with torch.no_grad():\n",
        "            W_learned, _ = model_tr(X_t, A_t)\n",
        "            y_hat_t = local_wls_ridge(X_t, y_t, W_learned, ridge=ridge_lambda, return_betas=False)\n",
        "    return model_tr, y_hat_t, history\n",
        "\n",
        "def run_one_config(\n",
        "    cfg_name,\n",
        "    wls_kind=\"ridge\",\n",
        "    ridge_lambda=10.0,\n",
        "    huber_delta=1.0,\n",
        "    huber_iters=5,\n",
        "    ent_w=5e-3,\n",
        "    smooth_w=1e-3,\n",
        "    graph_topk=None,\n",
        "    graph_symmetrize=False,\n",
        "    epochs=200,\n",
        "    lr=1e-3,\n",
        "    early_stop_patience=80,\n",
        "    print_every=25,\n",
        "    **extra_kwargs,\n",
        "):\n",
        "    model = MathematicallyCorrectGNNWeightNet(d_in=X_all.shape[1]).to(device)\n",
        "    train_kwargs = dict(\n",
        "        model=model,\n",
        "        X_all=X_all,\n",
        "        y_all=y_all,\n",
        "        A_prior=W_prior,\n",
        "        train_rows=train_rows,\n",
        "        val_rows=val_rows,\n",
        "        test_rows=test_rows,\n",
        "        epochs=epochs,\n",
        "        lr=lr,\n",
        "        ridge_lambda=ridge_lambda,\n",
        "        wls_kind=wls_kind,\n",
        "        huber_delta=huber_delta,\n",
        "        huber_iters=huber_iters,\n",
        "        ent_w=ent_w,\n",
        "        smooth_w=smooth_w,\n",
        "        N_per_year=N_per_year,\n",
        "        times=times,\n",
        "        graph_topk=graph_topk,\n",
        "        graph_symmetrize=graph_symmetrize,\n",
        "        early_stop=True,\n",
        "        es_patience=early_stop_patience,\n",
        "        device=device,\n",
        "        print_every=print_every,\n",
        "    )\n",
        "    train_kwargs.update(extra_kwargs)\n",
        "    res = safe_call(train_model, **train_kwargs)\n",
        "    model_tr, y_hat_t, history = unwrap_train_output(res, model, ridge_lambda=ridge_lambda)\n",
        "    y_pred = to_numpy(y_hat_t)\n",
        "\n",
        "    def collect(rows):\n",
        "        return regression_metrics(y_all[rows], y_pred[rows])\n",
        "\n",
        "    summary = {\n",
        "        \"name\": cfg_name,\n",
        "        \"rmse_tr\": collect(train_rows)[0],\n",
        "        \"mae_tr\": collect(train_rows)[1],\n",
        "        \"r2_tr\": collect(train_rows)[2],\n",
        "        \"rmse_va\": collect(val_rows)[0],\n",
        "        \"mae_va\": collect(val_rows)[1],\n",
        "        \"r2_va\": collect(val_rows)[2],\n",
        "        \"rmse_te\": collect(test_rows)[0],\n",
        "        \"mae_te\": collect(test_rows)[1],\n",
        "        \"r2_te\": collect(test_rows)[2],\n",
        "        \"history\": history,\n",
        "    }\n",
        "    return summary, model_tr\n",
        "\n",
        "experiments = []\n",
        "\n",
        "summ_B, model_B = run_one_config(\n",
        "    \"GNN + Ridge + Entropy (TopK=12)\",\n",
        "    wls_kind=\"ridge\",\n",
        "    ridge_lambda=10.0,\n",
        "    ent_w=5e-3,\n",
        "    smooth_w=1e-3,\n",
        "    graph_topk=12,\n",
        "    graph_symmetrize=False,\n",
        ")\n",
        "experiments.append((summ_B, model_B))\n",
        "\n",
        "summ_C, model_C = run_one_config(\n",
        "    \"GNN + Huber + TopK=12 + Sym\",\n",
        "    wls_kind=\"huber\",\n",
        "    ridge_lambda=10.0,\n",
        "    huber_delta=1.0,\n",
        "    huber_iters=5,\n",
        "    ent_w=5e-3,\n",
        "    smooth_w=1e-3,\n",
        "    graph_topk=12,\n",
        "    graph_symmetrize=True,\n",
        ")\n",
        "experiments.append((summ_C, model_C))\n",
        "\n",
        "summ_D, model_D = run_one_config(\n",
        "    \"GNN + Ridge + TopK=8 (KL placeholder)\",\n",
        "    wls_kind=\"ridge\",\n",
        "    ridge_lambda=10.0,\n",
        "    ent_w=0.0,\n",
        "    smooth_w=1e-3,\n",
        "    graph_topk=8,\n",
        "    graph_symmetrize=False,\n",
        ")\n",
        "experiments.append((summ_D, model_D))\n",
        "\n",
        "summ_E, model_E = run_one_config(\n",
        "    \"GNN + Ridge + TopK=6\",\n",
        "    wls_kind=\"ridge\",\n",
        "    ridge_lambda=7.0,\n",
        "    ent_w=5e-3,\n",
        "    smooth_w=1e-3,\n",
        "    graph_topk=6,\n",
        "    graph_symmetrize=False,\n",
        ")\n",
        "experiments.append((summ_E, model_E))\n",
        "\n",
        "in_sample_df = pd.concat([baseline_summary] + [pd.DataFrame([e[0]]) for e in experiments], ignore_index=True)\n",
        "print(\"In-sample comparison across configurations:\")\n",
        "display(in_sample_df.drop(columns=[\"history\"], errors=\"ignore\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### OOS Evaluation Protocol\n",
        "We next evaluate each trained model on 2023 using the three inference modes. For the fine-tune variant we reconstruct the 2019--2023 panel so that future rows can receive graph edges while their labels remain masked during optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "build_spatiotemporal_kernel() got an unexpected keyword argument 'knn_k'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[15], line 155\u001b[0m\n\u001b[0;32m    140\u001b[0m     rmse_o3, mae_o3, r2_o3 \u001b[38;5;241m=\u001b[39m regression_metrics(y_all_23[future_rows_ft], y_hat_23_np[future_rows_ft])\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: cfg_name,\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrmse_oos_trans\u001b[39m\u001b[38;5;124m\"\u001b[39m: rmse_o1,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr2_oos_reft\u001b[39m\u001b[38;5;124m\"\u001b[39m: r2_o3,\n\u001b[0;32m    153\u001b[0m     }\n\u001b[1;32m--> 155\u001b[0m oos_rows \u001b[38;5;241m=\u001b[39m [\u001b[43meval_oos_for_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummary\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m summary, model \u001b[38;5;129;01min\u001b[39;00m experiments]\n\u001b[0;32m    156\u001b[0m oos_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(oos_rows)\n\u001b[0;32m    158\u001b[0m summary_all \u001b[38;5;241m=\u001b[39m in_sample_df\u001b[38;5;241m.\u001b[39mmerge(oos_df, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[15], line 16\u001b[0m, in \u001b[0;36meval_oos_for_model\u001b[1;34m(model, cfg_name, cross_topk, lambda_blend, new_self_weight)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df_2023 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df_2023) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: cfg_name,\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrmse_oos_trans\u001b[39m\u001b[38;5;124m\"\u001b[39m: math\u001b[38;5;241m.\u001b[39mnan,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr2_oos_reft\u001b[39m\u001b[38;5;124m\"\u001b[39m: math\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m     14\u001b[0m     }\n\u001b[1;32m---> 16\u001b[0m y_trans \u001b[38;5;241m=\u001b[39m \u001b[43msafe_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredict_new_oos_transductive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_all\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_all\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoords_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoords_all\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimes_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_per_year\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_2023\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFEATURE_COLS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTIME_COL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlat_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLAT_COL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlon_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLON_COL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtau_s\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtau_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mknn_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprior_self_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda_blend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_blend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_topk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_topk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_self_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_self_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwls_kind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mridge\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mridge_lambda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhuber_delta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhuber_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m y_trans \u001b[38;5;241m=\u001b[39m to_numpy(y_trans)\n\u001b[0;32m     43\u001b[0m y_full \u001b[38;5;241m=\u001b[39m safe_call(\n\u001b[0;32m     44\u001b[0m     predict_new_fullgraph,\n\u001b[0;32m     45\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m     64\u001b[0m )\n",
            "Cell \u001b[1;32mIn[2], line 7\u001b[0m, in \u001b[0;36msafe_call\u001b[1;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m      5\u001b[0m sig \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(fn)\n\u001b[0;32m      6\u001b[0m allowed \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m sig\u001b[38;5;241m.\u001b[39mparameters}\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mallowed\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[10], line 35\u001b[0m, in \u001b[0;36mpredict_new_oos_transductive\u001b[1;34m(model, X_train, y_train, coords_train, times_train, new_df, feature_cols, time_col, lat_col, lon_col, tau_s, tau_t, knn_k, prior_self_weight, lambda_blend, wls_kind, ridge_lambda, huber_delta, huber_iters, graph_topk, graph_symmetrize, device, cross_topk, new_self_weight)\u001b[0m\n\u001b[0;32m     33\u001b[0m unique_times_old \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msort(np\u001b[38;5;241m.\u001b[39munique(times_train))\n\u001b[0;32m     34\u001b[0m coords_blocks_old \u001b[38;5;241m=\u001b[39m [coords_train[times_train \u001b[38;5;241m==\u001b[39m t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m unique_times_old]\n\u001b[1;32m---> 35\u001b[0m A_prior_old_np \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_spatiotemporal_kernel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoords_blocks_old\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43munique_times_old\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtau_s\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtau_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtau_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtau_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mknn_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mknn_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprior_self_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprior_self_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m A_prior_old \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(A_prior_old_np, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     45\u001b[0m X_old_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_train, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n",
            "\u001b[1;31mTypeError\u001b[0m: build_spatiotemporal_kernel() got an unexpected keyword argument 'knn_k'"
          ]
        }
      ],
      "source": [
        "def eval_oos_for_model(model, cfg_name, cross_topk=12, lambda_blend=0.8, new_self_weight=0.0):\n",
        "    if df_2023 is None or len(df_2023) == 0:\n",
        "        return {\n",
        "            \"name\": cfg_name,\n",
        "            \"rmse_oos_trans\": math.nan,\n",
        "            \"mae_oos_trans\": math.nan,\n",
        "            \"r2_oos_trans\": math.nan,\n",
        "            \"rmse_oos_full\": math.nan,\n",
        "            \"mae_oos_full\": math.nan,\n",
        "            \"r2_oos_full\": math.nan,\n",
        "            \"rmse_oos_reft\": math.nan,\n",
        "            \"mae_oos_reft\": math.nan,\n",
        "            \"r2_oos_reft\": math.nan,\n",
        "        }\n",
        "\n",
        "    y_trans = safe_call(\n",
        "        predict_new_oos_transductive,\n",
        "        model=model,\n",
        "        X_train=X_all,\n",
        "        y_train=y_all,\n",
        "        coords_train=coords_all,\n",
        "        times_train=np.repeat(times, N_per_year),\n",
        "        new_df=df_2023,\n",
        "        feature_cols=FEATURE_COLS,\n",
        "        time_col=TIME_COL,\n",
        "        lat_col=LAT_COL,\n",
        "        lon_col=LON_COL,\n",
        "        tau_s=1.0,\n",
        "        tau_t=1.0,\n",
        "        knn_k=8,\n",
        "        prior_self_weight=1.0,\n",
        "        lambda_blend=lambda_blend,\n",
        "        cross_topk=cross_topk,\n",
        "        new_self_weight=new_self_weight,\n",
        "        wls_kind=\"ridge\",\n",
        "        ridge_lambda=10.0,\n",
        "        huber_delta=1.0,\n",
        "        huber_iters=5,\n",
        "        device=device,\n",
        "    )\n",
        "    y_trans = to_numpy(y_trans)\n",
        "\n",
        "    y_full = safe_call(\n",
        "        predict_new_fullgraph,\n",
        "        model=model,\n",
        "        X_train=X_all,\n",
        "        y_train=y_all,\n",
        "        coords_train=coords_all,\n",
        "        times_train=np.repeat(times, N_per_year),\n",
        "        new_df=df_2023,\n",
        "        feature_cols=FEATURE_COLS,\n",
        "        time_col=TIME_COL,\n",
        "        lat_col=LAT_COL,\n",
        "        lon_col=LON_COL,\n",
        "        tau_s=1.0,\n",
        "        tau_t=1.0,\n",
        "        knn_k=8,\n",
        "        prior_self_weight=1.0,\n",
        "        wls_kind=\"ridge\",\n",
        "        ridge_lambda=10.0,\n",
        "        huber_delta=1.0,\n",
        "        huber_iters=5,\n",
        "        device=device,\n",
        "    )\n",
        "    y_full = to_numpy(y_full)\n",
        "\n",
        "    df_2019_2023 = df_full[df_full[TIME_COL] <= 2023].copy()\n",
        "    times_2019_2023 = sorted(df_2019_2023[TIME_COL].unique())\n",
        "    P23 = build_panel_arrays(\n",
        "        df_2019_2023,\n",
        "        TIME_COL,\n",
        "        TARGET_COL,\n",
        "        FEATURE_COLS,\n",
        "        LAT_COL,\n",
        "        LON_COL,\n",
        "        times_2019_2023,\n",
        "    )\n",
        "    X_all_23, y_all_23 = P23[\"X_all\"], P23[\"y_all\"]\n",
        "    coords_blocks_23, times_23, N_per_year_23 = P23[\"coords_blocks\"], P23[\"times\"], P23[\"N_per_year\"]\n",
        "\n",
        "    train_rows_ft = np.concatenate([year_rows(times_23, N_per_year_23, t) for t in times_23 if t <= 2021])\n",
        "    val_rows_ft = year_rows(times_23, N_per_year_23, 2022)\n",
        "    future_rows_ft = year_rows(times_23, N_per_year_23, 2023)\n",
        "\n",
        "    ft_res = safe_call(\n",
        "        finetune_transductive_with_future,\n",
        "        model=model,\n",
        "        X_all_full=X_all_23,\n",
        "        y_all_full=y_all_23,\n",
        "        coords_blocks_full=coords_blocks_23,\n",
        "        times_full=times_23,\n",
        "        train_rows=train_rows_ft,\n",
        "        val_rows=val_rows_ft,\n",
        "        future_rows=future_rows_ft,\n",
        "        lr=1e-4,\n",
        "        epochs=150,\n",
        "        ridge_lambda=10.0,\n",
        "        ent_w=5e-3,\n",
        "        smooth_w=1e-3,\n",
        "        knn_k=8,\n",
        "        tau_s=1.0,\n",
        "        tau_t=1.0,\n",
        "        prior_self_weight=1.0,\n",
        "        N_per_year=N_per_year_23,\n",
        "        print_every=25,\n",
        "        wls_kind=\"ridge\",\n",
        "        graph_topk=None,\n",
        "        graph_symmetrize=False,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    if isinstance(ft_res, dict):\n",
        "        y_hat_23 = ft_res.get(\"y_hat\", None)\n",
        "        model_ft = ft_res.get(\"model\", model)\n",
        "    else:\n",
        "        model_ft = model\n",
        "        y_hat_23 = None\n",
        "\n",
        "    if y_hat_23 is None:\n",
        "        with torch.no_grad():\n",
        "            A23 = build_spatiotemporal_kernel(\n",
        "                coords_blocks_23,\n",
        "                times_23,\n",
        "                tau_s=1.0,\n",
        "                tau_t=1.0,\n",
        "                k_neighbors=8,\n",
        "                prior_self_weight=1.0,\n",
        "                verbose=False,\n",
        "            )\n",
        "            X23_t = torch.tensor(X_all_23, dtype=torch.float32, device=device)\n",
        "            A23_t = torch.tensor(A23, dtype=torch.float32, device=device)\n",
        "            W_learned, _ = model_ft(X23_t, A23_t)\n",
        "            y_hat_23 = local_wls_ridge(X23_t, torch.tensor(y_all_23, dtype=torch.float32, device=device), W_learned, ridge=10.0, return_betas=False)\n",
        "\n",
        "    y_hat_23_np = to_numpy(y_hat_23)\n",
        "    yt = df_2023[TARGET_COL].values.astype(np.float32)\n",
        "\n",
        "    rmse_o1, mae_o1, r2_o1 = regression_metrics(yt, y_trans)\n",
        "    rmse_o2, mae_o2, r2_o2 = regression_metrics(yt, y_full)\n",
        "    rmse_o3, mae_o3, r2_o3 = regression_metrics(y_all_23[future_rows_ft], y_hat_23_np[future_rows_ft])\n",
        "\n",
        "    return {\n",
        "        \"name\": cfg_name,\n",
        "        \"rmse_oos_trans\": rmse_o1,\n",
        "        \"mae_oos_trans\": mae_o1,\n",
        "        \"r2_oos_trans\": r2_o1,\n",
        "        \"rmse_oos_full\": rmse_o2,\n",
        "        \"mae_oos_full\": mae_o2,\n",
        "        \"r2_oos_full\": r2_o2,\n",
        "        \"rmse_oos_reft\": rmse_o3,\n",
        "        \"mae_oos_reft\": mae_o3,\n",
        "        \"r2_oos_reft\": r2_o3,\n",
        "    }\n",
        "\n",
        "oos_rows = [eval_oos_for_model(model, summary[\"name\"]) for summary, model in experiments]\n",
        "oos_df = pd.DataFrame(oos_rows)\n",
        "\n",
        "summary_all = in_sample_df.merge(oos_df, on=\"name\", how=\"outer\")\n",
        "print(\"Combined in-sample and OOS metrics:\")\n",
        "display(summary_all)\n",
        "\n",
        "output_dir = Path(\"exp_outputs\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "summary_path = output_dir / \"gtwr_gnn_experiment_summary.csv\"\n",
        "summary_all.to_csv(summary_path, index=False)\n",
        "print(f\"Summary saved to {summary_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Discussion\n",
        "- **Panel alignment:** The revised balancing ensures that each spatial index corresponds to the same coordinate across years, avoiding silent corruption from earlier truncation.\n",
        "- **Regularization:** Exposing `smooth_w` in the loss highlights its effect; tuning this parameter now genuinely shapes the model.\n",
        "- **Model comparison:** Inspecting `summary_all` reveals how the GNN augmentations alter both in-sample fit and OOS generalization relative to the GTWR prior.\n",
        "\n",
        "Future improvements could incorporate explicit KL penalties toward the prior or data-driven selection of the spatial bandwidths, as well as uncertainty quantification via bootstrap resampling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. References\n",
        "- Fotheringham, A. Stewart, et al. *Geographically Weighted Regression*. Wiley, 2002.\n",
        "- Huang, Bo, et al. \"Geographically and Temporally Weighted Regression\". *Geographical Analysis* 42.2 (2010).\n",
        "- Veli\\v{c}kovi\\'c, Petar, et al. \"Graph Attention Networks\". *ICLR* (2018).\n",
        "- Kipf, Thomas N., and Max Welling. \"Semi-Supervised Classification with Graph Convolutional Networks\". *ICLR* (2017).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
