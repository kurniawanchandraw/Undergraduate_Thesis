\chapter{KERANGKA METODOLOGIS DAN ANALISIS ASIMTOTIK \emph{GRAPH ATTENTION-BASED GEOGRAPHICALLY WEIGHTED REGRESSION}}
\label{chap:metodologi}

Pada bab ini diuraikan metode \emph{Graph Attention-based Geographically Weighted Regression} (GA-GWR), sebuah ekstensi dari model \emph{Geographically Weighted Regression} (GWR) yang menggunakan jaringan saraf graf untuk mempelajari fungsi \emph{kernel} spasial secara adaptif dengan struktur pembobotan berbasis konsep perhatian (\emph{attention}). Pembahasan dimulai dengan analisis keterbatasan pembobotan spasial berbasis \emph{kernel} konvensional, dilanjutkan dengan konstruksi \emph{kernel} terpelajari berbasis jaringan saraf graf, integrasi ke dalam kerangka GWR, serta analisis asimtotik dari estimator yang dihasilkan.

\section{Keterbatasan Pembobotan Spasial Berbasis \emph{Kernel} Konvensional}

Sebagaimana telah diuraikan pada Bab~\ref{chap:landasan-teori}, model GWR menggunakan pendekatan pembobotan spasial yang mengandalkan fungsi \emph{kernel} tetap (\emph{fixed kernel}) untuk menangkap struktur dependensi spasial dalam data. Meskipun pendekatan ini telah banyak digunakan dalam berbagai aplikasi, terdapat beberapa keterbatasan fundamental yang berdampak pada akurasi estimasi dan validitas inferensi statistik. Pada bagian ini dibahas prinsip pembobotan spasial dalam GWR klasik, sumber bias induktif yang muncul akibat penggunaan \emph{kernel} tetap, serta implikasinya terhadap inferensi statistik.

\subsection{Prinsip Pembobotan Spasial dalam GWR}

Model GWR menggunakan estimator koefisien lokal yang diperoleh melalui metode \emph{locally weighted least squares} (LWLS) dengan matriks bobot spasial $\mathbf{W}(u_0, v_0)$. Fungsi bobot pada model GWR ditentukan melalui fungsi \emph{kernel}, di mana bobot antara observasi $i$ dan lokasi target $(u_0, v_0)$ diberikan oleh
\begin{equation}
    w_i(u_0, v_0) = K\!\left( \frac{d_i(u_0, v_0)}{h} \right),
\end{equation}
dengan $d_i(u_0, v_0)$ adalah jarak antara $(u_i, v_i)$ dan $(u_0, v_0)$, $h > 0$ adalah parameter \emph{bandwidth}, dan $K(\cdot)$ adalah fungsi \emph{kernel} tetap seperti Gaussian atau \emph{bisquare} \citep{fan1996local}. Fungsi \emph{kernel} dengan karakteristik demikian selanjutnya disebut sebagai \emph{kernel} spasial konvensional. Fungsi \emph{kernel} $K: \mathbb{R}^+ \to \mathbb{R}^+$ dalam GWR harus memenuhi beberapa sifat matematis sesuai dengan Asumsi \ref{asumsi:gwr_kernel}.

Dari definisi di atas, terlihat bahwa \emph{kernel} konvensional memiliki dua karakteristik utama:
\begin{enumerate}[label=(\roman*)]
    \item Bobot spasial sepenuhnya ditentukan oleh jarak geografis antara lokasi.
    \item Parameter \emph{bandwidth} $h$ bersifat tetap untuk seluruh wilayah studi.
\end{enumerate}
Kedua karakteristik ini membawa implikasi penting terhadap fleksibilitas representasi struktur dependensi spasial. Kelemahan fundamental dari formulasi di atas adalah bahwa bentuk fungsional $K(\cdot)$ dan metrik $d_{ij}$ ditentukan secara \emph{a priori} (sebelum analisis data) dan bersifat kaku. Peneliti memaksakan asumsi bahwa interaksi spasial di dunia nyata mengikuti kurva lonceng Gaussian atau peluruhan polinomial Bi-square yang mulus. Dalam terminologi \emph{machine learning}, hal ini merupakan bentuk bias induktif (\emph{inductive bias}) yang sangat kuat.

\subsection{Bias Induktif pada \emph{Kernel} Tetap}

\emph{Kernel} spasial konvensional membawa asumsi struktural yang bersifat \emph{a priori}, terutama isotropi dan monotonisitas pengaruh terhadap jarak. Secara matematis, ini berarti $K(\|\mathbf{u}_i - \mathbf{u}_j\|)$ hanya bergantung pada magnitudo vektor jarak, bukan arahnya. Geometri dari kernel ini adalah lingkaran (dalam 2D) atau bola (dalam 3D). Implikasinya adalah bahwa pengaruh spasial meluruh dengan laju yang sama ke segala arah, yaitu Utara, Selatan, Timur, dan Barat.

\begin{definisi}[\textbf{Bias induktif}]
    Bias induktif adalah kecenderungan sistematis suatu metode estimasi yang disebabkan oleh pembatasan kelas fungsi atau struktur model yang diasumsikan sebelum data diamati.
\end{definisi}

Dalam konteks GWR, bias induktif muncul karena \emph{kernel} tetap membatasi bobot spasial pada fungsi jarak isotropik tertentu, sehingga tidak mampu merepresentasikan struktur spasial yang bersifat anisotropik, tidak simetris, atau bergantung pada konteks non-geometris \citep{yan2024anisotropic}.

Secara matematis, misalkan $\kappa^\star(\mathbf{u}_i, \mathbf{u}_0)$ menyatakan \emph{kernel} spasial sejati yang mendasari proses data. Penggunaan \emph{kernel} konvensional menyiratkan aproksimasi
\begin{equation}
    \kappa^\star(\mathbf{u}_i, \mathbf{u}_0) \approx K\!\left(\frac{\|\mathbf{u}_i - \mathbf{u}_0\|}{h}\right),
\end{equation}
dengan $K(\cdot)$ dipilih dari kelas fungsi terbatas. Jika $\kappa^\star$ tidak berada dalam kelas tersebut, maka kesalahan aproksimasi tidak dapat dieliminasi meskipun ukuran sampel meningkat.

Keterbatasan ini diperparah oleh penggunaan \emph{bandwidth} tunggal yang bersifat global, yang mengasumsikan bahwa seluruh hubungan spasial beroperasi pada skala yang sama, padahal banyak proses spasial bersifat multi-skala \citep{fotheringham2017multiscale}.

Parameter $d_{ij}$ dalam kernel GWR hampir secara eksklusif dihitung sebagai jarak Euclidean garis lurus. Asumsi ini mengimplikasikan ruang yang kontinu, homogen, dan bebas hambatan. Namun, ruang geografis penuh dengan diskontinuitas dan hambatan topologis. Sungai, pegunungan, atau batas administratif dapat memutus interaksi spasial antara dua titik yang secara Euclidean sangat dekat. Dalam GWR konvensional, dua rumah yang berseberangan di tepi sungai tanpa jembatan akan dianggap sebagai tetangga dekat dengan bobot interaksi tinggi ($w_{ij} \approx 1$). Padahal, dalam realitas pasar perumahan atau akses layanan, jarak efektif mereka (jarak jaringan) mungkin sangat jauh. Penggunaan metrik jarak Euclidean mengabaikan faktor-faktor ini, sehingga menghasilkan bobot spasial yang tidak akurat dan bias dalam estimasi koefisien lokal \citep{lu2014}. Dalam studi transportasi atau epidemiologi, interaksi terjadi melalui jaringan jalan atau rute penerbangan. Penelitian menunjukkan bahwa kalibrasi GWR menggunakan jarak jaringan menghasilkan goodness-of-fit yang jauh lebih baik daripada jarak Euclidean. Namun, integrasi jarak non-Euclidean ke dalam GWR konvensional memerlukan perhitungan matriks jarak eksternal yang berat secara komputasi dan statis (tidak dipelajari dari data) \citep{lu2014}. 

\section{Pembelajaran \emph{Kernel} Spasial dengan Jaringan Saraf Graf}

Untuk mengatasi keterbatasan pembobotan spasial berbasis \emph{kernel} konvensional, pendekatan pembelajaran mesin modern menawarkan mekanisme yang lebih fleksibel dalam mempelajari struktur dependensi spasial secara langsung dari data. Salah satu pendekatan yang menonjol adalah penggunaan jaringan saraf graf (\emph{Graph Neural Networks}, GNN), yang dirancang untuk memodelkan relasi antarunit pengamatan yang direpresentasikan sebagai graf \citep{kipf2017, wu2021}. Berbeda dengan \emph{kernel} spasial tetap yang ditentukan \emph{a priori}, GNN memungkinkan pembelajaran bobot spasial yang adaptif terhadap struktur graf dan atribut data, sehingga memperluas kelas fungsi \emph{kernel} yang dapat direpresentasikan secara \emph{data-driven}. Model GWR yang menggunakan GNN untuk mempelajari \emph{kernel} spasial selanjutnya disebut sebagai \emph{Graph Attention-based Geographically Weighted Regression} (GA-GWR).

\subsection{Representasi Graf untuk Data Spasial}

Misalkan tersedia sampel acak $\{(y_i, \mathbf{x}_i, \mathbf{u}_i)\}_{i=1}^n$, dengan $y_i \in \mathbb{R}$ adalah variabel respons, $\mathbf{x}_i \in \mathbb{R}^p$ adalah vektor kovariat, dan $\mathbf{u}_i = (u_i, v_i)^\top \in \mathbb{R}^2$ menyatakan koordinat lokasi spasial. Data spasial ini dapat direpresentasikan sebagai graf dengan mempertimbangkan struktur ketetanggaan geografis.

\begin{definisi}[\textbf{Graf spasial lokal}]
    Untuk suatu lokasi target $\mathbf{u}_0 \in \mathbb{R}^2$ dan \emph{bandwidth} $h > 0$, didefinisikan himpunan tetangga lokal
    \begin{equation}
        \mathcal{N}_h(\mathbf{u}_0) = \{ i : d(\*u_i, \*u_0) \le h \}.
    \end{equation}
    Graf spasial lokal didefinisikan sebagai $\mathcal{G}_h(\mathbf{u}_0) = (\mathcal{V}_h, \mathcal{E}_h)$, dengan $\mathcal{V}_h = \mathcal{N}_h(\mathbf{u}_0)$ adalah himpunan simpul dan $\mathcal{E}_h$ adalah himpunan sisi yang dibangun berdasarkan keterhubungan spasial atau kedekatan topologis antarsimpul dalam $\mathcal{V}_h$.
\end{definisi}

Setiap simpul $i \in \mathcal{V}_h$ diasosiasikan dengan vektor fitur yang merepresentasikan kovariat dan posisi relatif terhadap lokasi target. Salah satu pendekatan yang dapat digunakan adalah representasi kartesian, yaitu
\begin{equation}
    \mathbf{z}_i(\mathbf{u}_0) = \bigl(\mathbf{x}_i^\top, \bigl(\mathbf{u}_i - \mathbf{u}_0\bigr)^\top\bigr)^\top = \bigl(\mathbf{x}_i^\top, \Delta u_i, \Delta v_i\bigr)^\top,
\end{equation}
dengan $\Delta u_i = u_i - u_0$ dan $\Delta v_i = v_i - v_0$. Representasi ini secara ekuivalen dapat dinyatakan dalam koordinat polar sebagai
\begin{equation}
    \mathbf{u}_i - \mathbf{u}_0 = r_i (\cos\phi_i, \sin\phi_i)^\top,
\end{equation}
dengan $r_i = \|\mathbf{u}_i - \mathbf{u}_0\|$ adalah jarak dari lokasi target ke observasi $i$, dan $\phi_i = \arctan(\Delta v_i / \Delta u_i) \in [0, 2\pi)$ adalah sudut arah dari $\mathbf{u}_0$ menuju $\mathbf{u}_i$ relatif terhadap sumbu horizontal. Penggunaan posisi relatif $\mathbf{u}_i - \mathbf{u}_0$ alih-alih koordinat absolut $\mathbf{u}_i$ memiliki beberapa keunggulan berikut.

\begin{proposisi}
    Misalkan $\mathbf{t} \in \mathbb{R}^2$ adalah vektor translasi sembarang. Jika seluruh lokasi ditranslasikan, yaitu $\mathbf{u}_i' = \mathbf{u}_i + \mathbf{t}$ dan $\mathbf{u}_0' = \mathbf{u}_0 + \mathbf{t}$, maka vektor fitur spasial tidak berubah:
    \begin{equation}
        \mathbf{u}_i' - \mathbf{u}_0' = (\mathbf{u}_i + \mathbf{t}) - (\mathbf{u}_0 + \mathbf{t}) = \mathbf{u}_i - \mathbf{u}_0.
    \end{equation}
    Akibatnya, $\mathbf{z}_i(\mathbf{u}_0') = \mathbf{z}_i(\mathbf{u}_0)$, sehingga bobot GWR yang dihasilkan bersifat invarian terhadap translasi sistem koordinat.
\end{proposisi}

Invariansi translasi ini memastikan bahwa model dapat digeneralisasi ke lokasi target baru tanpa bergantung pada posisi absolut dalam sistem koordinat.

Posisi relatif $\mathbf{u}_i - \mathbf{u}_0 = (\Delta u_i, \Delta v_i)^\top$ menyimpan informasi arah dan jarak secara terpisah. Misalkan dua observasi $i$ dan $j$ memiliki jarak yang sama terhadap $\mathbf{u}_0$, yaitu $\|\mathbf{u}_i - \mathbf{u}_0\| = \|\mathbf{u}_j - \mathbf{u}_0\| = r$, tetapi berada pada arah yang berbeda:
\begin{equation}
    \mathbf{u}_i - \mathbf{u}_0 = r(\cos\phi_i, \sin\phi_i)^\top, \quad \mathbf{u}_j - \mathbf{u}_0 = r(\cos\phi_j, \sin\phi_j)^\top,
\end{equation}
dengan $\phi_i \neq \phi_j$. Sebab $\mathbf{z}_i(\mathbf{u}_0) \neq \mathbf{z}_j(\mathbf{u}_0)$, maka jaringan saraf graf dapat mempelajari pemetaan nilai yang berbeda meskipun jaraknya sama. Hal ini memungkinkan representasi struktur anisotropi, berbeda dengan \emph{kernel} isotropik konvensional yang hanya bergantung pada jarak $\|\mathbf{u}_i - \mathbf{u}_0\|$. Selain itu, representasi posisi relatif juga menyediakan informasi jarak serta orientasi yang esensial untuk menentukan bobot spasial adaptif.

\subsection{Fleksibilitas Metrik Jarak dalam Konstruksi Representasi}

Pembahasan sebelumnya menggunakan representasi kartesian $\mathbf{u}_i - \mathbf{u}_0$ yang implisit mengasumsikan jarak Euclidean. Namun, keunggulan pendekatan berbasis GNN adalah fleksibilitas dalam mengakomodasi metrik jarak alternatif yang dapat lebih sesuai dengan struktur data spasial sesungguhnya. Bagian ini membuktikan secara matematis bahwa berbagai metrik jarak dapat diintegrasikan ke dalam kerangka GA-GWR tanpa mengorbankan validitas teoretis.

\begin{proposisi}
    Misalkan $d(\cdot, \cdot): \mathbb{R}^2 \times \mathbb{R}^2 \to \mathbb{R}^+$ adalah metrik jarak sembarang yang memenuhi:
    \begin{enumerate}[label=(\roman*)]
        \item $d(\mathbf{u}, \mathbf{v}) \geq 0$ untuk semua $\mathbf{u}, \mathbf{v}$.
        \item $d(\mathbf{u}, \mathbf{v}) = 0 \iff \mathbf{u} = \mathbf{v}$.
        \item $d(\mathbf{u}, \mathbf{v}) = d(\mathbf{v}, \mathbf{u})$.
        \item $d(\mathbf{u}, \mathbf{w}) \leq d(\mathbf{u}, \mathbf{v}) + d(\mathbf{v}, \mathbf{w})$.
        \item $d(\mathbf{u} + \mathbf{t}, \mathbf{v} + \mathbf{t}) = d(\mathbf{u}, \mathbf{v})$ untuk semua $\mathbf{t} \in \mathbb{R}^2$ (\emph{translasi-invarian}).
    \end{enumerate}
    Ketetanggaan lokal dapat didefinisikan secara umum sebagai
    \begin{equation}
        \mathcal{N}_h^d(\mathbf{u}_0) = \{ i : d(\mathbf{u}_i, \mathbf{u}_0) \leq h \}.
    \end{equation}
\end{proposisi}

Beberapa contoh metrik jarak yang memenuhi sifat di atas adalah sebagai berikut.

\begin{enumerate}[label=(\alph*)]
    \item Jarak Euclidean, yaitu metrik standar pada ruang $\mathbb{R}^2$:
    \begin{equation}
        d_{\mathrm{Euc}}(\mathbf{u}, \mathbf{v}) = \sqrt{(u_1 - v_1)^2 + (u_2 - v_2)^2} = \|\mathbf{u} - \mathbf{v}\|_2.
    \end{equation}
    Metrik ini menghasilkan ketetanggaan berbentuk lingkaran dan bersifat \emph{isotropik} (sama ke segala arah).
    
    \item Jarak Manhattan, yaitu metrik norma-$L_1$:
    \begin{equation}
        d_{\mathrm{Man}}(\mathbf{u}, \mathbf{v}) = |u_1 - v_1| + |u_2 - v_2| = \|\mathbf{u} - \mathbf{v}\|_1.
    \end{equation}
    Metrik ini menghasilkan ketetanggaan berbentuk belah ketupat dan sesuai untuk lingkungan dengan struktur jalan grid ortogonal.
    
    \item Jarak Chebyshev, yaitu metrik norma-$L_\infty$:
    \begin{equation}
        d_{\mathrm{Cheb}}(\mathbf{u}, \mathbf{v}) = \max\{|u_1 - v_1|, |u_2 - v_2|\} = \|\mathbf{u} - \mathbf{v}\|_\infty.
    \end{equation}
    Metrik ini menghasilkan ketetanggaan berbentuk persegi dan berguna ketika perpindahan diagonal memiliki biaya sama dengan perpindahan ortogonal.
    
    \item Jarak Minkowski, generalisasi norma-$L_p$ untuk $p \geq 1$:
    \begin{equation}
        d_p(\mathbf{u}, \mathbf{v}) = \left(|u_1 - v_1|^p + |u_2 - v_2|^p\right)^{1/p}.
    \end{equation}
    Kasus khusus: $p = 1$ (Manhattan), $p = 2$ (Euclidean), $p \to \infty$ (Chebyshev).
    
    \item Jarak jaringan, yaitu metrik jarak pada graf jaringan $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ dengan bobot sisi $w_{ij}$ berupa
    \begin{equation}
        d_{\mathrm{net}}(\mathbf{u}, \mathbf{v}) = \min_{\text{path } P: \mathbf{u} \to \mathbf{v}} \sum_{(i,j) \in P} w_{ij},
    \end{equation}
    yaitu panjang jalur terdekat (\emph{shortest path}) antara $\mathbf{u}$ dan $\mathbf{v}$ dalam jaringan. Metrik ini memenuhi sifat (i)--(iv) secara ketat, dan sifat (v) terpenuhi secara efektif karena jarak hanya bergantung pada struktur topologis jaringan.
    
    \item Jarak Haversine, untuk koordinat geografis (\emph{latitude}, \emph{longitude}) pada permukaan bumi:
    \begin{align}
        d_{\mathrm{hav}}(\mathbf{u}, \mathbf{v}) = \\ 2R \arcsin\left(\sqrt{\sin^2\left(\frac{\phi_v - \phi_u}{2}\right) + \cos(\phi_u)\cos(\phi_v)\sin^2\left(\frac{\lambda_v - \lambda_u}{2}\right)}\right),
    \end{align}
    dengan $R$ adalah jari-jari bumi, $\phi$ adalah \emph{latitude}, dan $\lambda$ adalah \emph{longitude}. Metrik ini sesuai untuk data geografis berskala besar.
    
    \item Jarak Mahalanobis, yaitu metrik yang memperhitungkan struktur kovarians data:
    \begin{equation}
        d_{\mathrm{Mah}}(\mathbf{u}, \mathbf{v}; \boldsymbol{\Sigma}) = \sqrt{(\mathbf{u} - \mathbf{v})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{u} - \mathbf{v})},
    \end{equation}
    dengan $\boldsymbol{\Sigma}$ adalah matriks kovarians. Metrik ini menghasilkan ketetanggaan berbentuk elips dan secara eksplisit memodelkan \emph{anisotropi} spasial.
\end{enumerate}

Secara khusus, jarak jaringan $d_{\mathrm{net}}$ memenuhi sifat-sifat metrik sebagai berikut,
\begin{enumerate}[label=(\roman*)]
    \item Panjang \emph{shortest path} selalu $\geq 0$ karena bobot sisi non-negatif.
    \item $d_{\mathrm{net}}(\mathbf{u}, \mathbf{u}) = 0$ karena tidak ada sisi yang perlu dilalui.
    \item Untuk graf tak berarah, \emph{shortest path} $\mathbf{u} \to \mathbf{v}$ sama dengan $\mathbf{v} \to \mathbf{u}$.
    \item Jika ada jalur $\mathbf{u} \to \mathbf{w}$ melalui $\mathbf{v}$, maka $d_{\mathrm{net}}(\mathbf{u}, \mathbf{w}) \leq d_{\mathrm{net}}(\mathbf{u}, \mathbf{v}) + d_{\mathrm{net}}(\mathbf{v}, \mathbf{w})$ karena \emph{shortest path} optimal.
    \item Jarak jaringan bergantung pada topologi graf, bukan koordinat absolut. Dalam praktik, translasi koordinat tidak mengubah struktur jaringan sehingga jarak tetap sama.
\end{enumerate}

\subsection{Fungsi Skor Spasial dan Jaringan Saraf Graf}

Dalam kerangka ini, jaringan saraf graf digunakan untuk mempelajari fungsi skor yang merepresentasikan kekuatan interaksi spasial antarsimpul dalam graf lokal.

\begin{definisi}[\textbf{Fungsi skor spasial}]
    Fungsi skor spasial adalah pemetaan
    \begin{equation}
        s^\star : (\mathbf{z}_i(\mathbf{u}_0), \mathcal{G}_h(\mathbf{u}_0)) \mapsto \mathbb{R},
    \end{equation}
    yang merepresentasikan kontribusi relatif observasi ke-$i$ terhadap estimasi lokal di $\mathbf{u}_0$.
\end{definisi}

Fungsi $s^\star$ bersifat tidak diketahui dan dapat bergantung secara kompleks pada kovariat, posisi relatif, serta struktur graf lokal. Dalam analisis ini, fungsi skor sejati $s^\star$ diasumsikan kontinu pada domain kompak dan invarian terhadap permutasi simpul dalam graf lokal.

\begin{definisi}
    Diberikan graf lokal $\mathcal{G}_h(\mathbf{u}_0)$, jaringan saraf graf dengan parameter $\boldsymbol{\theta}$ mendefinisikan fungsi
    \begin{equation}
        s_{\boldsymbol{\theta}}(i, \mathbf{u}_0) = \mathrm{GNN}_{\boldsymbol{\theta}}\bigl(\mathcal{G}_h(\mathbf{u}_0), \mathbf{z}_i(\mathbf{u}_0)\bigr),
    \end{equation}
    yang menghasilkan skor untuk setiap simpul $i \in \mathcal{N}_h(\mathbf{u}_0)$.
\end{definisi}

Berdasarkan teorema~\ref{thm:uat_ann} untuk jaringan saraf graf, di bawah asumsi kontinuitas dan invariansi permutasi, untuk setiap $\varepsilon > 0$ terdapat parameter $\boldsymbol{\theta}^\star$ sedemikian sehingga
\begin{equation}
    \sup_{i \in \mathcal{N}_h(\mathbf{u}_0)} \bigl| s_{\boldsymbol{\theta}^\star}(i, \mathbf{u}_0) - s^\star(i, \mathbf{u}_0) \bigr| < \varepsilon.
\end{equation}
Persamaan di atas menunjukkan bahwa GNN dapat mengaproksimasi fungsi skor spasial sejati dengan presisi yang diinginkan, sehingga memungkinkan pembelajaran bobot spasial yang adaptif dan kompleks.

\subsection{Konstruksi \emph{Kernel} Spasial Berbasis GNN}

Fungsi skor yang dipelajari kemudian dipetakan menjadi bobot \emph{kernel} melalui normalisasi \emph{softmax}. Sebelum mendefinisikan \emph{kernel} terestimasi, perlu ditetapkan terlebih dahulu bahwa setiap \emph{kernel} konvensional dapat direpresentasikan secara ekuivalen dalam bentuk \emph{softmax}. Penggunaan normalisasi \emph{softmax} ini yang memberikan nama ``\emph{attention}" pada model GA-GWR karena mekanisme ini mirip dengan mekanisme perhatian dalam jaringan saraf.

Misalkan $w_i = K(d_i/h) > 0$ adalah bobot \emph{kernel} konvensional untuk observasi $i \in \mathcal{N}_h(\mathbf{u}_0)$. Matriks bobot diagonal ditulis sebagai $\mathbf{W} = \mathrm{diag}(w_1, \ldots, w_m)$ dengan $m = |\mathcal{N}_h(\mathbf{u}_0)|$. Didefinisikan bobot ternormalisasi sebagai
\begin{equation}
    \tilde{w}_i = \frac{w_i}{\sum_{j=1}^m w_j} = \frac{K(d_i/h)}{\sum_{j=1}^m K(d_j/h)},
\end{equation}
sehingga $\tilde{w}_i > 0$ dan $\sum_{i=1}^m \tilde{w}_i = 1$. Dengan demikian, $\{\tilde{w}_i\}_{i=1}^m$ membentuk distribusi probabilitas diskrit atas himpunan tetangga lokal.

\begin{proposisi}
    Estimator \emph{locally weighted least squares} bersifat invarian terhadap penskalaan bobot. Secara eksplisit, jika $\widetilde{\mathbf{W}} = \mathrm{diag}(\tilde{w}_1, \ldots, \tilde{w}_m)$ adalah matriks bobot ternormalisasi, maka
    \begin{equation}
        (\mathbf{X}^\top \widetilde{\mathbf{W}} \mathbf{X})^{-1} \mathbf{X}^\top \widetilde{\mathbf{W}} \mathbf{y} = (\mathbf{X}^\top \mathbf{W} \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{W} \mathbf{y}.
    \end{equation}
\end{proposisi}

\begin{proof}
    Sebab $\tilde{w}_i = w_i / \sum_j w_j$, maka $\widetilde{\mathbf{W}} = c^{-1} \mathbf{W}$ dengan $c = \sum_{j=1}^m w_j > 0$. Substitusi memberikan
    \begin{align}
        (\mathbf{X}^\top \widetilde{\mathbf{W}} \mathbf{X})^{-1} \mathbf{X}^\top \widetilde{\mathbf{W}} \mathbf{y} &= (c^{-1} \mathbf{X}^\top \mathbf{W} \mathbf{X})^{-1} (c^{-1} \mathbf{X}^\top \mathbf{W} \mathbf{y}) \nonumber \\
        &= c \cdot (\mathbf{X}^\top \mathbf{W} \mathbf{X})^{-1} \cdot c^{-1} \cdot \mathbf{X}^\top \mathbf{W} \mathbf{y} \nonumber \\
        &= (\mathbf{X}^\top \mathbf{W} \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{W} \mathbf{y}.
    \end{align}
    Dengan demikian, estimator koefisien lokal identik untuk bobot asli $\mathbf{W}$ dan bobot ternormalisasi $\widetilde{\mathbf{W}}$.
\end{proof}

Proposisi di atas menunjukkan bahwa normalisasi bobot tidak mengubah estimator koefisien lokal. Oleh karena itu, tanpa mengurangi keumuman, dapat diasumsikan bahwa \emph{kernel} sejati bersifat ternormalisasi, yaitu $\kappa^\star(i \mid \mathbf{u}_0) = \tilde{w}_i$ dengan $\sum_i \kappa^\star(i \mid \mathbf{u}_0) = 1$. Selanjutnya, dapat ditunjukkan bahwa setiap distribusi probabilitas diskrit dapat dinyatakan sebagai transformasi \emph{softmax}.

\begin{proposisi}
    Misalkan $\{\kappa^\star(i \mid \mathbf{u}_0)\}_{i=1}^m$ adalah distribusi probabilitas diskrit positif, yaitu $\kappa^\star(i \mid \mathbf{u}_0) > 0$ dan $\sum_{i=1}^m \kappa^\star(i \mid \mathbf{u}_0) = 1$. Maka terdapat fungsi skor $s^\star : \{1, \ldots, m\} \to \mathbb{R}$ sedemikian sehingga
    \begin{equation}
        \kappa^\star(i \mid \mathbf{u}_0) = \frac{\exp\{s^\star(i, \mathbf{u}_0)\}}{\sum_{j=1}^m \exp\{s^\star(j, \mathbf{u}_0)\}} = \mathrm{softmax}(\mathbf{s}^\star)_i.
    \end{equation}
\end{proposisi}

\begin{proof}
    Didefinisikan $s^\star(i, \mathbf{u}_0) = \log \kappa^\star(i \mid \mathbf{u}_0)$ untuk setiap $i \in \{1, \ldots, m\}$. Sebab $\kappa^\star(i \mid \mathbf{u}_0) > 0$, logaritma terdefinisi dengan baik dan $s^\star(i, \mathbf{u}_0) \in \mathbb{R}$. Verifikasi dilakukan dengan substitusi langsung, yaitu
    \begin{align}
        \frac{\exp\{s^\star(i, \mathbf{u}_0)\}}{\sum_{j=1}^m \exp\{s^\star(j, \mathbf{u}_0)\}} &= \frac{\exp\{\log \kappa^\star(i \mid \mathbf{u}_0)\}}{\sum_{j=1}^m \exp\{\log \kappa^\star(j \mid \mathbf{u}_0)\}} \nonumber \\
        &= \frac{\kappa^\star(i \mid \mathbf{u}_0)}{\sum_{j=1}^m \kappa^\star(j \mid \mathbf{u}_0)} \notag \\ &= \frac{\kappa^\star(i \mid \mathbf{u}_0)}{1} \notag \\ &= \kappa^\star(i \mid \mathbf{u}_0).
    \end{align}
    Perlu dicatat bahwa $s^\star$ bersifat unik hingga konstanta aditif, yaitu jika diketahui $\tilde{s}(i) = s^\star(i) + c$ untuk $c \in \mathbb{R}$ sembarang, maka
    \begin{equation}
        \frac{\exp\{\tilde{s}(i)\}}{\sum_j \exp\{\tilde{s}(j)\}} = \frac{e^c \exp\{s^\star(i)\}}{e^c \sum_j \exp\{s^\star(j)\}} = \frac{\exp\{s^\star(i)\}}{\sum_j \exp\{s^\star(j)\}},
    \end{equation}
    sehingga model dapat menggunakan vektor skor yang tidak perlu dinormalisasi secara eksplisit, meningkatkan fleksibilitas dalam desain arsitektur GNN tanpa mengorbankan interpretabilitas bobot kernel.
\end{proof}

Dengan menggabungkan kedua proposisi di atas, diperoleh rantai kesetaraan untuk \emph{kernel} konvensional, yaitu
\begin{equation}
    w_i = K\!\left(\frac{d_i}{h}\right) \quad \xrightarrow{\text{normalisasi}} \quad \tilde{w}_i = \frac{w_i}{\sum_j w_j} \quad \xrightarrow{\textit{log-transform}} \quad s^\star_i = \log \tilde{w}_i,
\end{equation}
sehingga secara eksplisit, fungsi skor yang merepresentasikan \emph{kernel} konvensional adalah
\begin{equation}
    s^\star(i, \mathbf{u}_0) = \log K\!\left(\frac{d_i}{h}\right) - \log \sum_{j=1}^m K\!\left(\frac{d_j}{h}\right).
\end{equation}
Sebab suku kedua konstan untuk semua $i$ dan \emph{softmax} invarian terhadap translasi konstan, fungsi skor dapat disederhanakan menjadi
\begin{equation}
    s^\star(i, \mathbf{u}_0) = \log K\!\left(\frac{\|\mathbf{u}_i - \mathbf{u}_0\|}{h}\right).
\end{equation}

\begin{definisi}[\textbf{\emph{Kernel} spasial terestimasi}]
    \emph{Kernel} spasial terestimasi berbasis GNN didefinisikan sebagai
    \begin{equation}
        \widehat{\kappa}_{\boldsymbol{\theta}}(i \mid \mathbf{u}_0) = \frac{\exp\{ s_{\boldsymbol{\theta}}(i, \mathbf{u}_0) \}}{\sum_{j \in \mathcal{N}_h(\mathbf{u}_0)} \exp\{ s_{\boldsymbol{\theta}}(j, \mathbf{u}_0) \}}.
    \end{equation}
\end{definisi}

\begin{proposisi}
    \emph{Kernel} terestimasi $\widehat{\kappa}_{\boldsymbol{\theta}}(\cdot \mid \mathbf{u}_0)$ memenuhi sifat stokastik baris (\emph{row stochastic}):
    \begin{enumerate}[label=(\roman*)]
        \item Non-negativitas: $\widehat{\kappa}_{\boldsymbol{\theta}}(i \mid \mathbf{u}_0) > 0$ untuk semua $i \in \mathcal{N}_h(\mathbf{u}_0)$.
        \item Normalisasi: $\displaystyle\sum_{i \in \mathcal{N}_h(\mathbf{u}_0)} \widehat{\kappa}_{\boldsymbol{\theta}}(i \mid \mathbf{u}_0) = 1$.
    \end{enumerate}
    Sifat ini merupakan konsekuensi langsung dari transformasi \emph{softmax} dan memungkinkan interpretasi $\widehat{\kappa}_{\boldsymbol{\theta}}(i \mid \mathbf{u}_0)$ sebagai probabilitas diskrit atas himpunan tetangga lokal.
\end{proposisi}

\begin{proof}
    Untuk setiap $i \in \mathcal{N}_h(\mathbf{u}_0)$, karena fungsi eksponensial bernilai positif untuk semua argumen real, maka $\exp\{s_{\boldsymbol{\theta}}(i, \mathbf{u}_0)\} > 0$. Akibatnya, $\widehat{\kappa}_{\boldsymbol{\theta}}(i \mid \mathbf{u}_0) > 0$, yang membuktikan sifat (i). Untuk sifat (ii), dengan menjumlahkan atas seluruh $i \in \mathcal{N}_h(\mathbf{u}_0)$ diperoleh
    \begin{align}
        \sum_{i \in \mathcal{N}_h(\mathbf{u}_0)} \widehat{\kappa}_{\boldsymbol{\theta}}(i \mid \mathbf{u}_0) &= \sum_{i \in \mathcal{N}_h(\mathbf{u}_0)} \frac{\exp\{ s_{\boldsymbol{\theta}}(i, \mathbf{u}_0) \}}{\sum_{j \in \mathcal{N}_h(\mathbf{u}_0)} \exp\{ s_{\boldsymbol{\theta}}(j, \mathbf{u}_0) \}} \notag \\ &= \frac{\sum_{i} \exp\{ s_{\boldsymbol{\theta}}(i, \mathbf{u}_0) \}}{\sum_{j} \exp\{ s_{\boldsymbol{\theta}}(j, \mathbf{u}_0) \}} \notag\\ &= 1.
    \end{align}
\end{proof}

Selanjutnya, ditunjukkan bahwa jika fungsi skor terestimasi mengaproksimasi fungsi skor sejati, maka \emph{kernel} terestimasi juga mengaproksimasi \emph{kernel} sejati.

\begin{teorema}
    Misalkan $|\mathcal{N}_h(\mathbf{u}_0)| = m$ adalah jumlah tetangga lokal. Jika fungsi skor terestimasi mengaproksimasi fungsi skor sejati secara \emph{uniform}, yaitu
    \begin{equation}
        \sup_{i \in \mathcal{N}_h(\mathbf{u}_0)} \bigl| s_{\boldsymbol{\theta}}(i, \mathbf{u}_0) - s^\star(i, \mathbf{u}_0) \bigr| \leq \delta
    \end{equation}
    untuk suatu $\delta > 0$, maka \emph{kernel} terestimasi mengaproksimasi \emph{kernel} sejati dengan batas galat
    \begin{equation}
        \sup_{i \in \mathcal{N}_h(\mathbf{u}_0)} \bigl| \widehat{\kappa}_{\boldsymbol{\theta}}(i \mid \mathbf{u}_0) - \kappa^\star(i \mid \mathbf{u}_0) \bigr| \leq 2\delta.
    \end{equation}
    Secara khusus, jika $\delta \to 0$, maka $\widehat{\kappa}_{\boldsymbol{\theta}} \to \kappa^\star$ secara \emph{uniform}.
\end{teorema}

\begin{proof}
    Didefinisikan $a_i = s_{\boldsymbol{\theta}}(i, \mathbf{u}_0)$ dan $b_i = s^\star(i, \mathbf{u}_0)$ untuk menyederhanakan notasi. Berdasarkan teorema~\ref{thm:uat_ann} pada skor, $|a_i - b_i| \leq \delta$ untuk semua $i \in \mathcal{N}_h(\mathbf{u}_0)$. Untuk setiap $i$, selisih \emph{kernel} dapat ditulis sebagai
    \begin{equation}
        \widehat{\kappa}_{\boldsymbol{\theta}}(i \mid \mathbf{u}_0) - \kappa^\star(i \mid \mathbf{u}_0) = \frac{e^{a_i}}{\sum_j e^{a_j}} - \frac{e^{b_i}}{\sum_j e^{b_j}}.
    \end{equation}
    
    Dengan menyamakan penyebut, diperoleh
    \begin{align}
        \widehat{\kappa}_{\boldsymbol{\theta}}(i \mid \mathbf{u}_0) - \kappa^\star(i \mid \mathbf{u}_0) &= \frac{e^{a_i} \sum_j e^{b_j} - e^{b_i} \sum_j e^{a_j}}{\left(\sum_j e^{a_j}\right)\left(\sum_j e^{b_j}\right)}.
    \end{align}
    
    Pembilang dapat diuraikan sebagai
    \begin{align}
        e^{a_i} \sum_j e^{b_j} - e^{b_i} \sum_j e^{a_j} &= \sum_j \left( e^{a_i + b_j} - e^{b_i + a_j} \right).
    \end{align}
    
    Untuk melanjutkan, diperlukan sifat Lipschitz dari fungsi \emph{softmax} yang dinyatakan dalam lema berikut.
    
    \begin{lemma} Fungsi $\mathrm{softmax}: \mathbb{R}^m \to \mathbb{R}^m$ yang didefinisikan oleh
    \begin{equation}
        [\mathrm{softmax}(\mathbf{a})]_i = \frac{e^{a_i}}{\sum_{j=1}^m e^{a_j}}
    \end{equation}
    memenuhi ketaksamaan Lipschitz, yaitu untuk setiap $\mathbf{a}, \mathbf{b} \in \mathbb{R}^m$,
    \begin{equation}
        \|\mathrm{softmax}(\mathbf{a}) - \mathrm{softmax}(\mathbf{b})\|_\infty \leq 2 \|\mathbf{a} - \mathbf{b}\|_\infty.
    \end{equation}
    \end{lemma}
    
    Kembali ke bukti utama, dengan mengidentifikasi $\mathbf{a} = (a_1, \ldots, a_m)^\top$ dengan $a_i = s_{\boldsymbol{\theta}}(i, \mathbf{u}_0)$ dan $\mathbf{b} = (b_1, \ldots, b_m)^\top$ dengan $b_i = s^\star(i, \mathbf{u}_0)$, serta menggunakan asumsi $\|\mathbf{a} - \mathbf{b}\|_\infty \leq \delta$, maka berdasarkan lema di atas diperoleh
    \begin{equation}
        \sup_{i \in \mathcal{N}_h(\mathbf{u}_0)} \bigl| \widehat{\kappa}_{\boldsymbol{\theta}}(i \mid \mathbf{u}_0) - \kappa^\star(i \mid \mathbf{u}_0) \bigr| = \|\mathrm{softmax}(\mathbf{a}) - \mathrm{softmax}(\mathbf{b})\|_\infty \leq 2\delta.
    \end{equation}
\end{proof}

Dengan demikian, penggunaan GNN memperluas kelas fungsi \emph{kernel} dari keluarga \emph{kernel} tetap menuju kelas \emph{kernel} adaptif yang dipelajari dari data, tanpa mengorbankan struktur lokalitas yang diperlukan untuk analisis asimtotik.


\section{Integrasi \emph{Kernel} Terestimasi ke dalam Kerangka GWR}

Pada bagian ini diuraikan integrasi \emph{kernel} spasial terestimasi berbasis GNN ke dalam kerangka inferensi GWR. Pembahasan mencakup definisi formal model, peran \emph{kernel} sebagai \emph{nuisance parameter}, permasalahan endogenitas yang muncul, serta skema \emph{cross-fitting} untuk memulihkan validitas inferensi.

\subsection{Definisi Model \emph{Graph Attention-based} GWR (GA-GWR)}

Model \emph{Graph Attention-based} (GA-GWR) mengintegrasikan \emph{kernel} spasial terestimasi ke dalam kerangka regresi terboboti geografis. Model struktural yang mendasari proses pembangkitan data diasumsikan sebagai berikut.

\begin{definisi}
    Misalkan $\{(y_i, \mathbf{x}_i, \mathbf{u}_i)\}_{i=1}^n$ adalah sampel acak dengan $y_i \in \mathbb{R}$, $\mathbf{x}_i \in \mathbb{R}^p$, dan $\mathbf{u}_i \in \mathbb{R}^2$. Model struktural diasumsikan sebagai
    \begin{equation}
        y_i = \mathbf{x}_i^\top \boldsymbol{\beta}(\mathbf{u}_i) + \varepsilon_i,
    \end{equation}
    dengan $\boldsymbol{\beta}: \mathbb{R}^2 \to \mathbb{R}^p$ adalah fungsi koefisien spasial yang halus dan $\varepsilon_i$ adalah galat acak dengan mean nol. Target inferensi adalah koefisien lokal $\boldsymbol{\beta}(\mathbf{u}_0)$ untuk suatu lokasi target tetap $\mathbf{u}_0 \in \mathbb{R}^2$.
\end{definisi}

Berbeda dengan GWR klasik yang menggunakan \emph{kernel} tetap $K(\cdot)$, pada GA-GWR bobot spasial diperoleh dari \emph{kernel} terestimasi berbasis GNN. Matriks bobot didefinisikan sebagai berikut.

\begin{definisi}
    Matriks bobot untuk lokasi target $\mathbf{u}_0$ didefinisikan sebagai
    \begin{equation}
        \mathbf{W}_{\boldsymbol{\theta}}(\mathbf{u}_0) = \mathrm{diag}\bigl(\widehat{\kappa}_{\boldsymbol{\theta}}(1 \mid \mathbf{u}_0), \ldots, \widehat{\kappa}_{\boldsymbol{\theta}}(n \mid \mathbf{u}_0)\bigr),
    \end{equation}
    dengan $\widehat{\kappa}_{\boldsymbol{\theta}}(i \mid \mathbf{u}_0) > 0$ jika $i \in \mathcal{N}_h(\mathbf{u}_0)$ dan $\widehat{\kappa}_{\boldsymbol{\theta}}(i \mid \mathbf{u}_0) = 0$ jika $i \notin \mathcal{N}_h(\mathbf{u}_0)$.
\end{definisi}

\begin{definisi}
    Estimator koefisien lokal pada lokasi target $\mathbf{u}_0$ didefinisikan sebagai
    \begin{equation}
        \widehat{\boldsymbol{\beta}}_{\boldsymbol{\theta}}(\mathbf{u}_0) = \bigl(\mathbf{X}^\top \mathbf{W}_{\boldsymbol{\theta}}(\mathbf{u}_0) \mathbf{X}\bigr)^{-1} \mathbf{X}^\top \mathbf{W}_{\boldsymbol{\theta}}(\mathbf{u}_0) \mathbf{y},
    \end{equation}
    dengan $\mathbf{X} = (\mathbf{x}_1, \ldots, \mathbf{x}_n)^\top$ adalah matriks desain dan $\mathbf{y} = (y_1, \ldots, y_n)^\top$ adalah vektor respons.
\end{definisi}

Estimator ini memiliki bentuk yang identik dengan estimator GWR klasik, tetapi dengan matriks bobot $\mathbf{W}_{\boldsymbol{\theta}}(\mathbf{u}_0)$ yang dihasilkan dari \emph{kernel} terestimasi. Perbedaan fundamental terletak pada sifat stokastik dari matriks bobot, yang membawa implikasi penting terhadap analisis asimtotik.

\subsection{\emph{Kernel} Terestimasi sebagai \emph{Nuisance Parameter}}

Dalam kerangka GA-GWR, parameter GNN $\boldsymbol{\theta}$ yang menghasilkan \emph{kernel} terestimasi berperan sebagai \emph{nuisance parameter}, yaitu parameter yang diperlukan untuk estimasi tetapi bukan target inferensi utama. Peran \emph{kernel} sebagai \emph{nuisance parameter} membawa beberapa implikasi penting sebagai berikut.
\begin{enumerate}[label=(\roman*)]
    \item Parameter $\boldsymbol{\theta}$ dapat berdimensi sangat tinggi (ribuan hingga jutaan parameter pada arsitektur GNN modern), jauh melebihi dimensi target $\boldsymbol{\beta}(\mathbf{u}_0) \in \mathbb{R}^p$.
    \item Estimasi dilakukan dalam dua tahap: pertama mengestimasi $\boldsymbol{\theta}$ melalui pelatihan GNN, kemudian menggunakan $\widehat{\boldsymbol{\theta}}$ untuk menghitung $\widehat{\boldsymbol{\beta}}_{\widehat{\boldsymbol{\theta}}}(\mathbf{u}_0)$.
    \item Estimasi $\boldsymbol{\theta}$ menambah sumber ketidakpastian yang harus diperhitungkan dalam inferensi.
\end{enumerate}

Kerangka teoretis untuk menangani \emph{nuisance parameter} berdimensi tinggi dalam konteks inferensi semiparametrik telah dikembangkan dalam literatur ekonometrika dan statistik, serta dinamakan \emph{double machine learning} atau \emph{debiased machine learning} \citep{chernozhukov2024}. Pendekatan ini akan diadaptasi untuk konteks GA-GWR pada bagian selanjutnya.

\subsection{Permasalahan Endogenitas dan Ketergantungan Data}

Ketika \emph{kernel} diestimasi dari data yang sama dengan data yang digunakan untuk mengestimasi koefisien lokal, muncul permasalahan endogenitas yang dapat menginvalidasi inferensi standar. Permasalahan ini bersumber dari ketergantungan parameter GNN $\widehat{\boldsymbol{\theta}}$ terhadap galat $\boldsymbol{\varepsilon}$ melalui fungsi kerugian, terlepas dari bentuk spesifik fungsi kerugian yang digunakan.

\begin{proposisi}
    Misalkan $\mathcal{L}: \Theta \times \mathbb{R}^n \to \mathbb{R}$ adalah fungsi kerugian sembarang yang memenuhi:
    \begin{enumerate}[label=(\alph*)]
        \item $\mathcal{L}(\boldsymbol{\theta}; \mathbf{y})$ bergantung pada vektor respons $\mathbf{y}$; dan
        \item $\widehat{\boldsymbol{\theta}} = \argmin_{\boldsymbol{\theta} \in \Theta} \mathcal{L}(\boldsymbol{\theta}; \mathbf{y})$ ada dan terukur.
    \end{enumerate}
    Jika parameter \emph{kernel} $\boldsymbol{\theta}$ diestimasi menggunakan seluruh data $\{(y_i, \mathbf{x}_i, \mathbf{u}_i)\}_{i=1}^n$, maka matriks bobot $\mathbf{W}_{\widehat{\boldsymbol{\theta}}}(\mathbf{u}_0)$ bergantung pada vektor galat $\boldsymbol{\varepsilon} = (\varepsilon_1, \ldots, \varepsilon_n)^\top$. Akibatnya, kondisi
    \begin{equation}
        \mathbb{E}\bigl[\mathbf{X}^\top \mathbf{W}_{\widehat{\boldsymbol{\theta}}}(\mathbf{u}_0) \boldsymbol{\varepsilon}\bigr] \neq \mathbf{0}
    \end{equation}
    dapat terjadi, yang melanggar Asumsi~\ref{asumsi:gwr_error} yang diperlukan untuk konsistensi estimator. Dengan kata lain, eksogenitas galat tidak terjamin jika \emph{kernel} diestimasi dari data yang sama.
\end{proposisi}

\begin{proof}
    Dari model struktural $y_i = \mathbf{x}_i^\top \boldsymbol{\beta}(\mathbf{u}_i) + \varepsilon_i$, vektor respons dapat ditulis sebagai
    \begin{equation}
        \mathbf{y} = \mathbf{f}(\mathbf{X}, \mathbf{U}) + \boldsymbol{\varepsilon},
    \end{equation}
    dengan $\mathbf{f}(\mathbf{X}, \mathbf{U}) = (\mathbf{x}_1^\top \boldsymbol{\beta}(\mathbf{u}_1), \ldots, \mathbf{x}_n^\top \boldsymbol{\beta}(\mathbf{u}_n))^\top$ adalah komponen deterministik. Sebab $\mathcal{L}(\boldsymbol{\theta}; \mathbf{y})$ bergantung pada $\mathbf{y}$ (kondisi (a)), maka
    \begin{equation}
        \mathcal{L}(\boldsymbol{\theta}; \mathbf{y}) = \mathcal{L}\bigl(\boldsymbol{\theta}; \mathbf{f}(\mathbf{X}, \mathbf{U}) + \boldsymbol{\varepsilon}\bigr) =: \widetilde{\mathcal{L}}(\boldsymbol{\theta}; \mathbf{X}, \mathbf{U}, \boldsymbol{\varepsilon})
    \end{equation}
    adalah fungsi eksplisit dari $\boldsymbol{\varepsilon}$.
    
    Selanjutnya, sebab $\widehat{\boldsymbol{\theta}} = \argmin_{\boldsymbol{\theta}} \widetilde{\mathcal{L}}(\boldsymbol{\theta}; \mathbf{X}, \mathbf{U}, \boldsymbol{\varepsilon})$ (kondisi (b)), maka $\widehat{\boldsymbol{\theta}}$ adalah fungsi terukur dari $(\mathbf{X}, \mathbf{U}, \boldsymbol{\varepsilon})$:
    \begin{equation}
        \widehat{\boldsymbol{\theta}} = g(\mathbf{X}, \mathbf{U}, \boldsymbol{\varepsilon})
    \end{equation}
    untuk suatu fungsi $g$ yang bergantung pada bentuk $\mathcal{L}$ dan prosedur optimisasi. Konsekuensinya, bobot $w_i = \widehat{\kappa}_{\widehat{\boldsymbol{\theta}}}(i \mid \mathbf{u}_0)$ juga bergantung pada $\boldsymbol{\varepsilon}$:
    \begin{equation}
        w_i = h_i(\mathbf{X}, \mathbf{U}, \boldsymbol{\varepsilon}).
    \end{equation}
    
    Ekspektasi suku galat tertimbang dapat didekomposisi menggunakan identitas kovarians $\mathbb{E}[AB] = \mathbb{E}[A]\mathbb{E}[B] + \mathrm{Cov}(A, B)$:
    \begin{align}
        \mathbb{E}[w_i \varepsilon_i] &= \mathbb{E}[w_i] \cdot \underbrace{\mathbb{E}[\varepsilon_i]}_{= 0} + \mathrm{Cov}(w_i, \varepsilon_i) = \mathrm{Cov}(w_i, \varepsilon_i).
    \end{align}
    Sebab $w_i = h_i(\mathbf{X}, \mathbf{U}, \boldsymbol{\varepsilon})$ bergantung pada $\varepsilon_i$ melalui rantai $\boldsymbol{\varepsilon} \to \widehat{\boldsymbol{\theta}} \to w_i$, maka dapat terjadi $\mathrm{Cov}(w_i, \varepsilon_i) \neq 0$. Dengan demikian, mungkin untuk:
    \begin{equation}
        \mathbb{E}\bigl[\mathbf{X}^\top \mathbf{W}_{\widehat{\boldsymbol{\theta}}} \boldsymbol{\varepsilon}\bigr] = \sum_{i=1}^n \mathbf{x}_i \cdot \mathrm{Cov}(w_i, \varepsilon_i) \neq \mathbf{0},
    \end{equation}
    sehingga eksogenitas tidak terjamin.
\end{proof}

Untuk memperjelas mekanisme kovarians secara kuantitatif, berikut adalah karakterisasi menggunakan ekspansi Taylor.

\begin{lemma}
    Misalkan $\widehat{\boldsymbol{\theta}}$ memiliki ekspansi Taylor di sekitar $\boldsymbol{\varepsilon} = \mathbf{0}$, yaitu
    \begin{equation}
        \widehat{\boldsymbol{\theta}}(\boldsymbol{\varepsilon}) = \widehat{\boldsymbol{\theta}}(\mathbf{0}) + \mathbf{J}_{\boldsymbol{\theta}} \boldsymbol{\varepsilon} + \mathcal{O}(\|\boldsymbol{\varepsilon}\|^2),
    \end{equation}
    dengan $\mathbf{J}_{\boldsymbol{\theta}} = \frac{\partial \widehat{\boldsymbol{\theta}}}{\partial \boldsymbol{\varepsilon}^\top}\big|_{\boldsymbol{\varepsilon} = \mathbf{0}}$ adalah matriks Jacobian, maka kovarians antara bobot dan galat adalah
    \begin{equation}
        \mathrm{Cov}(w_i, \varepsilon_j) = \sigma^2 \nabla_{\boldsymbol{\theta}} w_i^\top [\mathbf{J}_{\boldsymbol{\theta}}]_{:,j} + \mathcal{O}(\sigma^4),
    \end{equation}
    dengan $[\mathbf{J}_{\boldsymbol{\theta}}]_{:,j}$ adalah kolom ke-$j$ dari $\mathbf{J}_{\boldsymbol{\theta}}$.
\end{lemma}

\begin{proof}
    Ekspansi bobot di sekitar $\boldsymbol{\varepsilon} = \mathbf{0}$ menghasilkan
    \begin{equation}
        w_i(\boldsymbol{\varepsilon}) = w_i(\mathbf{0}) + \nabla_{\boldsymbol{\theta}} w_i^\top \cdot \mathbf{J}_{\boldsymbol{\theta}} \boldsymbol{\varepsilon} + \mathcal{O}(\|\boldsymbol{\varepsilon}\|^2).
    \end{equation}
    Sebab $\mathbb{E}[\varepsilon_j] = 0$ dan $\mathbb{E}[\boldsymbol{\varepsilon} \varepsilon_j] = \sigma^2 \mathbf{e}_j$ (dengan $\mathbf{e}_j$ adalah vektor unit ke-$j$), maka
    \begin{align}
        \mathrm{Cov}(w_i, \varepsilon_j) &= \mathbb{E}\bigl[(w_i - \mathbb{E}[w_i]) \varepsilon_j\bigr] \nonumber \\
        &= \mathbb{E}\bigl[(\nabla_{\boldsymbol{\theta}} w_i^\top \mathbf{J}_{\boldsymbol{\theta}} \boldsymbol{\varepsilon}) \varepsilon_j\bigr] + \mathcal{O}(\sigma^4) \nonumber \\
        &= \nabla_{\boldsymbol{\theta}} w_i^\top \mathbf{J}_{\boldsymbol{\theta}} \mathbb{E}[\boldsymbol{\varepsilon} \varepsilon_j] \nonumber \\
        &= \sigma^2 \nabla_{\boldsymbol{\theta}} w_i^\top [\mathbf{J}_{\boldsymbol{\theta}}]_{:,j}.
    \end{align}
\end{proof}

Lema di atas menunjukkan bahwa $\mathrm{Cov}(w_i, \varepsilon_j) \neq 0$ selama $\mathbf{J}_{\boldsymbol{\theta}} \neq \mathbf{0}$ (estimator $\widehat{\boldsymbol{\theta}}$ sensitif terhadap \emph{noise}) dan $\nabla_{\boldsymbol{\theta}} w_i \neq \mathbf{0}$ (bobot responsif terhadap parameter). Kedua kondisi ini hampir selalu terpenuhi dalam praktik karena:
\begin{enumerate}[label=(\roman*)]
    \item pelatihan GNN menggunakan respons $\mathbf{y}$ yang mengandung galat $\boldsymbol{\varepsilon}$;
    \item optimisasi menyesuaikan $\boldsymbol{\theta}$ untuk meminimalkan kerugian, sehingga $\widehat{\boldsymbol{\theta}}$ sensitif terhadap perubahan dalam $\mathbf{y}$ (dengan demikian terhadap $\boldsymbol{\varepsilon}$);
    \item bobot yang dihasilkan berkorelasi dengan galat, yaitu observasi dengan $\varepsilon_i$ besar cenderung diberi bobot yang disesuaikan untuk mengakomodasi nilai $y_i$ yang ekstrem; dan
    \item korelasi antara bobot dan galat merusak sifat \emph{mean-zero} dari suku galat tertimbang.
\end{enumerate}

Permasalahan ini analog dengan \emph{overfitting} dalam pembelajaran mesin, tetapi dengan konsekuensi yang lebih serius, bukan hanya prediksi yang buruk, tetapi \emph{inferensi yang tidak valid}. Uji statistik dan interval konfidensi yang dihasilkan tidak dapat dipercaya.

\subsection{Skema \emph{Cross-Fitting} untuk Inferensi Valid}

Untuk mengatasi permasalahan endogenitas, digunakan skema \emph{cross-fitting} yang memisahkan data untuk estimasi \emph{kernel} dan estimasi koefisien lokal. Pendekatan ini mengadaptasi teknik \emph{double/debiased machine learning} \citep{chernozhukov2024} ke konteks GWR.

\begin{definisi}[\textbf{Skema \emph{cross-fitting}}]
    Partisi himpunan indeks $\{1, \ldots, n\}$ menjadi $K$ \emph{fold} yang saling lepas, yaitu
    \begin{equation}
        \{1, \ldots, n\} = \mathcal{I}_1 \cup \mathcal{I}_2 \cup \cdots \cup \mathcal{I}_K, \quad \mathcal{I}_k \cap \mathcal{I}_\ell = \emptyset \text{ untuk } k \neq \ell.
    \end{equation}
    Untuk setiap \emph{fold} $k$, didefinisikan:
    \begin{enumerate}[label=(\roman*)]
        \item \emph{fold} pelatihan, yaitu $\mathcal{I}_{-k} = \{1, \ldots, n\} \setminus \mathcal{I}_k$; dan
        \item \emph{fold} estimasi, yaitu $\mathcal{I}_k$.
    \end{enumerate}
    Parameter GNN $\widehat{\boldsymbol{\theta}}^{(-k)}$ diestimasi hanya menggunakan data pada $\mathcal{I}_{-k}$, kemudian digunakan untuk menghitung bobot pada observasi di $\mathcal{I}_k$.
\end{definisi}

Untuk setiap \emph{fold} $k$, dapat didefinisikan estimator parsial yang hanya menggunakan observasi pada \emph{fold} tersebut.

\begin{definisi}
    Untuk setiap \emph{fold} $k \in \{1, \ldots, K\}$, estimator parsial didefinisikan sebagai
    \begin{equation}
        \widehat{\boldsymbol{\beta}}^{(k)}(\mathbf{u}_0) = \bigl(\mathbf{X}_k^\top \mathbf{W}^{(-k)}_k(\mathbf{u}_0) \mathbf{X}_k\bigr)^{-1} \mathbf{X}_k^\top \mathbf{W}^{(-k)}_k(\mathbf{u}_0) \mathbf{y}_k,
    \end{equation}
    dengan $\mathbf{X}_k$ dan $\mathbf{y}_k$ adalah matriks desain dan vektor respons untuk observasi di $\mathcal{I}_k$, serta $\mathbf{W}^{(-k)}_k(\mathbf{u}_0)$ adalah matriks bobot diagonal yang dihitung menggunakan parameter $\widehat{\boldsymbol{\theta}}^{(-k)}$.
\end{definisi}

Estimator parsial $\widehat{\boldsymbol{\beta}}^{(k)}(\mathbf{u}_0)$ bersifat valid dalam arti eksogenitas terpenuhi, namun hanya menggunakan sebagian kecil data (sekitar $n/K$ observasi). Untuk memanfaatkan seluruh data sambil mempertahankan validitas inferensi, estimator-estimator parsial digabungkan melalui skema rata-rata tertimbang.

\begin{teorema}
    Misalkan $\mathbf{H}_k(\mathbf{u}_0) = \mathbf{X}_k^\top \mathbf{W}^{(-k)}_k(\mathbf{u}_0) \mathbf{X}_k$ adalah matriks informasi lokal untuk \emph{fold} $k$. Estimator \emph{cross-fitted} yang didefinisikan sebagai rata-rata tertimbang dari estimator parsial
    \begin{equation}
        \widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0) = \Biggl(\sum_{k=1}^K \mathbf{H}_k(\mathbf{u}_0)\Biggr)^{-1} \Biggl(\sum_{k=1}^K \mathbf{H}_k(\mathbf{u}_0) \widehat{\boldsymbol{\beta}}^{(k)}(\mathbf{u}_0)\Biggr)
    \end{equation}
    ekuivalen dengan solusi masalah \emph{weighted least squares} gabungan
    \begin{equation}
        \widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0) = \argmin_{\boldsymbol{\beta} \in \mathbb{R}^p} \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i(\mathbf{u}_0) \bigl(y_i - \mathbf{x}_i^\top \boldsymbol{\beta}\bigr)^2,
    \end{equation}
    yang memiliki bentuk tertutup
    \begin{equation}
        \widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0) = \Biggl(\sum_{k=1}^K \mathbf{X}_k^\top \mathbf{W}^{(-k)}_k(\mathbf{u}_0) \mathbf{X}_k\Biggr)^{-1} \Biggl(\sum_{k=1}^K \mathbf{X}_k^\top \mathbf{W}^{(-k)}_k(\mathbf{u}_0) \mathbf{y}_k\Biggr).
    \end{equation}
\end{teorema}

\begin{proof}
    Didefinisikan fungsi objektif
    \begin{equation}
        L(\boldsymbol{\beta}) = \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i(\mathbf{u}_0) \bigl(y_i - \mathbf{x}_i^\top \boldsymbol{\beta}\bigr)^2.
    \end{equation}
    Sebab partisi $\{\mathcal{I}_1, \ldots, \mathcal{I}_K\}$ bersifat saling lepas dan lengkap, maka penjumlahan ganda di atas melingkupi seluruh observasi $i \in \{1, \ldots, n\}$, dengan setiap observasi $i$ diberi bobot $w^{(-k(i))}_i(\mathbf{u}_0)$ dengan $k(i)$ adalah \emph{fold} yang memuat $i$.
    
    Untuk memperoleh kondisi orde pertama, dihitung turunan parsial terhadap $\boldsymbol{\beta}$ sebagai berikut.
    \begin{align}
        \frac{\partial L}{\partial \boldsymbol{\beta}} &= -2 \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i(\mathbf{u}_0) \mathbf{x}_i \bigl(y_i - \mathbf{x}_i^\top \boldsymbol{\beta}\bigr).
    \end{align}
    Dengan menyamakan turunan ke nol dan menyusun ulang, diperoleh
    \begin{equation}
        \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i(\mathbf{u}_0) \mathbf{x}_i y_i = \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i(\mathbf{u}_0) \mathbf{x}_i \mathbf{x}_i^\top \widehat{\boldsymbol{\beta}}.
    \end{equation}
    Suku di ruas kiri dapat ditulis dalam notasi matriks sebagai
    \begin{equation}
        \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i(\mathbf{u}_0) \mathbf{x}_i y_i = \sum_{k=1}^K \mathbf{X}_k^\top \mathbf{W}^{(-k)}_k(\mathbf{u}_0) \mathbf{y}_k,
    \end{equation}
    dan suku di ruas kanan sebagai
    \begin{equation}
        \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i(\mathbf{u}_0) \mathbf{x}_i \mathbf{x}_i^\top = \sum_{k=1}^K \mathbf{X}_k^\top \mathbf{W}^{(-k)}_k(\mathbf{u}_0) \mathbf{X}_k.
    \end{equation}
    
    Dengan demikian, persamaan normal menjadi
    \begin{equation}
        \Biggl(\sum_{k=1}^K \mathbf{X}_k^\top \mathbf{W}^{(-k)}_k(\mathbf{u}_0) \mathbf{X}_k\Biggr) \widehat{\boldsymbol{\beta}} = \sum_{k=1}^K \mathbf{X}_k^\top \mathbf{W}^{(-k)}_k(\mathbf{u}_0) \mathbf{y}_k.
    \end{equation}
    Dengan asumsi bahwa matriks $\sum_{k=1}^K \mathbf{X}_k^\top \mathbf{W}^{(-k)}_k(\mathbf{u}_0) \mathbf{X}_k$ invertibel, solusinya adalah
    \begin{equation}
        \label{eq:cf_closedform}
        \widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0) = \Biggl(\sum_{k=1}^K \mathbf{X}_k^\top \mathbf{W}^{(-k)}_k(\mathbf{u}_0) \mathbf{X}_k\Biggr)^{-1} \Biggl(\sum_{k=1}^K \mathbf{X}_k^\top \mathbf{W}^{(-k)}_k(\mathbf{u}_0) \mathbf{y}_k\Biggr).
    \end{equation}
    
    Dari definisi estimator parsial, berlaku
    \begin{equation}
        \widehat{\boldsymbol{\beta}}^{(k)}(\mathbf{u}_0) = \mathbf{H}_k(\mathbf{u}_0)^{-1} \mathbf{X}_k^\top \mathbf{W}^{(-k)}_k(\mathbf{u}_0) \mathbf{y}_k,
    \end{equation}
    sehingga
    \begin{equation}
        \mathbf{X}_k^\top \mathbf{W}^{(-k)}_k(\mathbf{u}_0) \mathbf{y}_k = \mathbf{H}_k(\mathbf{u}_0) \widehat{\boldsymbol{\beta}}^{(k)}(\mathbf{u}_0).
    \end{equation}
    Substitusi ke Persamaan~\eqref{eq:cf_closedform} menghasilkan
    \begin{align}
        \widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0) &= \Biggl(\sum_{k=1}^K \mathbf{H}_k(\mathbf{u}_0)\Biggr)^{-1} \Biggl(\sum_{k=1}^K \mathbf{H}_k(\mathbf{u}_0) \widehat{\boldsymbol{\beta}}^{(k)}(\mathbf{u}_0)\Biggr).
    \end{align}
    
    Bentuk ini merupakan \emph{rata-rata tertimbang matriks} (\emph{matrix-weighted average}) dari estimator parsial, dengan bobot berupa matriks informasi lokal $\mathbf{H}_k(\mathbf{u}_0)$. 
\end{proof}

\noindent Perlu dicatat bahwa jika $K = 1$ (tanpa \emph{cross-fitting}), maka $\widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0) = \widehat{\boldsymbol{\beta}}^{(1)}(\mathbf{u}_0)$, yang merupakan estimator GA-GWR standar tanpa pemisahan data.

Skema \emph{cross-fitting} memulihkan eksogenitas melalui mekanisme berikut.

\begin{proposisi}
    \label{prop:cf_exogeneity}
    Untuk setiap \emph{fold} $k$, karena $\widehat{\boldsymbol{\theta}}^{(-k)}$ diestimasi tanpa menggunakan data pada $\mathcal{I}_k$, maka kondisional pada $\widehat{\boldsymbol{\theta}}^{(-k)}$ berlaku
    \begin{equation}
        \mathbb{E}\bigl[\mathbf{X}_k^\top \mathbf{W}^{(-k)}_k(\mathbf{u}_0) \boldsymbol{\varepsilon}_k \mid \widehat{\boldsymbol{\theta}}^{(-k)}\bigr] = \mathbf{0},
    \end{equation}
    dengan $\boldsymbol{\varepsilon}_k$ adalah vektor galat untuk observasi di $\mathcal{I}_k$. Hal ini karena $\mathbf{W}^{(-k)}_k(\mathbf{u}_0)$ independen dari $\boldsymbol{\varepsilon}_k$.
\end{proposisi}

\begin{proof}
    Langkah pertama adalah menunjukkan independensi antara bobot dan galat. Berdasarkan konstruksi \emph{cross-fitting}, parameter $\widehat{\boldsymbol{\theta}}^{(-k)}$ diestimasi menggunakan data pada \emph{fold} pelatihan $\mathcal{I}_{-k}$, sehingga $\widehat{\boldsymbol{\theta}}^{(-k)}$ merupakan fungsi terukur dari $\{(y_i, \mathbf{x}_i, \mathbf{u}_i) : i \in \mathcal{I}_{-k}\}$. Secara eksplisit, dapat ditulis
    \begin{equation}
        \widehat{\boldsymbol{\theta}}^{(-k)} = g\bigl(\{(y_i, \mathbf{x}_i, \mathbf{u}_i)\}_{i \in \mathcal{I}_{-k}}\bigr)
    \end{equation}
    untuk suatu fungsi terukur $g$. Sebab galat $\{\varepsilon_i\}_{i=1}^n$ diasumsikan independen antarobservasi, maka himpunan galat pada \emph{fold} pelatihan $\{\varepsilon_i : i \in \mathcal{I}_{-k}\}$ independen dari himpunan galat pada \emph{fold} estimasi $\{\varepsilon_i : i \in \mathcal{I}_k\}$. Sebab $\widehat{\boldsymbol{\theta}}^{(-k)}$ hanya bergantung pada data di $\mathcal{I}_{-k}$ dan matriks bobot $\mathbf{W}^{(-k)}_k(\mathbf{u}_0)$ merupakan fungsi deterministik dari $\widehat{\boldsymbol{\theta}}^{(-k)}$ dan lokasi $\{\mathbf{u}_i : i \in \mathcal{I}_k\}$, maka
    \begin{equation}
        \mathbf{W}^{(-k)}_k(\mathbf{u}_0) \perp\!\!\!\perp \boldsymbol{\varepsilon}_k \mid \{\mathbf{x}_i, \mathbf{u}_i\}_{i \in \mathcal{I}_k}.
    \end{equation}
    
    Matriks bobot $\mathbf{W}^{(-k)}_k(\mathbf{u}_0)$ berbentuk diagonal dengan elemen $w^{(-k)}_i(\mathbf{u}_0) = \widehat{\kappa}_{\widehat{\boldsymbol{\theta}}^{(-k)}}(i \mid \mathbf{u}_0)$ untuk $i \in \mathcal{I}_k$. Dengan demikian, produk $\mathbf{X}_k^\top \mathbf{W}^{(-k)}_k(\mathbf{u}_0) \boldsymbol{\varepsilon}_k$ dapat diuraikan sebagai
    \begin{align}
        \mathbf{X}_k^\top \mathbf{W}^{(-k)}_k(\mathbf{u}_0) \boldsymbol{\varepsilon}_k &= \begin{pmatrix} \mathbf{x}_{i_1} & \mathbf{x}_{i_2} & \cdots & \mathbf{x}_{i_{|k|}} \end{pmatrix} \begin{pmatrix} w^{(-k)}_{i_1} & & \\ & \ddots & \\ & & w^{(-k)}_{i_{|k|}} \end{pmatrix} \begin{pmatrix} \varepsilon_{i_1} \\ \vdots \\ \varepsilon_{i_{|k|}} \end{pmatrix} \nonumber \\
        &= \sum_{i \in \mathcal{I}_k} w^{(-k)}_i(\mathbf{u}_0) \mathbf{x}_i \varepsilon_i,
    \end{align}
    dengan $\{i_1, i_2, \ldots, i_{|k|}\}$ adalah enumerasi elemen-elemen $\mathcal{I}_k$. Dengan menggunakan linearitas ekspektasi kondisional dan independensi yang telah ditunjukkan, diperoleh
    \begin{align}
        \mathbb{E}\bigl[\mathbf{X}_k^\top \mathbf{W}^{(-k)}_k(\mathbf{u}_0) \boldsymbol{\varepsilon}_k \mid \widehat{\boldsymbol{\theta}}^{(-k)}\bigr] &= \mathbb{E}\Biggl[\sum_{i \in \mathcal{I}_k} w^{(-k)}_i(\mathbf{u}_0) \mathbf{x}_i \varepsilon_i \Biggm| \widehat{\boldsymbol{\theta}}^{(-k)}\Biggr] \nonumber \\
        &= \sum_{i \in \mathcal{I}_k} \mathbb{E}\bigl[w^{(-k)}_i(\mathbf{u}_0) \mathbf{x}_i \varepsilon_i \mid \widehat{\boldsymbol{\theta}}^{(-k)}\bigr]. \label{eq:3.3.34}
    \end{align}
    Sebab $w^{(-k)}_i(\mathbf{u}_0)$ merupakan fungsi deterministik dari $\widehat{\boldsymbol{\theta}}^{(-k)}$ dan $\mathbf{u}_i$, maka kondisional pada $\widehat{\boldsymbol{\theta}}^{(-k)}$, bobot $w^{(-k)}_i(\mathbf{u}_0)$ bersifat konstan. Dengan demikian, Persamaan~\eqref{eq:3.3.34} dapat ditulis ulang sebagai
    \begin{align}
        \sum_{i \in \mathcal{I}_k} \mathbb{E}\bigl[w^{(-k)}_i(\mathbf{u}_0) \mathbf{x}_i \varepsilon_i \mid \widehat{\boldsymbol{\theta}}^{(-k)}\bigr] &= \sum_{i \in \mathcal{I}_k} w^{(-k)}_i(\mathbf{u}_0) \, \mathbb{E}\bigl[\mathbf{x}_i \varepsilon_i \mid \widehat{\boldsymbol{\theta}}^{(-k)}\bigr].
    \end{align}
    Selanjutnya, karena $\widehat{\boldsymbol{\theta}}^{(-k)} \perp\!\!\!\perp (\mathbf{x}_i, \varepsilon_i)$ untuk setiap $i \in \mathcal{I}_k$, maka berlaku
    \begin{equation}
        \mathbb{E}\bigl[\mathbf{x}_i \varepsilon_i \mid \widehat{\boldsymbol{\theta}}^{(-k)}\bigr] = \mathbb{E}[\mathbf{x}_i \varepsilon_i].
    \end{equation}
    
    Dengan menggunakan hukum ekspektasi bersyarat (\emph{tower property}) dan asumsi eksogenitas $\mathbb{E}[\varepsilon_i \mid \mathbf{x}_i, \mathbf{u}_i] = 0$, diperoleh
    \begin{align}
        \mathbb{E}[\mathbf{x}_i \varepsilon_i] &= \mathbb{E}\bigl[\mathbb{E}[\mathbf{x}_i \varepsilon_i \mid \mathbf{x}_i, \mathbf{u}_i]\bigr] \nonumber \\
        &= \mathbb{E}\bigl[\mathbf{x}_i \, \mathbb{E}[\varepsilon_i \mid \mathbf{x}_i, \mathbf{u}_i]\bigr] \nonumber \\
        &= \mathbb{E}[\mathbf{x}_i \cdot 0] = \mathbf{0}.
    \end{align}
    Dengan menggabungkan hasil-hasil di atas, diperoleh
    \begin{equation}
        \mathbb{E}\bigl[\mathbf{X}_k^\top \mathbf{W}^{(-k)}_k(\mathbf{u}_0) \boldsymbol{\varepsilon}_k \mid \widehat{\boldsymbol{\theta}}^{(-k)}\bigr] = \sum_{i \in \mathcal{I}_k} w^{(-k)}_i(\mathbf{u}_0) \cdot \mathbf{0} = \mathbf{0}.
    \end{equation}
\end{proof}

Dengan pemulihan eksogenitas ini, hukum bilangan besar dapat diterapkan kembali dan konsistensi estimator dapat dijamin. Analisis asimtotik lengkap akan diuraikan pada bagian selanjutnya.


\section{Analisis Asimtotik Koefisien Lokal pada GA-GWR}

Bagian ini mengembangkan teori asimtotik untuk estimator \emph{cross-fitted} GA-GWR. Analisis mencakup dekomposisi bias-variansi, konsistensi, dan distribusi asimtotik koefisien lokal. Untuk menyederhanakan notasi, dependensi terhadap lokasi target $\mathbf{u}_0$ akan dihilangkan bila tidak menimbulkan ambiguitas.

\subsection{Asumsi-Asumsi Regularitas}

Analisis asimtotik memerlukan sejumlah asumsi regularitas yang mencakup struktur data, properti fungsi koefisien, serta karakteristik \emph{kernel} terestimasi. Asumsi-asumsi ini dikelompokkan menjadi tiga kategori, yaitu asumsi data dan model, asumsi kehalusan fungsi koefisien, serta asumsi lokalitas dan \emph{kernel}.

Asumsi data dan model menetapkan kondisi dasar pada observasi, galat, dan kovariat. Di bawah ini adalah keempat asumsi utama dalam kategori ini.

\begin{asumsi}{GA-GWR}[\textbf{Independensi}]
    \label{asumsi:gagwr_independensi}
    Observasi $\{(y_i, \mathbf{x}_i, \mathbf{u}_i)\}_{i=1}^n$ bersifat independen dan berdistribusi identik (\emph{i.i.d.}).
\end{asumsi}
\noindent Asumsi independensi diperlukan untuk penerapan hukum bilangan besar dan teorema limit pusat. Dalam konteks spasial, asumsi ini dapat direlaksasi menjadi dependensi spasial yang melemah (\emph{mixing conditions}), namun pembahasan tersebut berada di luar cakupan analisis ini.

\begin{asumsi}{GA-GWR}[\textbf{Eksogenitas Lokal}]
    \label{asumsi:gagwr_eksogenitas}
    Galat memenuhi kondisi ekspektasi kondisional nol, yaitu
    \begin{equation}
        \mathbb{E}[\varepsilon_i \mid \mathbf{x}_i, \mathbf{u}_i] = 0.
    \end{equation}
\end{asumsi}
\noindent Asumsi eksogenitas menjamin bahwa galat tidak berkorelasi dengan kovariat dan lokasi. Bersama dengan skema \emph{cross-fitting}, asumsi ini memastikan bahwa suku galat tertimbang memiliki ekspektasi nol.

\begin{asumsi}{GA-GWR}[\textbf{Homoskedastisitas Kondisional}]
    \label{asumsi:gagwr_homoskedastisitas}
    Variansi galat bersyarat bersifat konstan, yaitu
    \begin{equation}
        \mathrm{Var}(\varepsilon_i \mid \mathbf{x}_i, \mathbf{u}_i) = \sigma^2 < \infty.
    \end{equation}
\end{asumsi}
\noindent Asumsi homoskedastisitas menyederhanakan analisis variansi. Kasus heteroskedastisitas dapat ditangani dengan modifikasi pada estimator variansi, namun tidak mengubah hasil konsistensi.

\begin{asumsi}{GA-GWR}[\textbf{Momen Kovariat Terbatas}]
    \label{asumsi:gagwr_momen_kovariat}
    Kovariat memiliki momen kedua yang terbatas:
    \begin{equation}
        \mathbb{E}[\|\mathbf{x}_i\|^2] < \infty.
    \end{equation}
\end{asumsi}
\noindent Asumsi ini menjamin eksistensi dan keterbatasan matriks $\mathbf{X}^\top \mathbf{W} \mathbf{X}$ serta inversnya.

Asumsi selanjutnya berkaitan dengan kehalusan fungsi koefisien spasial. Asumsi ini penting untuk mengendalikan bias yang muncul akibat aproksimasi lokal. Berikut adalah asumsi kehalusan yang digunakan.

\begin{asumsi}{GA-GWR}[\textbf{Diferensiabilitas Fungsi Koefisien}]
    \label{asumsi:gagwr_diferensiabilitas}
    Fungsi koefisien $\boldsymbol{\beta}: \mathbb{R}^2 \to \mathbb{R}^p$ dua kali terdiferensiasikan kontinu di sekitar lokasi target $\mathbf{u}_0$. Secara eksplisit, terdapat lingkungan $\mathcal{B}_\delta(\mathbf{u}_0)$ sedemikian sehingga untuk setiap $\mathbf{u} \in \mathcal{B}_\delta(\mathbf{u}_0)$:
    \begin{align}
        \boldsymbol{\beta}(\mathbf{u}) &= \boldsymbol{\beta}(\mathbf{u}_0) + \nabla \boldsymbol{\beta}(\mathbf{u}_0)^\top (\mathbf{u} - \mathbf{u}_0) + \frac{1}{2} (\mathbf{u} - \mathbf{u}_0)^\top \mathbf{H}_{\boldsymbol{\beta}}(\mathbf{u}_0) (\mathbf{u} - \mathbf{u}_0) + \\ &o(\|\mathbf{u} - \mathbf{u}_0\|^2),
    \end{align}
    dengan $\nabla \boldsymbol{\beta}(\mathbf{u}_0) \in \mathbb{R}^{2 \times p}$ adalah matriks gradien dan $\mathbf{H}_{\boldsymbol{\beta}}(\mathbf{u}_0)$ adalah tensor Hessian.
\end{asumsi}
\noindent Kehalusan fungsi koefisien memungkinkan ekspansi Taylor yang menjadi dasar analisis bias. Orde kehalusan menentukan laju konvergensi bias terhadap nol.

Asumsi terakhir berkaitan dengan karakteristik \emph{kernel} terestimasi dan lokalitas estimasi. Asumsi-asumsi ini penting untuk mengendalikan bias dan variansi estimator. Berikut adalah asumsi-asumsi tersebut.

\begin{asumsi}{GA-GWR}[\textbf{\emph{Shrinking Neighborhood}}]
    \label{asumsi:gagwr_shrinking_neighborhood}
    Parameter \emph{bandwidth} $h = h_n$ memenuhi $h_n \to 0$ ketika $n \to \infty$.
\end{asumsi}
\noindent Asumsi ini menjamin bahwa observasi yang digunakan dalam estimasi lokal semakin terkonsentrasi di sekitar lokasi target, sehingga aproksimasi $\boldsymbol{\beta}(\mathbf{u}_i) \approx \boldsymbol{\beta}(\mathbf{u}_0)$ semakin akurat.

\begin{asumsi}{GA-GWR}[\textbf{Ukuran Sampel Lokal Divergen}]
    \label{asumsi:gagwr_ukuran_sampel_lokal}
    Jumlah observasi efektif di dalam \emph{neighborhood} divergen:
    \begin{equation}
        n h_n^{d_{\mathrm{eff}}} \to \infty \quad \text{ketika } n \to \infty,
    \end{equation}
    dengan $d_{\mathrm{eff}} \in (0, 2]$ adalah dimensi fraktal efektif dari metrik jarak yang digunakan.
\end{asumsi}
\noindent Untuk metrik Euclidean pada ruang homogen, $d_{\mathrm{eff}} = d = 2$. Untuk metrik non-Euclidean (misalnya jarak jaringan), $d_{\mathrm{eff}}$ dapat berbeda dan perlu diestimasi dari struktur data lokal.

\begin{proposisi}
    Dimensi fraktal efektif dapat diestimasi dari distribusi jumlah observasi dalam ball lokal. Untuk suatu lokasi target $\mathbf{u}_0$ dan rangkaian radius $h_1 < h_2 < \cdots < h_K$, didefinisikan
    \begin{equation}
        m_k = |\mathcal{N}_{h_k}^d(\mathbf{u}_0)| = \#\{i : d(\mathbf{u}_i, \mathbf{u}_0) \leq h_k\}.
    \end{equation}
    Dimensi fraktal diestimasi melalui regresi log-log:
    \begin{equation}
        \hat{d}_{\mathrm{eff}} = \frac{\sum_{k} (\log h_k - \bar{\log h})(\log m_k - \bar{\log m})}{\sum_k (\log h_k - \bar{\log h})^2},
    \end{equation}
    dengan bar menunjukkan rata-rata. Nilai $\hat{d}_{\mathrm{eff}}$ umumnya stabil untuk lokasi dalam wilayah yang relatif homogen.
\end{proposisi}

\begin{asumsi}{GA-GWR}[\textbf{Keterbatasan Bobot \emph{Kernel}}]
    \label{asumsi:gagwr_keterbatasan_bobot}
    Bobot \emph{kernel} terestimasi memenuhi kondisi keterbatasan:
    \begin{equation}
        \sup_{i \in \mathcal{N}_h(\mathbf{u}_0)} w_i(\mathbf{u}_0) \leq \frac{C}{n h^d}
    \end{equation}
    untuk suatu konstanta $C > 0$ yang tidak bergantung pada $n$.
\end{asumsi}
\noindent Asumsi ini mencegah satu observasi mendominasi estimator dan diperlukan untuk penerapan teorema limit pusat Lindeberg--Feller.

\begin{asumsi}{GA-GWR}[\textbf{Simetri Lokal Asimtotik}]
    \label{asumsi:gagwr_simetri_lokal}
    Bobot \emph{kernel} memenuhi kondisi simetri lokal, yaitu
    \begin{equation}
        \sum_{i=1}^n w_i(\mathbf{u}_0) (\mathbf{u}_i - \mathbf{u}_0) \xrightarrow{p} \mathbf{0}.
    \end{equation}
\end{asumsi}
\noindent Asumsi simetri lokal menjamin bahwa bobot tidak bias secara sistematis ke satu arah. Hal ini dicapai melalui desain arsitektur GNN yang menggunakan koordinat relatif $\mathbf{u}_i - \mathbf{u}_0$ sebagai input.

\begin{asumsi}{GA-GWR}[\textbf{Kontinuitas Bobot}]
    \label{asumsi:gagwr_kontinuitas_bobot}
    Fungsi bobot bersifat Lipschitz kontinu terhadap \emph{input}, yaitu
    \begin{equation}
        |w_i(\mathbf{u}_0; \mathbf{z}) - w_i(\mathbf{u}_0; \mathbf{z}')| \leq L \|\mathbf{z} - \mathbf{z}'\|
    \end{equation}
    untuk suatu konstanta Lipschitz $L > 0$.
\end{asumsi}
\noindent Kontinuitas diperlukan untuk penerapan hukum bilangan besar \emph{uniform} dan kontrol suku sisa dalam ekspansi Taylor.

\subsection{Analisis Bias Asimtotik Koefisien Lokal}

Analisis bias dimulai dengan dekomposisi estimator \emph{cross-fitted} ke dalam suku target, suku bias, dan suku galat.

\begin{teorema}
    \label{thm:gagwr_dekomposisi_est}
    Estimator \emph{cross-fitted} GA-GWR dapat didekomposisikan sebagai
    \begin{equation}
        \widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0) = \boldsymbol{\beta}(\mathbf{u}_0) + \mathbf{B}_n(\mathbf{u}_0) + \mathbf{V}_n(\mathbf{u}_0),
    \end{equation}
    dengan suku bias
    \begin{equation}
        \mathbf{B}_n(\mathbf{u}_0) = \Biggl(\sum_{k=1}^K \mathbf{H}_k\Biggr)^{-1} \Biggl(\sum_{k=1}^K \mathbf{X}_k^\top \mathbf{W}^{(-k)}_k \mathbf{X}_k \bigl(\boldsymbol{\beta}(\mathbf{U}_k) - \boldsymbol{\beta}(\mathbf{u}_0)\bigr)\Biggr)
    \end{equation}
    dan suku galat (\emph{noise term})
    \begin{equation}
        \mathbf{V}_n(\mathbf{u}_0) = \Biggl(\sum_{k=1}^K \mathbf{H}_k\Biggr)^{-1} \Biggl(\sum_{k=1}^K \mathbf{X}_k^\top \mathbf{W}^{(-k)}_k \boldsymbol{\varepsilon}_k\Biggr),
    \end{equation}
    dengan $\boldsymbol{\beta}(\mathbf{U}_k) - \boldsymbol{\beta}(\mathbf{u}_0)$ adalah vektor yang elemen ke-$i$-nya (untuk $i \in \mathcal{I}_k$) adalah $\boldsymbol{\beta}(\mathbf{u}_i) - \boldsymbol{\beta}(\mathbf{u}_0)$.
\end{teorema}

\begin{proof}
    Dari model struktural, untuk setiap observasi $i \in \mathcal{I}_k$ berlaku
    \begin{equation}
        y_i = \mathbf{x}_i^\top \boldsymbol{\beta}(\mathbf{u}_i) + \varepsilon_i.
    \end{equation}
    Dengan menambah dan mengurangi $\mathbf{x}_i^\top \boldsymbol{\beta}(\mathbf{u}_0)$, diperoleh
    \begin{equation}
        y_i = \mathbf{x}_i^\top \boldsymbol{\beta}(\mathbf{u}_0) + \mathbf{x}_i^\top \bigl(\boldsymbol{\beta}(\mathbf{u}_i) - \boldsymbol{\beta}(\mathbf{u}_0)\bigr) + \varepsilon_i.
    \end{equation}
    Dalam notasi matriks untuk \emph{fold} $k$:
    \begin{equation}
        \mathbf{y}_k = \mathbf{X}_k \boldsymbol{\beta}(\mathbf{u}_0) + \mathbf{X}_k \bigl(\boldsymbol{\beta}(\mathbf{U}_k) - \boldsymbol{\beta}(\mathbf{u}_0)\bigr) + \boldsymbol{\varepsilon}_k.
    \end{equation}
    Substitusi ke estimator \emph{cross-fitted}:
    \begin{align}
        \widehat{\boldsymbol{\beta}}_{\mathrm{CF}} &= \Biggl(\sum_{k=1}^K \mathbf{H}_k\Biggr)^{-1} \Biggl(\sum_{k=1}^K \mathbf{X}_k^\top \mathbf{W}^{(-k)}_k \mathbf{y}_k\Biggr) \nonumber \\
        &= \Biggl(\sum_{k=1}^K \mathbf{H}_k\Biggr)^{-1} \Biggl(\sum_{k=1}^K \mathbf{X}_k^\top \mathbf{W}^{(-k)}_k \Bigl[\mathbf{X}_k \boldsymbol{\beta}(\mathbf{u}_0) + \mathbf{X}_k \bigl(\boldsymbol{\beta}(\mathbf{U}_k) - \boldsymbol{\beta}(\mathbf{u}_0)\bigr) + \boldsymbol{\varepsilon}_k\Bigr]\Biggr).
    \end{align}
    Dengan linearitas penjumlahan dan fakta bahwa $\mathbf{X}_k^\top \mathbf{W}^{(-k)}_k \mathbf{X}_k = \mathbf{H}_k$, diperoleh
    \begin{align}
        \widehat{\boldsymbol{\beta}}_{\mathrm{CF}} &= \Biggl(\sum_{k=1}^K \mathbf{H}_k\Biggr)^{-1} \Biggl(\sum_{k=1}^K \mathbf{H}_k\Biggr) \boldsymbol{\beta}(\mathbf{u}_0) \nonumber \\
        &\quad + \Biggl(\sum_{k=1}^K \mathbf{H}_k\Biggr)^{-1} \Biggl(\sum_{k=1}^K \mathbf{X}_k^\top \mathbf{W}^{(-k)}_k \mathbf{X}_k \bigl(\boldsymbol{\beta}(\mathbf{U}_k) - \boldsymbol{\beta}(\mathbf{u}_0)\bigr)\Biggr) \nonumber \\
        &\quad + \Biggl(\sum_{k=1}^K \mathbf{H}_k\Biggr)^{-1} \Biggl(\sum_{k=1}^K \mathbf{X}_k^\top \mathbf{W}^{(-k)}_k \boldsymbol{\varepsilon}_k\Biggr) \nonumber \\
        &= \boldsymbol{\beta}(\mathbf{u}_0) + \mathbf{B}_n(\mathbf{u}_0) + \mathbf{V}_n(\mathbf{u}_0).
    \end{align}
\end{proof}

Untuk menganalisis perilaku asimtotik suku bias, digunakan ekspansi Taylor dari fungsi koefisien.

\begin{proposisi}
    Di bawah Asumsi~\ref{asumsi:gagwr_diferensiabilitas}, untuk setiap $\mathbf{u}_i \in \mathcal{N}_h(\mathbf{u}_0)$:
    \begin{equation}
        \boldsymbol{\beta}(\mathbf{u}_i) - \boldsymbol{\beta}(\mathbf{u}_0) = \nabla \boldsymbol{\beta}(\mathbf{u}_0)^\top (\mathbf{u}_i - \mathbf{u}_0) + \mathbf{R}_i,
    \end{equation}
    dengan suku sisa $\mathbf{R}_i$ memenuhi $\|\mathbf{R}_i\| \leq C_\beta \|\mathbf{u}_i - \mathbf{u}_0\|^2$ untuk suatu konstanta $C_\beta > 0$.
\end{proposisi}

Dengan ekspansi ini, suku bias dapat diuraikan menjadi dua komponen.

\begin{teorema}
    Suku bias dapat ditulis sebagai
    \begin{equation}
        \mathbf{B}_n(\mathbf{u}_0) = \mathbf{B}_n^{(1)}(\mathbf{u}_0) + \mathbf{B}_n^{(2)}(\mathbf{u}_0),
    \end{equation}
    dengan suku bias orde pertama
    \begin{equation}
        \mathbf{B}_n^{(1)}(\mathbf{u}_0) = \Biggl(\sum_{k=1}^K \mathbf{H}_k\Biggr)^{-1} \Biggl(\sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i \mathbf{x}_i \mathbf{x}_i^\top \nabla \boldsymbol{\beta}(\mathbf{u}_0)^\top (\mathbf{u}_i - \mathbf{u}_0)\Biggr)
    \end{equation}
    dan suku bias orde kedua
    \begin{equation}
        \mathbf{B}_n^{(2)}(\mathbf{u}_0) = \Biggl(\sum_{k=1}^K \mathbf{H}_k\Biggr)^{-1} \Biggl(\sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i \mathbf{x}_i \mathbf{x}_i^\top \mathbf{R}_i\Biggr).
    \end{equation}
\end{teorema}

\begin{proof}
    Dengan mensubstitusikan ekspansi Taylor ke dalam ekspresi bias:
    \begin{align}
        & &&\sum_{k=1}^K \mathbf{X}_k^\top \mathbf{W}^{(-k)}_k \mathbf{X}_k \bigl(\boldsymbol{\beta}(\mathbf{U}_k) - \boldsymbol{\beta}(\mathbf{u}_0)\bigr)\nonumber\\ &= &&\sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i \mathbf{x}_i \mathbf{x}_i^\top \bigl(\boldsymbol{\beta}(\mathbf{u}_i) - \boldsymbol{\beta}(\mathbf{u}_0)\bigr) \nonumber \\
        &= &&\sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i \mathbf{x}_i \mathbf{x}_i^\top \Bigl[\nabla \boldsymbol{\beta}(\mathbf{u}_0)^\top (\mathbf{u}_i - \mathbf{u}_0) + \mathbf{R}_i\Bigr].
    \end{align}
    Pemisahan menjadi dua suku memberikan hasil yang diklaim.
\end{proof}

\begin{proposisi}
    \label{prop:gagwr_bias_orde_pertama}
    Di bawah Asumsi~\ref{asumsi:gagwr_simetri_lokal}, suku bias orde pertama memenuhi
    \begin{equation}
        \mathbf{B}_n^{(1)}(\mathbf{u}_0) \xrightarrow{p} \mathbf{0}.
    \end{equation}
\end{proposisi}

\begin{proof}
    Perhatikan bahwa
    \begin{align}
        &\sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i \mathbf{x}_i \mathbf{x}_i^\top \nabla \boldsymbol{\beta}(\mathbf{u}_0)^\top (\mathbf{u}_i - \mathbf{u}_0) \noindent\\ &= \Biggl(\sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i \mathbf{x}_i \mathbf{x}_i^\top\Biggr) \nabla \boldsymbol{\beta}(\mathbf{u}_0)^\top \Biggl(\sum_{j=1}^n \tilde{w}_j (\mathbf{u}_j - \mathbf{u}_0)\Biggr),
    \end{align}
    dengan $\tilde{w}_j$ adalah bobot yang sesuai untuk observasi $j$. Berdasarkan Asumsi~\ref{asumsi:gagwr_simetri_lokal}, suku $\sum_j \tilde{w}_j (\mathbf{u}_j - \mathbf{u}_0) \xrightarrow{p} \mathbf{0}$, sehingga $\mathbf{B}_n^{(1)}(\mathbf{u}_0) \xrightarrow{p} \mathbf{0}$.
\end{proof}

\begin{teorema}
    \label{thm:gagwr_laju_bias}
    Di bawah Asumsi \ref{asumsi:gagwr_diferensiabilitas}--\ref{asumsi:gagwr_simetri_lokal}, suku bias memenuhi
    \begin{equation}
        \mathbf{B}_n(\mathbf{u}_0) = \mathcal{O}_p(h^2).
    \end{equation}
    Secara khusus, karena $h \to 0$, maka $\mathbf{B}_n(\mathbf{u}_0) \xrightarrow{p} \mathbf{0}$.
\end{teorema}

\begin{proof}
    Dari Proposisi~\ref{prop:gagwr_bias_orde_pertama}, $\mathbf{B}_n^{(1)}(\mathbf{u}_0) \xrightarrow{p} \mathbf{0}$. Untuk suku bias orde kedua, diperhatikan bahwa
    \begin{align}
        \Biggl\|\sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i \mathbf{x}_i \mathbf{x}_i^\top \mathbf{R}_i\Biggr\| &\leq \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i \|\mathbf{x}_i\|^2 \|\mathbf{R}_i\| \nonumber \\
        &\leq C_\beta \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i \|\mathbf{x}_i\|^2 \|\mathbf{u}_i - \mathbf{u}_0\|^2.
    \end{align}
    Sebab $\mathbf{u}_i \in \mathcal{N}_h(\mathbf{u}_0)$ menyiratkan $\|\mathbf{u}_i - \mathbf{u}_0\| \leq h$, maka
    \begin{equation}
        \|\mathbf{u}_i - \mathbf{u}_0\|^2 \leq h^2.
    \end{equation}
    
    Dengan demikian,
    \begin{align}
        \Biggl\|\sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i \mathbf{x}_i \mathbf{x}_i^\top \mathbf{R}_i\Biggr\| &\leq C_\beta h^2 \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i \|\mathbf{x}_i\|^2.
    \end{align}
    Sebab bobot ternormalisasi $\left(\sum_i w_i = 1\right)$ dan $\mathbb{E}[\|\mathbf{x}_i\|^2] < \infty$, maka suku terakhir terbatas dalam probabilitas. Dengan mengalikan dengan $\bigl(\sum_k \mathbf{H}_k\bigr)^{-1}$ yang juga terbatas, diperoleh
    \begin{equation}
        \|\mathbf{B}_n^{(2)}(\mathbf{u}_0)\| = \mathcal{O}_p(h^2).
    \end{equation}
    Dengan demikian, $\mathbf{B}_n(\mathbf{u}_0) = \mathbf{B}_n^{(1)}(\mathbf{u}_0) + \mathbf{B}_n^{(2)}(\mathbf{u}_0) = o_p(1) + \mathcal{O}_p(h^2) = \mathcal{O}_p(h^2)$.
\end{proof}

Hasil $\mathbf{B}_n(\mathbf{u}_0) = \mathcal{O}_p(h^2)$ berlaku secara umum untuk setiap metrik jarak yang memenuhi Asumsi \ref{asumsi:gagwr_independensi}--\ref{asumsi:gagwr_kontinuitas_bobot}. Secara khusus:
\begin{enumerate}[label=(\roman*)]
    \item Jika fungsi koefisien $\boldsymbol{\beta}(\cdot)$ bersifat dua kali terdiferensiasikan kontinu (Asumsi~\ref{asumsi:gagwr_diferensiabilitas}), maka ekspansi Taylor orde dua
    \begin{equation}
        \boldsymbol{\beta}(\mathbf{u}_i) - \boldsymbol{\beta}(\mathbf{u}_0) = \mathcal{O}(\|\mathbf{u}_i - \mathbf{u}_0\|^2)
    \end{equation}
    tetap berlaku tanpa memperhatikan metrik yang digunakan, asalkan observasi dalam $\mathcal{N}_h^d(\mathbf{u}_0)$ memenuhi $\|\mathbf{u}_i - \mathbf{u}_0\| = \mathcal{O}(h)$ dalam norma Euclidean lokal.
    
    \item Untuk metrik yang sangat berbeda dari Euclidean (misalnya jarak jaringan dengan $d_{\mathrm{eff}} \ll 2$), meskipun laju bias tetap $\mathcal{O}_p(h^2)$ dalam parameter $h$, interpretasi geometris berubah. $h$ sekarang menyatakan radius dalam metrik non-Euclidean, bukan Euclidean.
    
    \item Orde bias tidak bergantung pada metrik, melainkan pada kehalusan fungsi $\boldsymbol{\beta}$ berdasarkan Asumsi~\ref{asumsi:gagwr_diferensiabilitas} dan Asumsi~\ref{asumsi:gagwr_simetri_lokal}.
\end{enumerate}

\begin{akibat}
    Estimator \emph{cross-fitted} GA-GWR bersifat \emph{asymptotically unbiased}, yaitu
    \begin{equation}
        \mathbb{E}\bigl[\widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0)\bigr] - \boldsymbol{\beta}(\mathbf{u}_0) = \mathcal{O}(h^2) \to 0 \quad \text{ketika } n\to\infty \text{ dan } h \to 0.
    \end{equation}
\end{akibat}

\subsection{Analisis Bias Asimtotik Variansi Galat}

Selain estimasi koefisien lokal, diperlukan pula estimasi variansi galat $\sigma^2$ untuk konstruksi interval konfidensi dan uji statistik.

\begin{teorema}
    Estimator variansi galat \emph{cross-fitted} yang diperoleh dari minimisasi fungsi \emph{log-likelihood} tertimbang Gaussian diberikan oleh
    \begin{equation}
        \widehat{\sigma}^2_{\mathrm{CF}}(\mathbf{u}_0) = \frac{\displaystyle\sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i(\mathbf{u}_0) \bigl(y_i - \mathbf{x}_i^\top \widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0)\bigr)^2}{\displaystyle\sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i(\mathbf{u}_0) - p},
    \end{equation}
    dengan $p$ adalah jumlah parameter (dimensi $\boldsymbol{\beta}$).
\end{teorema}

\begin{proof}
    Diasumsikan galat berdistribusi normal $\varepsilon_i \mid \mathbf{x}_i, \mathbf{u}_i \sim \mathcal{N}(0, \sigma^2)$. Fungsi \emph{log-likelihood} tertimbang lokal untuk lokasi $\mathbf{u}_0$ diberikan oleh
    \begin{equation}
        \ell(\boldsymbol{\beta}, \sigma^2; \mathbf{u}_0) = -\frac{1}{2} \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i(\mathbf{u}_0) \left[ \log(2\pi\sigma^2) + \frac{(y_i - \mathbf{x}_i^\top \boldsymbol{\beta})^2}{\sigma^2} \right].
    \end{equation}
    Dengan mendefinisikan $W_{\mathrm{tot}} = \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i(\mathbf{u}_0)$ dan $$\mathrm{RSS}_w = \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i(\mathbf{u}_0) (y_i - \mathbf{x}_i^\top \boldsymbol{\beta})^2,$$ fungsi \emph{log-likelihood} dapat ditulis sebagai
    \begin{equation}
        \ell(\boldsymbol{\beta}, \sigma^2; \mathbf{u}_0) = -\frac{W_{\mathrm{tot}}}{2} \log(2\pi) - \frac{W_{\mathrm{tot}}}{2} \log(\sigma^2) - \frac{\mathrm{RSS}_w}{2\sigma^2}.
    \end{equation}
    Untuk memperoleh estimator $\sigma^2$, dihitung turunan parsial terhadap $\sigma^2$ dan disamakan dengan nol:
    \begin{equation}
        \frac{\partial \ell}{\partial \sigma^2} = -\frac{W_{\mathrm{tot}}}{2\sigma^2} + \frac{\mathrm{RSS}_w}{2(\sigma^2)^2} = 0.
    \end{equation}
    Dengan mengalikan kedua ruas dengan $2(\sigma^2)^2$ dan menyusun ulang, diperoleh
    \begin{equation}
        \mathrm{RSS}_w = W_{\mathrm{tot}} \cdot \sigma^2,
    \end{equation}
    sehingga estimator \emph{maximum likelihood} tertimbang adalah
    \begin{equation}
        \widehat{\sigma}^2_{\mathrm{ML}} = \frac{\mathrm{RSS}_w}{W_{\mathrm{tot}}} = \frac{\displaystyle\sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i(\mathbf{u}_0) \bigl(y_i - \mathbf{x}_i^\top \widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0)\bigr)^2}{\displaystyle\sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i(\mathbf{u}_0)}.
    \end{equation}
    Namun, estimator MLE ini bersifat bias ke bawah karena tidak memperhitungkan derajat kebebasan yang hilang akibat estimasi $p$ parameter dalam $\boldsymbol{\beta}$. Koreksi bias dilakukan dengan mengganti penyebut $W_{\mathrm{tot}}$ dengan $W_{\mathrm{tot}} - p$, yang menghasilkan estimator tak-bias (atau mendekati tak-bias)
    \begin{equation}
        \widehat{\sigma}^2_{\mathrm{CF}}(\mathbf{u}_0) = \frac{\displaystyle\sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i(\mathbf{u}_0) \bigl(y_i - \mathbf{x}_i^\top \widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0)\bigr)^2}{\displaystyle\sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i(\mathbf{u}_0) - p}.
    \end{equation}    
    Koreksi ini analog dengan pembagi $(n - p)$ pada estimator variansi OLS klasik $\widehat{\sigma}^2 = \mathrm{RSS}/(n - p)$.
\end{proof}

Estimator ini merupakan generalisasi dari estimator variansi GWR klasik dengan koreksi derajat kebebasan. Sebab $\sum_i w_i = 1$, penyebut menjadi $1 - p \cdot \bar{w}$ dengan $\bar{w}$ adalah rata-rata bobot, yang untuk sampel besar mendekati $1$.

Untuk menganalisis sifat asimtotik $\widehat{\sigma}^2_{\mathrm{CF}}$, residual didekomposisikan sebagai berikut.

\begin{proposisi}
    Residual tertimbang dapat ditulis sebagai
    \begin{equation}
        y_i - \mathbf{x}_i^\top \widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0) = \varepsilon_i - \mathbf{x}_i^\top \bigl(\widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0) - \boldsymbol{\beta}(\mathbf{u}_i)\bigr).
    \end{equation}
    Dengan menambah dan mengurangi $\boldsymbol{\beta}(\mathbf{u}_0)$:
    \begin{equation}
        y_i - \mathbf{x}_i^\top \widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0) = \varepsilon_i + \mathbf{x}_i^\top \bigl(\boldsymbol{\beta}(\mathbf{u}_i) - \boldsymbol{\beta}(\mathbf{u}_0)\bigr) - \mathbf{x}_i^\top \bigl(\widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0) - \boldsymbol{\beta}(\mathbf{u}_0)\bigr).
    \end{equation}
\end{proposisi}

\begin{proof}
    Dari model struktural $y_i = \mathbf{x}_i^\top \boldsymbol{\beta}(\mathbf{u}_i) + \varepsilon_i$, maka
    \begin{align}
        y_i - \mathbf{x}_i^\top \widehat{\boldsymbol{\beta}}_{\mathrm{CF}} &= \mathbf{x}_i^\top \boldsymbol{\beta}(\mathbf{u}_i) + \varepsilon_i - \mathbf{x}_i^\top \widehat{\boldsymbol{\beta}}_{\mathrm{CF}} \nonumber \\
        &= \varepsilon_i + \mathbf{x}_i^\top \bigl(\boldsymbol{\beta}(\mathbf{u}_i) - \widehat{\boldsymbol{\beta}}_{\mathrm{CF}}\bigr) \nonumber \\
        &= \varepsilon_i + \mathbf{x}_i^\top \bigl(\boldsymbol{\beta}(\mathbf{u}_i) - \boldsymbol{\beta}(\mathbf{u}_0)\bigr) - \mathbf{x}_i^\top \bigl(\widehat{\boldsymbol{\beta}}_{\mathrm{CF}} - \boldsymbol{\beta}(\mathbf{u}_0)\bigr).
    \end{align}
\end{proof}

Apabile didefinisikan notasi ringkas, yaitu $\delta_i := \boldsymbol{\beta}(\mathbf{u}_i) - \boldsymbol{\beta}(\mathbf{u}_0)$ adalah deviasi koefisien lokal dari target dan $\widehat{\mathbf{\Delta}} := \widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0) - \boldsymbol{\beta}(\mathbf{u}_0) = \mathbf{B}_n + \mathbf{V}_n$ adalah galat estimasi, residual menjadi
\begin{equation}
    \widehat{e}_i = y_i - \mathbf{x}_i^\top \widehat{\boldsymbol{\beta}}_{\mathrm{CF}} = \varepsilon_i + \mathbf{x}_i^\top \delta_i - \mathbf{x}_i^\top \widehat{\mathbf{\Delta}}.
\end{equation}

\begin{teorema}
    Jumlah kuadrat residual tertimbang dapat diekspansi sebagai
    \begin{align}
        \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i \widehat{e}_i^2 &= \underbrace{\sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i \varepsilon_i^2}_{\text{(I): suku utama}} + \underbrace{\sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i (\mathbf{x}_i^\top \delta_i)^2}_{\text{(II): bias lokal}} \nonumber \\
        &\quad + \underbrace{\widehat{\mathbf{\Delta}}^\top \Biggl(\sum_{k=1}^K \mathbf{H}_k\Biggr) \widehat{\mathbf{\Delta}}}_{\text{(III): galat estimasi}} + \text{(suku silang)}.
    \end{align}
\end{teorema}

\begin{proof}
    Dengan mengkuadratkan ekspresi residual akan didapatkan
    \begin{align}
        \widehat{e}_i^2 &= \bigl(\varepsilon_i + \mathbf{x}_i^\top \delta_i - \mathbf{x}_i^\top \widehat{\mathbf{\Delta}}\bigr)^2 \nonumber \\
        &= \varepsilon_i^2 + (\mathbf{x}_i^\top \delta_i)^2 + (\mathbf{x}_i^\top \widehat{\mathbf{\Delta}})^2 \nonumber \\
        &\quad + 2\varepsilon_i \mathbf{x}_i^\top \delta_i - 2\varepsilon_i \mathbf{x}_i^\top \widehat{\mathbf{\Delta}} - 2(\mathbf{x}_i^\top \delta_i)(\mathbf{x}_i^\top \widehat{\mathbf{\Delta}}).
    \end{align}
    Selanjutnya, dengan menjumlahkan bobot
    \begin{align}
        \sum_{k,i} w^{(-k)}_i \widehat{e}_i^2 &= \sum_{k,i} w^{(-k)}_i \varepsilon_i^2 + \sum_{k,i} w^{(-k)}_i (\mathbf{x}_i^\top \delta_i)^2 + \sum_{k,i} w^{(-k)}_i (\mathbf{x}_i^\top \widehat{\mathbf{\Delta}})^2 \nonumber \\
        &\quad + 2\sum_{k,i} w^{(-k)}_i \varepsilon_i \mathbf{x}_i^\top \delta_i - 2\sum_{k,i} w^{(-k)}_i \varepsilon_i \mathbf{x}_i^\top \widehat{\mathbf{\Delta}} \nonumber \\
        &\quad - 2\sum_{k,i} w^{(-k)}_i (\mathbf{x}_i^\top \delta_i)(\mathbf{x}_i^\top \widehat{\mathbf{\Delta}}).
    \end{align}
    Untuk suku (III), perhatikan bahwa
    \begin{align}
        \sum_{k,i} w^{(-k)}_i (\mathbf{x}_i^\top \widehat{\mathbf{\Delta}})^2 &= \sum_{k,i} w^{(-k)}_i \widehat{\mathbf{\Delta}}^\top \mathbf{x}_i \mathbf{x}_i^\top \widehat{\mathbf{\Delta}} \nonumber \\
        &= \widehat{\mathbf{\Delta}}^\top \Biggl(\sum_{k,i} w^{(-k)}_i \mathbf{x}_i \mathbf{x}_i^\top\Biggr) \widehat{\mathbf{\Delta}} \nonumber \\
        &= \widehat{\mathbf{\Delta}}^\top \Biggl(\sum_{k=1}^K \mathbf{H}_k\Biggr) \widehat{\mathbf{\Delta}}.
    \end{align}
\end{proof}

\begin{teorema}
    \label{thm:gagwr_konsistensi_variansi}
    Di bawah Asumsi~\ref{asumsi:gagwr_independensi}--\ref{asumsi:gagwr_kontinuitas_bobot}, estimator variansi galat \emph{cross-fitted} bersifat konsisten:
    \begin{equation}
        \widehat{\sigma}^2_{\mathrm{CF}}(\mathbf{u}_0) \xrightarrow{p} \sigma^2.
    \end{equation}
\end{teorema}

\begin{proof}
    Pada suku utama, berdasarkan hukum bilangan besar tertimbang dan Asumsi~\ref{asumsi:gagwr_homoskedastisitas}:
    \begin{equation}
        \sum_{k,i} w^{(-k)}_i \varepsilon_i^2 \xrightarrow{p} \mathbb{E}[\varepsilon_i^2] = \sigma^2.
    \end{equation}
    Selanjutnya, pada bias lokal, karena $\|\delta_i\| = \|\boldsymbol{\beta}(\mathbf{u}_i) - \boldsymbol{\beta}(\mathbf{u}_0)\| = \mathcal{O}(h)$ untuk $\mathbf{u}_i \in \mathcal{N}_h(\mathbf{u}_0)$, maka
    \begin{equation}
        \sum_{k,i} w^{(-k)}_i (\mathbf{x}_i^\top \delta_i)^2 \leq \sum_{k,i} w^{(-k)}_i \|\mathbf{x}_i\|^2 \|\delta_i\|^2 = \mathcal{O}_p(h^2) \to 0.
    \end{equation}
    Dari hasil sebelumnya, $\widehat{\mathbf{\Delta}} = \mathcal{O}_p(h^2) + \mathcal{O}_p\bigl((nh^d)^{-1/2}\bigr)$. Maka
    \begin{equation}
        \widehat{\mathbf{\Delta}}^\top \Biggl(\sum_k \mathbf{H}_k\Biggr) \widehat{\mathbf{\Delta}} = \mathcal{O}_p\bigl(h^4 + (nh^d)^{-1}\bigr) \to 0.
    \end{equation}
    
    Dengan argumen serupa, semua suku silang konvergen ke nol dalam probabilitas karena melibatkan produk dari suku yang konvergen ke nol. Dengan demikian,
    \begin{equation}
        \sum_{k,i} w^{(-k)}_i \widehat{e}_i^2 \xrightarrow{p} \sigma^2,
    \end{equation}
    dan karena penyebut $\sum_{k,i} w^{(-k)}_i - p \to 1 - 0 = 1$ (dengan koreksi derajat bebas yang dapat dihilangkan secara asimtotik), maka $\widehat{\sigma}^2_{\mathrm{CF}} \xrightarrow{p} \sigma^2$.
\end{proof}

\begin{akibat}
    Bias estimator variansi galat memenuhi
    \begin{equation}
        \mathbb{E}[\widehat{\sigma}^2_{\mathrm{CF}}(\mathbf{u}_0)] - \sigma^2 = \mathcal{O}(h^2) + \mathcal{O}\bigl((nh^d)^{-1}\bigr),
    \end{equation}
    yang konvergen ke nol di bawah kondisi asimtotik \ref{asumsi:gagwr_shrinking_neighborhood} dan \ref{asumsi:gagwr_ukuran_sampel_lokal}.
\end{akibat}

\subsection{Konsistensi Koefisien Lokal}

Bagian ini membuktikan konsistensi estimator \emph{cross-fitted} GA-GWR secara formal. Konsistensi merupakan syarat minimal untuk validitas estimator, yaitu estimator harus konvergen ke nilai parameter sejati ketika ukuran sampel menuju tak hingga.

\begin{teorema}
    Di bawah Asumsi \ref{asumsi:gagwr_independensi}--\ref{asumsi:gagwr_kontinuitas_bobot}, estimator \emph{cross-fitted} GA-GWR bersifat konsisten, yaitu
    \begin{equation}
        \widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0) \xrightarrow{p} \boldsymbol{\beta}(\mathbf{u}_0) \quad \text{ketika } n \to \infty.
    \end{equation}
\end{teorema}

\begin{proof}
    Dari Teorema~\ref{thm:gagwr_dekomposisi_est}, berlaku
    \begin{equation}
        \widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0) - \boldsymbol{\beta}(\mathbf{u}_0) = \mathbf{B}_n(\mathbf{u}_0) + \mathbf{V}_n(\mathbf{u}_0).
    \end{equation}
    Untuk membuktikan konsistensi, perlu ditunjukkan bahwa $\mathbf{B}_n(\mathbf{u}_0) \xrightarrow{p} \mathbf{0}$ dan komponen galat $\mathbf{V}_n(\mathbf{u}_0) \xrightarrow{p} \mathbf{0}$. Dari Teorema~\ref{thm:gagwr_laju_bias}, telah ditunjukkan bahwa
    \begin{equation}
        \mathbf{B}_n(\mathbf{u}_0) = \mathcal{O}_p(h^2).
    \end{equation}
    Sebab $h \to 0$, berdasarkan Asumsi \ref{asumsi:gagwr_shrinking_neighborhood}, maka $\mathbf{B}_n(\mathbf{u}_0) \xrightarrow{p} \mathbf{0}$.
    
    Suku galat diberikan oleh
    \begin{equation}
        \mathbf{V}_n(\mathbf{u}_0) = \Biggl(\sum_{k=1}^K \mathbf{H}_k\Biggr)^{-1} \Biggl(\sum_{k=1}^K \mathbf{X}_k^\top \mathbf{W}^{(-k)}_k \boldsymbol{\varepsilon}_k\Biggr).
    \end{equation}
    Untuk menganalisis konvergensi, ditulis dalam bentuk yang lebih eksplisit:
    \begin{equation}
        \mathbf{V}_n(\mathbf{u}_0) = \Biggl(\frac{1}{nh^d} \sum_{k=1}^K \mathbf{H}_k\Biggr)^{-1} \Biggl(\frac{1}{nh^d} \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i \mathbf{x}_i \varepsilon_i\Biggr).
    \end{equation}
    Didefinisikan matriks
    \begin{equation}
        \mathbf{Q}_n(\mathbf{u}_0) := \frac{1}{nh^d} \sum_{k=1}^K \mathbf{H}_k = \frac{1}{nh^d} \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i \mathbf{x}_i \mathbf{x}_i^\top.
    \end{equation}
    Berdasarkan hukum bilangan besar tertimbang lokal, dengan menggunakan Asumsi \ref{asumsi:gagwr_independensi}, \ref{asumsi:gagwr_momen_kovariat}, \ref{asumsi:gagwr_shrinking_neighborhood}, dan \ref{asumsi:gagwr_kontinuitas_bobot}:
    \begin{equation}
        \mathbf{Q}_n(\mathbf{u}_0) \xrightarrow{p} \mathbf{Q}(\mathbf{u}_0),
    \end{equation}
    dengan $\mathbf{Q}(\mathbf{u}_0) = \mathbb{E}[\mathbf{x}_i \mathbf{x}_i^\top \mid \mathbf{u}_i \approx \mathbf{u}_0]$ adalah matriks momen lokal yang positif definit. Akibatnya,
    \begin{equation}
        \mathbf{Q}_n(\mathbf{u}_0)^{-1} \xrightarrow{p} \mathbf{Q}(\mathbf{u}_0)^{-1}.
    \end{equation}
    
    Selanjutnya, didefinisikan vektor
    \begin{equation}
        \mathbf{S}_n(\mathbf{u}_0) := \frac{1}{nh^d} \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i \mathbf{x}_i \varepsilon_i.
    \end{equation}
    Berdasarkan Proposisi~\ref{prop:cf_exogeneity}, untuk setiap \emph{fold} $k$:
    \begin{equation}
        \mathbb{E}\Bigl[w^{(-k)}_i \mathbf{x}_i \varepsilon_i \mid \widehat{\boldsymbol{\theta}}^{(-k)}\Bigr] = \mathbf{0} \quad \text{untuk } i \in \mathcal{I}_k.
    \end{equation}
    Dengan menggunakan hukum ekspektasi iterasi:
    \begin{equation}
        \mathbb{E}[\mathbf{S}_n(\mathbf{u}_0)] = \frac{1}{nh^d} \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} \mathbb{E}\Bigl[\mathbb{E}\bigl[w^{(-k)}_i \mathbf{x}_i \varepsilon_i \mid \widehat{\boldsymbol{\theta}}^{(-k)}\bigr]\Bigr] = \mathbf{0}.
    \end{equation}
    Untuk variansi, dengan menggunakan independensi antarobservasi \ref{asumsi:gagwr_independensi} dan Asumsi \ref{asumsi:gagwr_homoskedastisitas}:
    \begin{align}
        \mathrm{Var}(\mathbf{S}_n(\mathbf{u}_0)) &= \frac{1}{(nh^d)^2} \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} \mathbb{E}\bigl[(w^{(-k)}_i)^2 \mathbf{x}_i \mathbf{x}_i^\top \varepsilon_i^2\bigr] \nonumber \\
        &= \frac{\sigma^2}{(nh^d)^2} \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} \mathbb{E}\bigl[(w^{(-k)}_i)^2 \mathbf{x}_i \mathbf{x}_i^\top\bigr].
    \end{align}
    
    Berdasarkan Asumsi \ref{asumsi:gagwr_ukuran_sampel_lokal}, $w^{(-k)}_i \leq C/(nh^d)$, sehingga $(w^{(-k)}_i)^2 \leq C^2/(nh^d)^2$. Dengan demikian,
    \begin{align}
        \mathrm{Var}(\mathbf{S}_n(\mathbf{u}_0)) &\leq \frac{\sigma^2 C^2}{(nh^d)^4} \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} \mathbb{E}[\mathbf{x}_i \mathbf{x}_i^\top] \nonumber \\
        &= \frac{\sigma^2 C^2}{(nh^d)^4} \cdot n \cdot \mathbb{E}[\mathbf{x}_i \mathbf{x}_i^\top] \nonumber \\
        &= \mathcal{O}\bigl((nh^d)^{-3}\bigr) \to 0.
    \end{align}
    Sebab $\mathbb{E}[\mathbf{S}_n] = \mathbf{0}$ dan $\mathrm{Var}(\mathbf{S}_n) \to 0$, maka berdasarkan ketaksamaan Chebyshev:
    \begin{equation}
        \mathbf{S}_n(\mathbf{u}_0) \xrightarrow{p} \mathbf{0}.
    \end{equation}
    
    Sebab $\mathbf{Q}_n^{-1} \xrightarrow{p} \mathbf{Q}^{-1}$ dan $\mathbf{S}_n \xrightarrow{p} \mathbf{0}$, maka berdasarkan lemma Slutsky:
    \begin{equation}
        \mathbf{V}_n(\mathbf{u}_0) = \mathbf{Q}_n^{-1} \cdot (nh^d) \cdot \mathbf{S}_n \xrightarrow{p} \mathbf{Q}^{-1} \cdot \mathbf{0} = \mathbf{0}.
    \end{equation}
    Dengan menggabungkan hasil untuk suku bias dan suku galat:
    \begin{equation}
        \widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0) - \boldsymbol{\beta}(\mathbf{u}_0) = \mathbf{B}_n + \mathbf{V}_n \xrightarrow{p} \mathbf{0} + \mathbf{0} = \mathbf{0}.
    \end{equation}
\end{proof}

\begin{akibat}
    Laju konvergensi estimator \emph{cross-fitted} ditentukan oleh
    \begin{equation}
        \widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0) - \boldsymbol{\beta}(\mathbf{u}_0) = \mathcal{O}_p(h^2) + \mathcal{O}_p\bigl((nh^d)^{-1/2}\bigr),
    \end{equation}
    dengan suku pertama adalah kontribusi bias dan suku kedua adalah kontribusi variansi.
\end{akibat}

\subsection{Konsistensi Variansi Koefisien Lokal}

Untuk melakukan inferensi statistik, diperlukan estimasi variansi dari estimator koefisien lokal. Bagian ini mendefinisikan variansi teoretis, menurunkan estimatornya, dan membuktikan konsistensinya.

\begin{teorema}
    \label{thm:gagwr_variansi_cf}
    Di bawah Asumsi \ref{asumsi:gagwr_independensi}--\ref{asumsi:gagwr_kontinuitas_bobot}, variansi kondisional dari estimator \emph{cross-fitted} memiliki bentuk \emph{sandwich}:
    \begin{equation}
        \mathrm{Var}\bigl(\widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0) \mid \{\widehat{\boldsymbol{\theta}}^{(-k)}\}_{k=1}^K\bigr) = \mathbf{Q}_n(\mathbf{u}_0)^{-1} \boldsymbol{\Omega}_n(\mathbf{u}_0) \mathbf{Q}_n(\mathbf{u}_0)^{-1},
    \end{equation}
    dengan
    \begin{align}
        \mathbf{Q}_n(\mathbf{u}_0) &= \sum_{k=1}^K \mathbf{H}_k = \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i \mathbf{x}_i \mathbf{x}_i^\top, \\
        \boldsymbol{\Omega}_n(\mathbf{u}_0) &= \sigma^2 \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} \bigl(w^{(-k)}_i\bigr)^2 \mathbf{x}_i \mathbf{x}_i^\top.
    \end{align}
\end{teorema}

\begin{proof}
    Dari dekomposisi estimator, suku galat diberikan oleh
    \begin{equation}
        \mathbf{V}_n(\mathbf{u}_0) = \Biggl(\sum_{k=1}^K \mathbf{H}_k\Biggr)^{-1} \Biggl(\sum_{k=1}^K \mathbf{X}_k^\top \mathbf{W}^{(-k)}_k \boldsymbol{\varepsilon}_k\Biggr) = \mathbf{Q}_n^{-1} \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i \mathbf{x}_i \varepsilon_i.
    \end{equation}
    
    Kondisional pada $\{\widehat{\boldsymbol{\theta}}^{(-k)}\}$, bobot $w^{(-k)}_i$ bersifat deterministik. Dengan menggunakan independensi antarobservasi dan Asumsi \ref{asumsi:gagwr_homoskedastisitas}:
    \begin{align}
        \mathrm{Var}\Biggl(\sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i \mathbf{x}_i \varepsilon_i \Biggm| \{\widehat{\boldsymbol{\theta}}^{(-k)}\}\Biggr) &= \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} \bigl(w^{(-k)}_i\bigr)^2 \mathbf{x}_i \mathbf{x}_i^\top \mathrm{Var}(\varepsilon_i) \nonumber \\
        &= \sigma^2 \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} \bigl(w^{(-k)}_i\bigr)^2 \mathbf{x}_i \mathbf{x}_i^\top \nonumber \\
        &= \boldsymbol{\Omega}_n(\mathbf{u}_0).
    \end{align}
    Dengan menggunakan sifat variansi transformasi linear:
    \begin{align}
        \mathrm{Var}(\mathbf{V}_n \mid \{\widehat{\boldsymbol{\theta}}^{(-k)}\}) &= \mathbf{Q}_n^{-1} \mathrm{Var}\Biggl(\sum_{k,i} w^{(-k)}_i \mathbf{x}_i \varepsilon_i\Biggr) \mathbf{Q}_n^{-1} \nonumber \\
        &= \mathbf{Q}_n^{-1} \boldsymbol{\Omega}_n \mathbf{Q}_n^{-1}.
    \end{align}
    Sebab suku bias $\mathbf{B}_n$ bersifat deterministik kondisional pada $\{\widehat{\boldsymbol{\theta}}^{(-k)}\}$, maka
    \begin{equation}
        \mathrm{Var}\bigl(\widehat{\boldsymbol{\beta}}_{\mathrm{CF}} \mid \{\widehat{\boldsymbol{\theta}}^{(-k)}\}\bigr) = \mathrm{Var}(\mathbf{V}_n \mid \{\widehat{\boldsymbol{\theta}}^{(-k)}\}) = \mathbf{Q}_n^{-1} \boldsymbol{\Omega}_n \mathbf{Q}_n^{-1}.
    \end{equation}
\end{proof}

\begin{akibat}
    \label{cor:gagwr_orde_variansi}
    Variansi koefisien lokal memiliki orde
    \begin{equation}
        \mathrm{Var}\bigl(\widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0)\bigr) = \mathcal{O}\bigl((nh^d)^{-1}\bigr).
    \end{equation}
    Secara intuitif, \emph{effective local sample size} adalah $nh^d$, sehingga variansi berbanding terbalik dengannya.
\end{akibat}

\begin{proof}
    Dari Asumsi \ref{asumsi:gagwr_ukuran_sampel_lokal}, $w^{(-k)}_i \leq C/(nh^d)$. Dengan demikian,
    \begin{equation}
        \boldsymbol{\Omega}_n = \sigma^2 \sum_{k,i} (w^{(-k)}_i)^2 \mathbf{x}_i \mathbf{x}_i^\top \leq \frac{\sigma^2 C}{nh^d} \sum_{k,i} w^{(-k)}_i \mathbf{x}_i \mathbf{x}_i^\top = \frac{\sigma^2 C}{nh^d} \mathbf{Q}_n.
    \end{equation}
    Sebab $\mathbf{Q}_n = \mathcal{O}(1)$ dan $\boldsymbol{\Omega}_n = \mathcal{O}((nh^d)^{-1})$, maka
    \begin{equation}
        \mathbf{Q}_n^{-1} \boldsymbol{\Omega}_n \mathbf{Q}_n^{-1} = \mathcal{O}(1) \cdot \mathcal{O}\bigl((nh^d)^{-1}\bigr) \cdot \mathcal{O}(1) = \mathcal{O}\bigl((nh^d)^{-1}\bigr).
    \end{equation}
\end{proof}

\begin{teorema}
    Estimator variansi koefisien lokal \emph{cross-fitted} yang diperoleh dengan mengganti parameter populasi dengan estimator konsistennya diberikan oleh
    \begin{equation}
        \widehat{\mathrm{Var}}\bigl(\widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0)\bigr) = \widehat{\mathbf{Q}}_n(\mathbf{u}_0)^{-1} \widehat{\boldsymbol{\Omega}}_n(\mathbf{u}_0) \widehat{\mathbf{Q}}_n(\mathbf{u}_0)^{-1},
    \end{equation}
    dengan
    \begin{align}
        \widehat{\mathbf{Q}}_n(\mathbf{u}_0) &= \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i \mathbf{x}_i \mathbf{x}_i^\top = \sum_{k=1}^K \mathbf{H}_k, \\
        \widehat{\boldsymbol{\Omega}}_n(\mathbf{u}_0) &= \widehat{\sigma}^2_{\mathrm{CF}}(\mathbf{u}_0) \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} \bigl(w^{(-k)}_i\bigr)^2 \mathbf{x}_i \mathbf{x}_i^\top.
    \end{align}
\end{teorema}

\begin{proof}
    Dari Teorema~\ref{thm:gagwr_variansi_cf}, variansi kondisional sejati diberikan oleh
    \begin{equation}
        \mathrm{Var}\bigl(\widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0) \mid \{\widehat{\boldsymbol{\theta}}^{(-k)}\}_{k=1}^K\bigr) = \mathbf{Q}_n(\mathbf{u}_0)^{-1} \boldsymbol{\Omega}_n(\mathbf{u}_0) \mathbf{Q}_n(\mathbf{u}_0)^{-1},
    \end{equation}
    dengan
    \begin{align}
        \mathbf{Q}_n(\mathbf{u}_0) &= \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i \mathbf{x}_i \mathbf{x}_i^\top, \\
        \boldsymbol{\Omega}_n(\mathbf{u}_0) &= \sigma^2 \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} \bigl(w^{(-k)}_i\bigr)^2 \mathbf{x}_i \mathbf{x}_i^\top.
    \end{align}
    
    Perhatikan bahwa $\mathbf{Q}_n(\mathbf{u}_0)$ dapat dihitung langsung dari data karena hanya melibatkan bobot $w^{(-k)}_i$ dan kovariat $\mathbf{x}_i$ yang keduanya teramati. Dengan demikian, estimator untuk $\mathbf{Q}_n(\mathbf{u}_0)$ adalah dirinya sendiri:
    \begin{equation}
        \widehat{\mathbf{Q}}_n(\mathbf{u}_0) = \mathbf{Q}_n(\mathbf{u}_0) = \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i \mathbf{x}_i \mathbf{x}_i^\top = \sum_{k=1}^K \mathbf{H}_k.
    \end{equation}
    
    Untuk $\boldsymbol{\Omega}_n(\mathbf{u}_0)$, satu-satunya kuantitas yang tidak teramati adalah variansi galat $\sigma^2$. Berdasarkan Teorema~\ref{thm:gagwr_konsistensi_variansi}, estimator konsisten untuk $\sigma^2$ adalah $\widehat{\sigma}^2_{\mathrm{CF}}(\mathbf{u}_0)$. Dengan mensubstitusikan $\sigma^2$ dengan estimatornya, diperoleh
    \begin{equation}
        \widehat{\boldsymbol{\Omega}}_n(\mathbf{u}_0) = \widehat{\sigma}^2_{\mathrm{CF}}(\mathbf{u}_0) \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} \bigl(w^{(-k)}_i\bigr)^2 \mathbf{x}_i \mathbf{x}_i^\top.
    \end{equation}
    
    Dengan menggabungkan kedua estimator ke dalam bentuk \emph{sandwich}, diperoleh estimator variansi koefisien lokal:
    \begin{equation}
        \widehat{\mathrm{Var}}\bigl(\widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0)\bigr) = \widehat{\mathbf{Q}}_n(\mathbf{u}_0)^{-1} \widehat{\boldsymbol{\Omega}}_n(\mathbf{u}_0) \widehat{\mathbf{Q}}_n(\mathbf{u}_0)^{-1}.
    \end{equation}
    Bentuk \emph{sandwich} ini memiliki interpretasi geometris bahwa matriks $\widehat{\mathbf{Q}}_n^{-1}$ mentransformasi variabilitas dalam ruang kovariat tertimbang $\widehat{\boldsymbol{\Omega}}_n$ menjadi variabilitas dalam ruang parameter $\boldsymbol{\beta}$.
\end{proof}

Estimator ini menggantikan variansi galat sejati $\sigma^2$ dengan estimator konsistennya $\widehat{\sigma}^2_{\mathrm{CF}}$. Bentuk \emph{sandwich} menjamin \emph{robustness} terhadap spesifikasi yang tidak tepat pada struktur variansi.

\begin{teorema}
    Di bawah Asumsi \ref{asumsi:gagwr_independensi}--\ref{asumsi:gagwr_kontinuitas_bobot}, estimator variansi bersifat konsisten dalam arti
    \begin{equation}
        (nh^d) \cdot \widehat{\mathrm{Var}}\bigl(\widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0)\bigr) \xrightarrow{p} \mathbf{Q}(\mathbf{u}_0)^{-1} \boldsymbol{\Omega}(\mathbf{u}_0) \mathbf{Q}(\mathbf{u}_0)^{-1},
    \end{equation}
    dengan $\mathbf{Q}(\mathbf{u}_0)$ dan $\boldsymbol{\Omega}(\mathbf{u}_0)$ adalah limit probabilitas dari versi ternormalisasi $\mathbf{Q}_n$ dan $\boldsymbol{\Omega}_n$.
\end{teorema}

\begin{proof}    
    Perhatikan bahwa
    \begin{equation}
        \frac{1}{nh^d} \widehat{\mathbf{Q}}_n = \frac{1}{nh^d} \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i \mathbf{x}_i \mathbf{x}_i^\top.
    \end{equation}
    Berdasarkan hukum bilangan besar tertimbang lokal, didapatkan bahwa
    \begin{equation}
        \frac{1}{nh^d} \widehat{\mathbf{Q}}_n \xrightarrow{p} \mathbf{Q}(\mathbf{u}_0) = \mathbb{E}[\mathbf{x}_i \mathbf{x}_i^\top \mid \mathbf{u}_i \approx \mathbf{u}_0].
    \end{equation}
    
    Didefinisikan
    \begin{equation}
        \widetilde{\boldsymbol{\Omega}}_n = (nh^d) \cdot \widehat{\boldsymbol{\Omega}}_n = (nh^d) \cdot \widehat{\sigma}^2_{\mathrm{CF}} \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} (w^{(-k)}_i)^2 \mathbf{x}_i \mathbf{x}_i^\top.
    \end{equation}
    
    Dari Teorema~\ref{thm:gagwr_konsistensi_variansi}, $\widehat{\sigma}^2_{\mathrm{CF}} \xrightarrow{p} \sigma^2$. Untuk suku penjumlahan, dengan menggunakan Asumsi \ref{asumsi:gagwr_ukuran_sampel_lokal}:
    \begin{equation}
        (nh^d) \sum_{k,i} (w^{(-k)}_i)^2 \mathbf{x}_i \mathbf{x}_i^\top \xrightarrow{p} \boldsymbol{\Omega}'(\mathbf{u}_0),
    \end{equation}
    dengan $\boldsymbol{\Omega}'(\mathbf{u}_0)$ adalah limit yang terkait dengan momen bobot kuadrat lokal.
    Didefinisikan $\boldsymbol{\Omega}(\mathbf{u}_0) = \sigma^2 \boldsymbol{\Omega}'(\mathbf{u}_0)$. Oleh karena itu,
    \begin{equation}
        \widetilde{\boldsymbol{\Omega}}_n \xrightarrow{p} \sigma^2 \boldsymbol{\Omega}'(\mathbf{u}_0) = \boldsymbol{\Omega}(\mathbf{u}_0).
    \end{equation}
    Estimator variansi ternormalisasi dapat ditulis sebagai
    \begin{align}
        (nh^d) \cdot \widehat{\mathrm{Var}}\bigl(\widehat{\boldsymbol{\beta}}_{\mathrm{CF}}\bigr) &= \Bigl(\frac{1}{nh^d} \widehat{\mathbf{Q}}_n\Bigr)^{-1} \widetilde{\boldsymbol{\Omega}}_n \Bigl(\frac{1}{nh^d} \widehat{\mathbf{Q}}_n\Bigr)^{-1}.
    \end{align}
    
    Selanjutnya, sebab $\frac{1}{nh^d} \widehat{\mathbf{Q}}_n \xrightarrow{p} \mathbf{Q}(\mathbf{u}_0)$ yang invertibel, $\widetilde{\boldsymbol{\Omega}}_n \xrightarrow{p} \boldsymbol{\Omega}(\mathbf{u}_0)$, dan fungsi inversi matriks kontinu pada himpunan matriks invertibel, maka berdasarkan \emph{continuous mapping theorem} didapatkan
    \begin{equation}
        (nh^d) \cdot \widehat{\mathrm{Var}}\bigl(\widehat{\boldsymbol{\beta}}_{\mathrm{CF}}\bigr) \xrightarrow{p} \mathbf{Q}(\mathbf{u}_0)^{-1} \boldsymbol{\Omega}(\mathbf{u}_0) \mathbf{Q}(\mathbf{u}_0)^{-1}.
    \end{equation}
\end{proof}

\begin{akibat}
    Galat standar untuk komponen ke-$j$ dari koefisien lokal diestimasi sebagai
    \begin{equation}
        \widehat{\mathrm{se}}\bigl(\widehat{\beta}_{\mathrm{CF},j}(\mathbf{u}_0)\bigr) = \sqrt{\bigl[\widehat{\mathrm{Var}}\bigl(\widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0)\bigr)\bigr]_{jj}},
    \end{equation}
    dengan $[\cdot]_{jj}$ menandakan elemen diagonal ke-$j$ dari matriks variansi-kovariansi.
\end{akibat}

\subsection{Distribusi Asimtotik Koefisien Lokal}

Bagian ini menurunkan distribusi asimtotik dari estimator \emph{cross-fitted} GA-GWR. Hasil utama menunjukkan bahwa setelah dinormalisasi dengan $\sqrt{nh^d}$, estimator berdistribusi normal asimtotik di bawah kondisi \emph{undersmoothing}.

Berdasarkan Akibat~\ref{cor:gagwr_orde_variansi}, variansi estimator memiliki orde $\mathcal{O}((nh^d)^{-1})$. Hal ini menunjukkan bahwa \emph{effective local sample size} adalah $nh^d$, sehingga normalisasi yang tepat adalah $\sqrt{nh^d}$.

\begin{proposisi}[\textbf{Dekomposisi Terstandarisasi}]
    Estimator \emph{cross-fitted} yang dinormalisasi dapat didekomposisikan sebagai
    \begin{equation}
        \sqrt{nh^d}\bigl(\widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0) - \boldsymbol{\beta}(\mathbf{u}_0)\bigr) = \underbrace{\sqrt{nh^d} \mathbf{B}_n(\mathbf{u}_0)}_{\text{bias terstandarisasi}} + \underbrace{\sqrt{nh^d} \mathbf{V}_n(\mathbf{u}_0)}_{\text{galat terstandarisasi}}.
    \end{equation}
    Distribusi limit ditentukan oleh perilaku kedua suku ini.
\end{proposisi}

Suku galat terstandarisasi dapat ditulis dalam bentuk yang memfasilitasi penerapan Teorema Limit Pusat.

\begin{proposisi}
    Suku galat terstandarisasi dapat ditulis sebagai
    \begin{equation}
        \sqrt{nh^d} \mathbf{V}_n(\mathbf{u}_0) = \Biggl(\frac{1}{nh^d} \sum_{k=1}^K \mathbf{H}_k\Biggr)^{-1} \Biggl(\frac{1}{\sqrt{nh^d}} \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i \mathbf{x}_i \varepsilon_i\Biggr),
    \end{equation}
    yang merupakan produk dari dua faktor, yaitu satu yang konvergen dalam probabilitas dan satu yang konvergen dalam distribusi.
\end{proposisi}

\begin{teorema}
    \label{thm:gagwr_clt_cf}
    Di bawah Asumsi \ref{asumsi:gagwr_independensi}--\ref{asumsi:gagwr_kontinuitas_bobot}, didefinisikan
    \begin{equation}
        \mathbf{S}_n(\mathbf{u}_0) = \frac{1}{\sqrt{nh^d}} \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i \mathbf{x}_i \varepsilon_i.
    \end{equation}
    Maka
    \begin{equation}
        \mathbf{S}_n(\mathbf{u}_0) \xrightarrow{d} \mathcal{N}\bigl(\mathbf{0}, \boldsymbol{\Omega}(\mathbf{u}_0)\bigr),
    \end{equation}
    dengan $\boldsymbol{\Omega}(\mathbf{u}_0)$ adalah limit dari $\boldsymbol{\Omega}_n(\mathbf{u}_0)$ ternormalisasi.
\end{teorema}

\begin{proof}
    Bukti diuraikan menggunakan Teorema Limit Pusat Lindeberg-Feller untuk \emph{triangular array} dari variabel acak. Berdasarkan Proposisi~\ref{prop:cf_exogeneity}, untuk setiap \emph{fold} $k$ dan $i \in \mathcal{I}_k$ berlaku
    \begin{equation}
        \mathbb{E}\bigl[w^{(-k)}_i \mathbf{x}_i \varepsilon_i \mid \widehat{\boldsymbol{\theta}}^{(-k)}\bigr] = w^{(-k)}_i \mathbf{x}_i \mathbb{E}[\varepsilon_i \mid \mathbf{x}_i, \mathbf{u}_i] = \mathbf{0},
    \end{equation}
    karena bobot $w^{(-k)}_i$ dilatih pada data yang tidak termasuk observasi $i$, sehingga bersifat deterministik kondisional pada $\widehat{\boldsymbol{\theta}}^{(-k)}$. Dengan demikian, $\mathbb{E}[\mathbf{S}_n] = \mathbf{0}$.
    
    Variansi kondisional diberikan oleh
    \begin{align}
        \mathrm{Var}(\mathbf{S}_n \mid \{\widehat{\boldsymbol{\theta}}^{(-k)}\}) &= \frac{1}{nh^d} \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} \bigl(w^{(-k)}_i\bigr)^2 \mathbb{E}[\mathbf{x}_i \mathbf{x}_i^\top \varepsilon_i^2 \mid \mathbf{u}_i] \nonumber \\
        &= \frac{\sigma^2}{nh^d} \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} \bigl(w^{(-k)}_i\bigr)^2 \mathbf{x}_i \mathbf{x}_i^\top.
    \end{align}
    Selanjutnya, didefinisikan
    \begin{equation}
        \boldsymbol{\Omega}'_n := \frac{nh^d}{1} \cdot \frac{\sigma^2}{nh^d} \sum_{k,i} (w^{(-k)}_i)^2 \mathbf{x}_i \mathbf{x}_i^\top = \sigma^2 \sum_{k,i} (w^{(-k)}_i)^2 \mathbf{x}_i \mathbf{x}_i^\top.
    \end{equation}
    Berdasarkan hukum bilangan besar, $\boldsymbol{\Omega}'_n \xrightarrow{p} \boldsymbol{\Omega}(\mathbf{u}_0)$ yang positif definit dan terbatas.
    
    Untuk CLT Lindeberg-Feller, perlu diverifikasi bahwa untuk setiap $\delta > 0$ harus terpenuhi kondisi Lindeberg, yaitu
    \begin{equation}
        \lim_{n \to \infty} \frac{1}{\mathrm{Var}(\mathbf{S}_n)} \sum_{k,i} \mathbb{E}\bigl[\|w^{(-k)}_i \mathbf{x}_i \varepsilon_i\|^2 \mathbf{1}_{\{\|w^{(-k)}_i \mathbf{x}_i \varepsilon_i\| > \delta \sqrt{nh^d}\}}\bigr] = 0.
    \end{equation}
    Berdasarkan Asumsi \ref{asumsi:gagwr_ukuran_sampel_lokal}, $w^{(-k)}_i \leq C/(nh^d)$. Dengan demikian,
    \begin{equation}
        \|w^{(-k)}_i \mathbf{x}_i \varepsilon_i\| \leq \frac{C}{nh^d} \|\mathbf{x}_i\| |\varepsilon_i| \to 0
    \end{equation}
    untuk $n$ besar, karena $nh^d \to \infty$ (Asumsi~\ref{asumsi:gagwr_ukuran_sampel_lokal}). Ini berarti untuk $n$ cukup besar, $\|w^{(-k)}_i \mathbf{x}_i \varepsilon_i\| < \delta \sqrt{nh^d}$ hampir pasti, sehingga kondisi Lindeberg terpenuhi.
    
    Dengan demikian, karena suku-suku $w^{(-k)}_i \mathbf{x}_i \varepsilon_i$ independen antarobservasi, rata-rata nol, variansi total terbatas dan definit positif, dan kondisi Lindeberg terpenuhi, maka berdasarkan Teorema Limit Pusat Lindeberg-Feller akan dihasilkan
    \begin{equation}
        \mathbf{S}_n(\mathbf{u}_0) \xrightarrow{d} \mathcal{N}\bigl(\mathbf{0}, \boldsymbol{\Omega}(\mathbf{u}_0)\bigr).
    \end{equation}
\end{proof}

\begin{teorema}
    \label{thm:gagwr_asymptotic_distribution_cf}
    Di bawah Asumsi \ref{asumsi:gagwr_independensi}--\ref{asumsi:gagwr_kontinuitas_bobot}:
    \begin{equation}
        \sqrt{nh^d} \mathbf{V}_n(\mathbf{u}_0) \xrightarrow{d} \mathcal{N}\bigl(\mathbf{0}, \mathbf{Q}(\mathbf{u}_0)^{-1} \boldsymbol{\Omega}(\mathbf{u}_0) \mathbf{Q}(\mathbf{u}_0)^{-1}\bigr).
    \end{equation}
\end{teorema}

\begin{proof}
    Berdasarkan Teorema~\ref{thm:gagwr_clt_cf}, didapatkan $\frac{1}{nh^d} \sum_k \mathbf{H}_k \xrightarrow{p} \mathbf{Q}(\mathbf{u}_0)$ yang invertibel (definit positif) dan $\mathbf{S}_n \xrightarrow{d} \mathcal{N}(\mathbf{0}, \boldsymbol{\Omega}(\mathbf{u}_0))$.
    Sebab satu faktor konvergen dalam probabilitas ke matriks invertibel dan faktor lainnya konvergen dalam distribusi, maka berdasarkan Lemma Slutsky:
    \begin{align}
        \sqrt{nh^d} \mathbf{V}_n &= \Biggl(\frac{1}{nh^d} \sum_k \mathbf{H}_k\Biggr)^{-1} \mathbf{S}_n \nonumber \\
        &\xrightarrow{d} \mathbf{Q}(\mathbf{u}_0)^{-1} \cdot \mathcal{N}\bigl(\mathbf{0}, \boldsymbol{\Omega}(\mathbf{u}_0)\bigr) \nonumber \\
        &= \mathcal{N}\bigl(\mathbf{0}, \mathbf{Q}(\mathbf{u}_0)^{-1} \boldsymbol{\Omega}(\mathbf{u}_0) \mathbf{Q}(\mathbf{u}_0)^{-1}\bigr).
    \end{align}
\end{proof}

Perilaku suku bias terstandarisasi $\sqrt{nh^d} \mathbf{B}_n(\mathbf{u}_0)$ menentukan apakah distribusi asimtotik berpusat di nol atau tidak.

\begin{proposisi}
    Dari Teorema~\ref{thm:gagwr_laju_bias}, $\mathbf{B}_n(\mathbf{u}_0) = \mathcal{O}_p(h^2)$. Dengan demikian,
    \begin{equation}
        \sqrt{nh^d} \mathbf{B}_n(\mathbf{u}_0) = \mathcal{O}_p\bigl(\sqrt{nh^d} \cdot h^2\bigr) = \mathcal{O}_p\bigl(\sqrt{nh^{d+4}}\bigr).
    \end{equation}
\end{proposisi}

\noindent Terdapat dua skenario fundamental bergantung pada laju relatif antara $nh^d$ dan $h^2$.

\begin{teorema}
    \label{thm:gagwr_undersmoothing}
    Jika kondisi \emph{undersmoothing}
    \begin{equation}
        \sqrt{nh^d} \cdot h^2 \to 0 \quad \text{ketika } n \to \infty
    \end{equation}
    dipenuhi, maka suku bias terstandarisasi lenyap secara asimtotik:
    \begin{equation}
        \sqrt{nh^d} \mathbf{B}_n(\mathbf{u}_0) \xrightarrow{p} \mathbf{0}.
    \end{equation}
\end{teorema}

\begin{proof}
    Sebab $\mathbf{B}_n(\mathbf{u}_0) = \mathcal{O}_p(h^2)$, maka $\sqrt{nh^d} \mathbf{B}_n(\mathbf{u}_0) = \mathcal{O}_p(\sqrt{nh^d} h^2)$. Di bawah kondisi $\sqrt{nh^d} h^2 \to 0$, berlaku $\sqrt{nh^d} \mathbf{B}_n \xrightarrow{p} \mathbf{0}$.
\end{proof}

\begin{akibat}
    Kondisi \emph{undersmoothing} yang dapat dinyatakan dengan $\sqrt{nh^d} h^2 \to 0$ ekuivalen dengan
    \begin{equation}
        h = o\bigl(n^{-1/(d+4)}\bigr).
    \end{equation}
    Hal ini berarti \emph{bandwidth} harus menyusut lebih cepat dari laju optimal untuk estimasi titik (yang biasanya $h \asymp n^{-1/(d+4)}$).
\end{akibat}

\begin{proof}
    Kondisi $\sqrt{nh^d} h^2 \to 0$ ekuivalen dengan $nh^{d+4} \to 0$, yang ekuivalen dengan $h^{d+4} = o(n^{-1})$, atau $h = o(n^{-1/(d+4)})$.
\end{proof}

\begin{proposisi}
    Ketika menggunakan metrik alternatif dengan dimensi fraktal efektif $d_{\mathrm{eff}} \neq 2$, kondisi \emph{undersmoothing} menjadi
    \begin{equation}
        \sqrt{n h^{d_{\mathrm{eff}}}} \cdot h^2 \to 0 \quad \iff \quad h = o\bigl(n^{-1/(d_{\mathrm{eff}}+4)}\bigr).
    \end{equation}
\end{proposisi}


Implikasi praktis untuk pemilihan \emph{bandwidth}:
\begin{enumerate}[label=(\roman*)]
    \item Metrik Euclidean dengan $d_{\mathrm{eff}} = 2$ dapat dipilih $h = o(n^{-1/6}) \approx n^{-0.167}$
    \item Jarak jaringan urban dengan $d_{\mathrm{eff}} \approx 1$ dapat dipilih $h = o(n^{-1/5}) \approx n^{-0.2}$
    \item Metrik Intermediat dengan $d_{\mathrm{eff}} = 1.5$ dapat dipilih $h = o(n^{-1/5.5}) \approx n^{-0.182}$
\end{enumerate}

\begin{teorema}
    Di bawah Asumsi \ref{asumsi:gagwr_independensi}--\ref{asumsi:gagwr_kontinuitas_bobot} dan kondisi \emph{undersmoothing} $\sqrt{nh^d} h^2 \to 0$:
    \begin{equation}
        \sqrt{nh^d}\bigl(\widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0) - \boldsymbol{\beta}(\mathbf{u}_0)\bigr) \xrightarrow{d} \mathcal{N}\bigl(\mathbf{0}, \mathbf{Q}(\mathbf{u}_0)^{-1} \boldsymbol{\Omega}(\mathbf{u}_0) \mathbf{Q}(\mathbf{u}_0)^{-1}\bigr),
    \end{equation}
    dengan
    \begin{align}
        \mathbf{Q}(\mathbf{u}_0) &= \lim_{n \to \infty} \frac{1}{nh^d} \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} w^{(-k)}_i \mathbf{x}_i \mathbf{x}_i^\top, \\
        \boldsymbol{\Omega}(\mathbf{u}_0) &= \lim_{n \to \infty} \sigma^2 (nh^d) \sum_{k=1}^K \sum_{i \in \mathcal{I}_k} \bigl(w^{(-k)}_i\bigr)^2 \mathbf{x}_i \mathbf{x}_i^\top.
    \end{align}
\end{teorema}

\begin{proof}
    Dari dekomposisi terstandarisasi:
    \begin{equation}
        \sqrt{nh^d}\bigl(\widehat{\boldsymbol{\beta}}_{\mathrm{CF}} - \boldsymbol{\beta}\bigr) = \sqrt{nh^d} \mathbf{B}_n + \sqrt{nh^d} \mathbf{V}_n.
    \end{equation}
    
    Berdasarkan Teorema~\ref{thm:gagwr_undersmoothing}, $\sqrt{nh^d} \mathbf{B}_n \xrightarrow{p} \mathbf{0}$. Berdasarkan Teorema~\ref{thm:gagwr_asymptotic_distribution_cf} didapatkan
    \begin{equation}
        \sqrt{nh^d} \mathbf{V}_n \xrightarrow{d} \mathcal{N}\bigl(\mathbf{0}, \mathbf{Q}^{-1} \boldsymbol{\Omega} \mathbf{Q}^{-1}\bigr).
    \end{equation}
    Dengan menggunakan Lemma Slutsky untuk penjumlahan suku yang konvergen dalam probabilitas ke nol dengan suku yang konvergen dalam distribusi, maka diperoleh
    \begin{equation}
        \sqrt{nh^d}\bigl(\widehat{\boldsymbol{\beta}}_{\mathrm{CF}} - \boldsymbol{\beta}\bigr) \xrightarrow{d} \mathcal{N}\bigl(\mathbf{0}, \mathbf{Q}^{-1} \boldsymbol{\Omega} \mathbf{Q}^{-1}\bigr).
    \end{equation}
\end{proof}

Jika kondisi \emph{undersmoothing} tidak dipenuhi, yaitu $\sqrt{nh^d} h^2 \to c \neq 0$ untuk suatu konstanta $c$, maka distribusi asimtotik menjadi
\begin{equation}
    \sqrt{nh^d}\bigl(\widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0) - \boldsymbol{\beta}(\mathbf{u}_0)\bigr) \xrightarrow{d} \mathcal{N}\bigl(\boldsymbol{\mu}(\mathbf{u}_0), \mathbf{Q}^{-1} \boldsymbol{\Omega} \mathbf{Q}^{-1}\bigr),
\end{equation}
dengan $\boldsymbol{\mu}(\mathbf{u}_0) \neq \mathbf{0}$ adalah vektor bias asimtotik. Dalam kasus ini, interval konfidensi standar dan uji Wald tidak valid karena bias tidak diabaikan.

\begin{akibat}
    Untuk setiap komponen ke-$j$ dari vektor koefisien:
    \begin{equation}
        \sqrt{nh^d}\bigl(\widehat{\beta}_{\mathrm{CF},j}(\mathbf{u}_0) - \beta_j(\mathbf{u}_0)\bigr) \xrightarrow{d} \mathcal{N}\bigl(0, [\mathbf{Q}^{-1} \boldsymbol{\Omega} \mathbf{Q}^{-1}]_{jj}\bigr).
    \end{equation}
\end{akibat}

\subsection{Distribusi Asimtotik Uji Statistik}

Bagian ini menurunkan distribusi asimtotik dari statistik uji yang digunakan untuk inferensi pada koefisien lokal, termasuk statistik-$t$ untuk pengujian individual dan statistik Wald untuk pengujian gabungan.

Bagian pertama menurunkan distribusi asimtotik statistik-$t$ untuk pengujian hipotesis nol individual pada koefisien lokal dengan hiptosesis umum berupa
\begin{align*}
    H_0: \beta_j(\mathbf{u}_0) &= \beta_{j,0}, lawan\\
    H_1: \beta_j(\mathbf{u}_0) &\neq \beta_{j,0},
\end{align*}
dengan $\beta_{j,0}$ adalah nilai tertentu, yang biasanya adalah nol untuk menguji signifikansi.

\begin{teorema}
    Di bawah Asumsi \ref{asumsi:gagwr_independensi}--\ref{asumsi:gagwr_kontinuitas_bobot}, kondisi \emph{undersmoothing}, dan hipotesis nol $H_0: \beta_j(\mathbf{u}_0) = \beta_{j,0}$, statistik-$t$
    \begin{equation}
        t_j(\mathbf{u}_0) = \frac{\widehat{\beta}_{\mathrm{CF},j}(\mathbf{u}_0) - \beta_{j,0}}{\widehat{\mathrm{se}}\bigl(\widehat{\beta}_{\mathrm{CF},j}(\mathbf{u}_0)\bigr)}
    \end{equation}
    berdistribusi normal standar asimtotik:
    \begin{equation}
        t_j(\mathbf{u}_0) \xrightarrow{d} \mathcal{N}(0, 1) \quad \text{di bawah } H_0.
    \end{equation}
\end{teorema}

\begin{proof}
    Di bawah $H_0: \beta_j(\mathbf{u}_0) = \beta_{j,0}$, statistik-$t$ dapat ditulis sebagai
    \begin{equation}
        t_j = \frac{\widehat{\beta}_{\mathrm{CF},j} - \beta_j(\mathbf{u}_0)}{\widehat{\mathrm{se}}(\widehat{\beta}_{\mathrm{CF},j})}.
    \end{equation}
    Berdasarkan Teorema~\ref{thm:gagwr_asymptotic_distribution_cf} didapatkan
    \begin{equation}
        \sqrt{nh^d}\bigl(\widehat{\beta}_{\mathrm{CF},j} - \beta_j\bigr) \xrightarrow{d} \mathcal{N}\bigl(0, \sigma^2_j\bigr),
    \end{equation}
    dengan $\sigma^2_j = [\mathbf{Q}^{-1} \boldsymbol{\Omega} \mathbf{Q}^{-1}]_{jj}$.
    Selanjutnya, dari Teorema~\ref{thm:gagwr_konsistensi_variansi_cf} juga didapatkan
    \begin{equation}
        (nh^d) \cdot \widehat{\mathrm{Var}}(\widehat{\beta}_{\mathrm{CF},j}) \xrightarrow{p} \sigma^2_j.
    \end{equation}
    
    Oleh karena itu,
    \begin{equation}
        \widehat{\mathrm{se}}(\widehat{\beta}_{\mathrm{CF},j}) = \sqrt{\widehat{\mathrm{Var}}(\widehat{\beta}_{\mathrm{CF},j})} = \frac{\sigma_j + o_p(1)}{\sqrt{nh^d}},
    \end{equation}
    sehingga statistik-$t$ dapat ditulis sebagai
    \begin{align}
        t_j &= \frac{\widehat{\beta}_{\mathrm{CF},j} - \beta_j}{\widehat{\mathrm{se}}(\widehat{\beta}_{\mathrm{CF},j})} \nonumber \\
        &= \frac{\sqrt{nh^d}(\widehat{\beta}_{\mathrm{CF},j} - \beta_j)}{\sqrt{nh^d} \cdot \widehat{\mathrm{se}}(\widehat{\beta}_{\mathrm{CF},j})} \nonumber \\
        &= \frac{\sqrt{nh^d}(\widehat{\beta}_{\mathrm{CF},j} - \beta_j)}{\sigma_j + o_p(1)}.
    \end{align}
    
    Berdasarkan Lemma Slutsky, karena pembilang konvergen dalam distribusi ke $\mathcal{N}(0, \sigma^2_j)$ dan penyebut konvergen dalam probabilitas ke $\sigma_j$:
    \begin{equation}
        t_j \xrightarrow{d} \frac{\mathcal{N}(0, \sigma^2_j)}{\sigma_j} = \mathcal{N}(0, 1).
    \end{equation}
\end{proof}

\begin{akibat}
    Interval konfidensi $(1-\alpha) \times 100\%$ untuk koefisien lokal $\beta_j(\mathbf{u}_0)$ diberikan oleh
    \begin{equation}
        \mathrm{CI}_{1-\alpha}\bigl(\beta_j(\mathbf{u}_0)\bigr) = \widehat{\beta}_{\mathrm{CF},j}(\mathbf{u}_0) \pm z_{1-\alpha/2} \cdot \widehat{\mathrm{se}}\bigl(\widehat{\beta}_{\mathrm{CF},j}(\mathbf{u}_0)\bigr),
    \end{equation}
    dengan $z_{1-\alpha/2}$ adalah kuantil $(1-\alpha/2)$ dari distribusi normal standar.
\end{akibat}

\begin{proof}
    Dari distribusi asimtotik $t_j \xrightarrow{d} \mathcal{N}(0,1)$, didapatkan
    \begin{equation}
        \Pr\bigl(-z_{1-\alpha/2} \leq t_j \leq z_{1-\alpha/2}\bigr) \to 1 - \alpha.
    \end{equation}
    Oleh karena itu, dengan mensubstitusikan definisi $t_j$ dan menyusun ulang:
    \begin{equation}
        \Pr\Bigl(\widehat{\beta}_{\mathrm{CF},j} - z_{1-\alpha/2} \cdot \widehat{\mathrm{se}} \leq \beta_j \leq \widehat{\beta}_{\mathrm{CF},j} + z_{1-\alpha/2} \cdot \widehat{\mathrm{se}}\Bigr) \to 1 - \alpha.
    \end{equation}
\end{proof}

Untuk pengujian hipotesis gabungan yang melibatkan beberapa komponen koefisien secara simultan, digunakan statistik Wald. Uji ini dibangun atas hipotesis berupa
\begin{align*}
    H_0: \mathbf{R} \boldsymbol{\beta}(\mathbf{u}_0) &= \mathbf{r}, \quad \text{lawan} \\
    H_1: \mathbf{R} \boldsymbol{\beta}(\mathbf{u}_0) &\neq \mathbf{r},
\end{align*}
dengan $\mathbf{R}$ adalah matriks $q \times p$ yang menentukan kombinasi linier dari koefisien yang diuji, dan $\mathbf{r}$ adalah vektor $q \times 1$ dari nilai tertentu.

\begin{teorema}
    Di bawah Asumsi \ref{asumsi:gagwr_independensi}--\ref{asumsi:gagwr_kontinuitas_bobot}, kondisi \emph{undersmoothing}, dan hipotesis nol linear $H_0: \mathbf{R} \boldsymbol{\beta}(\mathbf{u}_0) = \mathbf{r}$ dengan $\mathbf{R}$ matriks $q \times p$ berpangkat penuh baris, statistik Wald
    \begin{equation}
        W(\mathbf{u}_0) = \bigl(\mathbf{R} \widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0) - \mathbf{r}\bigr)^\top \Bigl[\mathbf{R} \widehat{\mathrm{Var}}\bigl(\widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0)\bigr) \mathbf{R}^\top\Bigr]^{-1} \bigl(\mathbf{R} \widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0) - \mathbf{r}\bigr)
    \end{equation}
    berdistribusi $\chi^2$ asimtotik dengan $q$ derajat kebebasan, yaitu
    \begin{equation}
        W(\mathbf{u}_0) \xrightarrow{d} \chi^2_q \quad \text{di bawah } H_0,
    \end{equation}
    dengan $q = \mathrm{rank}(\mathbf{R})$ adalah jumlah restriksi.
\end{teorema}

\begin{proof}
    Di bawah $H_0$, berlaku $\mathbf{R} \boldsymbol{\beta}(\mathbf{u}_0) = \mathbf{r}$. Didefinisikan
    \begin{equation}
        \mathbf{\delta}_n := \mathbf{R} \widehat{\boldsymbol{\beta}}_{\mathrm{CF}} - \mathbf{r} = \mathbf{R}\bigl(\widehat{\boldsymbol{\beta}}_{\mathrm{CF}} - \boldsymbol{\beta}\bigr).
    \end{equation}
    Dari Teorema~\ref{thm:gagwr_asymptotic_distribution_cf} didapatkan
    \begin{equation}
        \sqrt{nh^d}\bigl(\widehat{\boldsymbol{\beta}}_{\mathrm{CF}} - \boldsymbol{\beta}\bigr) \xrightarrow{d} \mathcal{N}\bigl(\mathbf{0}, \mathbf{Q}^{-1} \boldsymbol{\Omega} \mathbf{Q}^{-1}\bigr).
    \end{equation}
    Sebab transformasi linear mempertahankan normalitas, maka
    \begin{equation}
        \sqrt{nh^d} \mathbf{\delta}_n = \mathbf{R} \sqrt{nh^d}\bigl(\widehat{\boldsymbol{\beta}}_{\mathrm{CF}} - \boldsymbol{\beta}\bigr) \xrightarrow{d} \mathcal{N}\bigl(\mathbf{0}, \mathbf{R} \mathbf{Q}^{-1} \boldsymbol{\Omega} \mathbf{Q}^{-1} \mathbf{R}^\top\bigr).
    \end{equation}
    Didefinisikan $\boldsymbol{\Sigma}_R := \mathbf{R} \mathbf{Q}^{-1} \boldsymbol{\Omega} \mathbf{Q}^{-1} \mathbf{R}^\top$. Dari Teorema~\ref{thm:gagwr_konsistensi_variansi_cf} didapatkan
    \begin{equation}
        (nh^d) \mathbf{R} \widehat{\mathrm{Var}}(\widehat{\boldsymbol{\beta}}_{\mathrm{CF}}) \mathbf{R}^\top \xrightarrow{p} \boldsymbol{\Sigma}_R.
    \end{equation}
    
    Statistik Wald dapat ditulis sebagai
    \begin{align}
        W &= \mathbf{\delta}_n^\top \bigl[\mathbf{R} \widehat{\mathrm{Var}}(\widehat{\boldsymbol{\beta}}_{\mathrm{CF}}) \mathbf{R}^\top\bigr]^{-1} \mathbf{\delta}_n \nonumber \\
        &= (nh^d) \mathbf{\delta}_n^\top \bigl[(nh^d) \mathbf{R} \widehat{\mathrm{Var}}(\widehat{\boldsymbol{\beta}}_{\mathrm{CF}}) \mathbf{R}^\top\bigr]^{-1} (nh^d) \mathbf{\delta}_n \cdot (nh^d)^{-1} \nonumber \\
        &= \bigl(\sqrt{nh^d} \mathbf{\delta}_n\bigr)^\top \bigl[(nh^d) \mathbf{R} \widehat{\mathrm{Var}} \mathbf{R}^\top\bigr]^{-1} \bigl(\sqrt{nh^d} \mathbf{\delta}_n\bigr).
    \end{align}
    Selanjutnya, berdasarkan \emph{continuous mapping theorem} dan Lemma Slutsky:
    \begin{equation}
        W \xrightarrow{d} \mathbf{Z}^\top \boldsymbol{\Sigma}_R^{-1} \mathbf{Z},
    \end{equation}
    dengan $\mathbf{Z} \sim \mathcal{N}(\mathbf{0}, \boldsymbol{\Sigma}_R)$.
    Bentuk kuadratik ini berdistribusi $\chi^2_q$ karena $\boldsymbol{\Sigma}_R^{-1/2} \mathbf{Z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}_q)$ dan
    \begin{equation}
        \mathbf{Z}^\top \boldsymbol{\Sigma}_R^{-1} \mathbf{Z} = \|\boldsymbol{\Sigma}_R^{-1/2} \mathbf{Z}\|^2 \sim \chi^2_q.
    \end{equation}
\end{proof}

\begin{akibat}
    Untuk menguji $H_0: \beta_j(\mathbf{u}_0) = 0$, pilih $\mathbf{R} = \mathbf{e}_j^\top$ (vektor satuan ke-$j$) dan $\mathbf{r} = 0$. Maka statistik Wald menjadi
    \begin{equation}
        W_j = \frac{\widehat{\beta}_{\mathrm{CF},j}^2}{\widehat{\mathrm{Var}}(\widehat{\beta}_{\mathrm{CF},j})} = t_j^2 \xrightarrow{d} \chi^2_1.
    \end{equation}
    Hal ini konsisten dengan hubungan $\chi^2_1 = (\mathcal{N}(0,1))^2$.
\end{akibat}

\begin{akibat}
    Untuk menguji hipotesis bahwa semua koefisien kecuali konstanta bernilai nol, $H_0: \beta_1 = \beta_2 = \cdots = \beta_{p-1} = 0$ dengan $\beta_0$ adalah konstanta, pilih
    \begin{equation}
        \mathbf{R} = \begin{pmatrix} \mathbf{0}_{(p-1) \times 1} & \mathbf{I}_{p-1} \end{pmatrix}, \quad \mathbf{r} = \mathbf{0}_{p-1}.
    \end{equation}
    Statistik Wald yang dihasilkan berdistribusi $\chi^2_{p-1}$ di bawah $H_0$.
\end{akibat}

Prosedur inferensi-inferensi di atas valid secara asimtotik jika dan hanya jika terpenuhi kondisi:
\begin{enumerate}
    \item Asumsi \ref{asumsi:gagwr_independensi}--\ref{asumsi:gagwr_kontinuitas_bobot} terpenuhi,
    \item kondisi \emph{undersmoothing} $\sqrt{nh^d} h^2 \to 0$ dipenuhi, dan
    \item \emph{cross-fitting} diimplementasikan dengan benar sehingga bobot kernel independen dari galat pada \emph{fold} estimasi.
\end{enumerate}
Tanpa \emph{undersmoothing}, interval konfidensi akan memiliki \emph{coverage probability} yang lebih rendah dari tingkat nominal karena bias yang tidak terabaikan.

\section{Analisis Komputasional GA-GWR}

Bagian ini menganalisis aspek komputasional dari GA-GWR yang diturunkan secara langsung dari formulasi teoretis pada bagian sebelumnya. Pembahasan meliputi struktur graf komputasional terdiferensiasi, analisis propagasi gradien (termasuk kondisi \emph{vanishing} dan \emph{exploding gradient}), kompleksitas algoritmik, serta kelayakan komputasional model secara keseluruhan.

\subsection{Graf Komputasional dan Diferensiabilitas}

Berdasarkan formulasi pada bagian sebelumnya, \emph{pipeline} komputasional GA-GWR dapat direpresentasikan sebagai graf arah asiklik (\emph{directed acyclic graph}, DAG) yang sepenuhnya terdiferensiasi. Subbagian ini menguraikan setiap komponen graf komputasional dan menganalisis propagasi gradien melalui setiap tahapan.

\begin{definisi}[\textbf{Graf komputasional GA-GWR}]
    \emph{Pipeline} komputasional GA-GWR adalah komposisi pemetaan
    \begin{equation}
        \boldsymbol{\theta} \xrightarrow{\phi_1} \mathbf{s} \xrightarrow{\phi_2} \mathbf{w} \xrightarrow{\phi_3} \widehat{\boldsymbol{\beta}} \xrightarrow{\phi_4} \mathcal{L},
    \end{equation}
    dengan komponen-komponen sebagai berikut.
    \begin{enumerate}[label=(\roman*)]
        \item Pemetaan $\phi_1: \boldsymbol{\theta} \mapsto \mathbf{s}$ adalah fungsi GNN yang menghasilkan vektor skor $\mathbf{s} = (s_1, \ldots, s_m)^\top$ dengan $s_i = s_{\boldsymbol{\theta}}(i, \mathbf{u}_0)$ untuk setiap $i \in \mathcal{N}_h(\mathbf{u}_0)$.
        \item Pemetaan $\phi_2: \mathbf{s} \mapsto \mathbf{w}$ adalah transformasi \emph{softmax} yang memetakan vektor skor ke bobot \emph{kernel}.
        \item Pemetaan $\phi_3: \mathbf{w} \mapsto \widehat{\boldsymbol{\beta}}$ adalah solusi LWLS dengan $\widehat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{W} \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{W} \mathbf{y}$.
        \item Pemetaan $\phi_4: \widehat{\boldsymbol{\beta}} \mapsto \mathcal{L}$ adalah fungsi kerugian kuadrat $\mathcal{L} = \sum_i (y_i - \mathbf{x}_i^\top \widehat{\boldsymbol{\beta}}(\mathbf{u}_i))^2$.
    \end{enumerate}
\end{definisi}

Setiap pemetaan dalam graf komputasional bersifat terdiferensiasi, sehingga gradien dapat dipropagasikan dari fungsi kerugian ke parameter GNN melalui aturan rantai (\emph{chain rule}).

\begin{proposisi}
    Komposisi $\mathcal{L} \circ \phi_4 \circ \phi_3 \circ \phi_2 \circ \phi_1$ terdiferensiasi hampir di mana-mana terhadap $\boldsymbol{\theta}$, dengan gradien
    \begin{equation}
        \frac{\partial \mathcal{L}}{\partial \boldsymbol{\theta}} = \frac{\partial \mathcal{L}}{\partial \widehat{\boldsymbol{\beta}}} \cdot \frac{\partial \widehat{\boldsymbol{\beta}}}{\partial \mathbf{w}} \cdot \frac{\partial \mathbf{w}}{\partial \mathbf{s}} \cdot \frac{\partial \mathbf{s}}{\partial \boldsymbol{\theta}}.
    \end{equation}
\end{proposisi}

\begin{proof}
    Diferensiabilitas setiap komponen ditunjukkan sebagai berikut.
    \begin{enumerate}[label=(\roman*)]
        \item Pemetaan GNN $\phi_1$ merupakan komposisi transformasi linear dan fungsi aktivasi yang terdiferensiasi (misalnya ReLU, yang terdiferensiasi kecuali di titik nol dengan ukuran Lebesgue nol).
        \item Transformasi \emph{softmax} $\phi_2$ terdiferensiasi di $\mathbb{R}^m$ karena fungsi eksponensial dan penjumlahan terdiferensiasi di mana-mana.
        \item Solusi LWLS $\phi_3$ terdiferensiasi ketika matriks $\mathbf{X}^\top \mathbf{W} \mathbf{X}$ invertibel.
        \item Fungsi kerugian kuadrat $\phi_4$ terdiferensiasi di mana-mana sebagai fungsi polinomial.
    \end{enumerate}
    Dengan demikian, komposisi keempat pemetaan terdiferensiasi hampir di mana-mana, dan gradien diperoleh melalui aturan rantai.
\end{proof}

Analisis propagasi gradien memerlukan karakterisasi eksplisit matriks Jacobian dari setiap komponen. Transformasi \emph{softmax} memiliki struktur Jacobian khusus yang penting untuk memahami dinamika gradien.

\begin{proposisi}
    Matriks Jacobian dari transformasi \emph{softmax} $\mathbf{w} = \mathrm{softmax}(\mathbf{s})$ adalah
    \begin{equation}
        \frac{\partial \mathbf{w}}{\partial \mathbf{s}} = \mathrm{diag}(\mathbf{w}) - \mathbf{w} \mathbf{w}^\top,
    \end{equation}
    dengan elemen-elemen
    \begin{equation}
        \frac{\partial w_i}{\partial s_j} = \begin{cases}
            w_i (1 - w_i) & \text{jika } i = j, \\
            -w_i w_j & \text{jika } i \neq j.
        \end{cases}
    \end{equation}
\end{proposisi}

\begin{proof}
    Untuk $i = j$, dengan menggunakan aturan hasil bagi diperoleh
    \begin{align}
        \frac{\partial w_i}{\partial s_i} &= \frac{\partial}{\partial s_i} \left( \frac{e^{s_i}}{\sum_k e^{s_k}} \right) = \frac{e^{s_i} \sum_k e^{s_k} - e^{s_i} \cdot e^{s_i}}{(\sum_k e^{s_k})^2} \nonumber \\
        &= \frac{e^{s_i}}{\sum_k e^{s_k}} - \frac{e^{2s_i}}{(\sum_k e^{s_k})^2} = w_i - w_i^2 = w_i(1 - w_i).
    \end{align}
    Untuk $i \neq j$, diperoleh
    \begin{align}
        \frac{\partial w_i}{\partial s_j} &= \frac{\partial}{\partial s_j} \left( \frac{e^{s_i}}{\sum_k e^{s_k}} \right) = \frac{0 - e^{s_i} \cdot e^{s_j}}{(\sum_k e^{s_k})^2} = -w_i w_j.
    \end{align}
    Dalam bentuk matriks, $\frac{\partial \mathbf{w}}{\partial \mathbf{s}} = \mathrm{diag}(\mathbf{w}) - \mathbf{w}\mathbf{w}^\top$.
\end{proof}

Struktur Jacobian \emph{softmax} memiliki implikasi penting terhadap efek gradien.

% \begin{akibat}
%     Matriks Jacobian $\mathbf{J}_{\mathrm{softmax}} = \mathrm{diag}(\mathbf{w}) - \mathbf{w}\mathbf{w}^\top$ memiliki sifat spektral berikut.
%     \begin{enumerate}[label=(\roman*)]
%         \item Nilai eigen $\lambda_i \in [0, 1/4]$ untuk semua $i$.
%         \item Norma spektral $\|\mathbf{J}_{\mathrm{softmax}}\|_2 \leq 1/2$.
%         \item Matriks memiliki nilai eigen nol dengan vektor eigen $\mathbf{1} = (1, \ldots, 1)^\top$ karena $\sum_i w_i = 1$ adalah konstanta.
%     \end{enumerate}
% \end{akibat}

% \begin{proof}
%     Matriks Jacobian $\mathbf{J}_{\mathrm{softmax}} = \mathrm{diag}(\mathbf{w}) - \mathbf{w}\mathbf{w}^\top$ dapat ditunjukkan semidefinit positif karena untuk setiap vektor $\mathbf{v}$ berlaku
%     \begin{equation}
%         \mathbf{v}^\top \mathbf{J}_{\mathrm{softmax}} \mathbf{v} = \sum_i w_i v_i^2 - \left(\sum_i w_i v_i\right)^2 = \sum_i w_i(v_i - \bar{v})^2 \geq 0,
%     \end{equation}
%     dengan $\bar{v} = \sum_i w_i v_i$ adalah rata-rata tertimbang. Jejak matriks adalah
%     \begin{equation}
%         \mathrm{tr}(\mathbf{J}_{\mathrm{softmax}}) = \sum_i w_i(1-w_i) = \sum_i w_i - \sum_i w_i^2 = 1 - \sum_i w_i^2.
%     \end{equation}
%     Dengan kendala $\sum_i w_i = 1$ dan $w_i \geq 0$, nilai minimum dari $\sum_i w_i^2$ adalah $1/m$ (dicapai saat $w_i = 1/m$ untuk semua $i$), sehingga $\mathrm{tr}(\mathbf{J}) \leq 1 - 1/m$.
    
%     Untuk setiap elemen diagonal, fungsi $f(w) = w(1-w)$ mencapai maksimum pada $w = 1/2$ dengan nilai $f(1/2) = 1/4$. Sebab elemen diagonal matriks semidefinit positif membatasi nilai eigen, maka semua nilai eigen memenuhi $\lambda_i \leq 1/4$.
    
%     Untuk norma spektral, dengan ketaksamaan segitiga matriks berlaku
%     \begin{equation}
%         \|\mathbf{J}_{\mathrm{softmax}}\|_2 = \|\mathrm{diag}(\mathbf{w}) - \mathbf{w}\mathbf{w}^\top\|_2 \leq \|\mathrm{diag}(\mathbf{w})\|_2 + \|\mathbf{w}\mathbf{w}^\top\|_2.
%     \end{equation}
%     Norma spektral matriks diagonal adalah $\|\mathrm{diag}(\mathbf{w})\|_2 = \max_i w_i \leq 1$, dan norma spektral matriks \emph{rank}-1 adalah $\|\mathbf{w}\mathbf{w}^\top\|_2 = \|\mathbf{w}\|_2^2 \leq 1$. Batas yang lebih ketat $\|\mathbf{J}_{\mathrm{softmax}}\|_2 \leq 1/2$ diperoleh dari analisis langsung nilai eigen maksimum, yang dicapai saat dua bobot sama besar ($w_1 = w_2 = 1/2$, sisanya nol).

%     Untuk vektor eigen nol, verifikasi langsung memberikan
%     \begin{align}
%         \mathbf{J}_{\mathrm{softmax}} \mathbf{1} &= \mathrm{diag}(\mathbf{w}) \mathbf{1} - \mathbf{w}(\mathbf{w}^\top \mathbf{1}) = \mathbf{w} - \mathbf{w} \cdot 1 = \mathbf{0}.
%     \end{align}
%     Keberadaan nilai eigen nol berasal dari sifat invariansi translasi \emph{softmax}, yaitu $\mathrm{softmax}(\mathbf{s} + c\mathbf{1}) = \mathrm{softmax}(\mathbf{s})$ untuk setiap konstanta $c \in \mathbb{R}$.
% \end{proof}

Komponen kritis dalam propagasi gradien adalah diferensiasi melalui solusi LWLS. Solusi ini merupakan fungsi implisit dari bobot, sehingga memerlukan penanganan khusus.

\begin{proposisi}
    Untuk solusi LWLS $\widehat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{W} \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{W} \mathbf{y}$ dengan $\mathbf{W} = \mathrm{diag}(w_1, \ldots, w_m)$, gradien terhadap bobot tunggal $w_i$ adalah
    \begin{equation}
        \frac{\partial \widehat{\boldsymbol{\beta}}}{\partial w_i} = \mathbf{M}^{-1} \mathbf{x}_i \bigl( y_i - \mathbf{x}_i^\top \widehat{\boldsymbol{\beta}} \bigr),
    \end{equation}
    dengan $\mathbf{M} = \mathbf{X}^\top \mathbf{W} \mathbf{X}$ adalah matriks informasi lokal tertimbang.
\end{proposisi}

\begin{proof}
    Identitas diferensiasi matriks invers menyatakan $\frac{\partial \mathbf{M}^{-1}}{\partial w_i} = -\mathbf{M}^{-1} \frac{\partial \mathbf{M}}{\partial w_i} \mathbf{M}^{-1}$. Dengan menggunakan aturan perkalian, diperoleh
    \begin{align}
        \frac{\partial \widehat{\boldsymbol{\beta}}}{\partial w_i} &= \frac{\partial \mathbf{M}^{-1}}{\partial w_i} \mathbf{X}^\top \mathbf{W} \mathbf{y} + \mathbf{M}^{-1} \frac{\partial (\mathbf{X}^\top \mathbf{W} \mathbf{y})}{\partial w_i}.
    \end{align}
    Sebab $\mathbf{M} = \sum_j w_j \mathbf{x}_j \mathbf{x}_j^\top$, maka $\frac{\partial \mathbf{M}}{\partial w_i} = \mathbf{x}_i \mathbf{x}_i^\top$. Demikian pula, $\frac{\partial (\mathbf{X}^\top \mathbf{W} \mathbf{y})}{\partial w_i} = \mathbf{x}_i y_i$. Substitusi memberikan
    \begin{align}
        \frac{\partial \widehat{\boldsymbol{\beta}}}{\partial w_i} &= -\mathbf{M}^{-1} \mathbf{x}_i \mathbf{x}_i^\top \mathbf{M}^{-1} \mathbf{X}^\top \mathbf{W} \mathbf{y} + \mathbf{M}^{-1} \mathbf{x}_i y_i \nonumber \\
        &= -\mathbf{M}^{-1} \mathbf{x}_i \mathbf{x}_i^\top \widehat{\boldsymbol{\beta}} + \mathbf{M}^{-1} \mathbf{x}_i y_i \nonumber \\
        &= \mathbf{M}^{-1} \mathbf{x}_i (y_i - \mathbf{x}_i^\top \widehat{\boldsymbol{\beta}}).
    \end{align}
\end{proof}

Bentuk gradien di atas menunjukkan bahwa kontribusi gradien dari observasi ke-$i$ proporsional terhadap residualnya $r_i = y_i - \mathbf{x}_i^\top \widehat{\boldsymbol{\beta}}$. Hal ini memberikan interpretasi bahwa optimasi akan menyesuaikan bobot berdasarkan seberapa baik model menyesuaikan setiap observasi.

\subsection{Analisis \emph{Vanishing} dan \emph{Exploding Gradient}}

Analisis stabilitas numerik memerlukan karakterisasi kondisi di mana gradien dapat mengalami \emph{vanishing} (menghilang) atau \emph{exploding} (meledak). Kedua fenomena ini merupakan masalah fundamental dalam pelatihan jaringan saraf dalam yang dapat mengganggu konvergensi optimasi.

\begin{definisi}[\textbf{\emph{Vanishing gradient}}]
    Fenomena \emph{vanishing gradient} terjadi ketika magnitudo gradien menjadi sangat kecil selama propagasi balik (\emph{backpropagation}), sehingga pembaruan parameter menjadi tidak signifikan. Secara formal, jika $\mathbf{g}_t = \frac{\partial \mathcal{L}}{\partial \boldsymbol{\theta}}$ adalah gradien pada iterasi $t$, maka \emph{vanishing gradient} terjadi ketika
    \begin{equation}
        \|\mathbf{g}_t\| \to 0 \quad \text{meskipun } \mathcal{L}(\boldsymbol{\theta}_t) \gg \mathcal{L}^\star,
    \end{equation}
    dengan $\mathcal{L}^\star$ adalah nilai optimal. Fenomena ini menyebabkan pembelajaran terhenti sebelum mencapai solusi optimal.
\end{definisi}

\begin{definisi}[\textbf{\emph{Exploding gradient}}]
    Fenomena \emph{exploding gradient} terjadi ketika magnitudo gradien tumbuh secara eksponensial selama propagasi balik, menyebabkan pembaruan parameter yang tidak stabil. Secara formal, \emph{exploding gradient} terjadi ketika
    \begin{equation}
        \|\mathbf{g}_t\| \to \infty,
    \end{equation}
    yang mengakibatkan divergensi parameter dan ketidakstabilan numerik.
\end{definisi}

Untuk mendeteksi kedua fenomena ini dalam praktik, dapat digunakan metrik berikut.

\begin{definisi}[\textbf{Indikator stabilitas gradien}]
    Stabilitas gradien dapat dideteksi melalui rasio norma gradien terhadap norma parameter:
    \begin{equation}
        \rho_t = \frac{\|\mathbf{g}_t\|}{\|\boldsymbol{\theta}_t\| + \epsilon},
    \end{equation}
    dengan $\epsilon > 0$ adalah konstanta kecil untuk stabilitas numerik. Kondisi $\rho_t \ll 1$ mengindikasikan potensi \emph{vanishing gradient}, sedangkan $\rho_t \gg 1$ mengindikasikan potensi \emph{exploding gradient}. Dalam praktik, pelatihan stabil dicapai ketika $\rho_t \in [10^{-7}, 10^{3}]$.
\end{definisi}

Dalam konteks GA-GWR, kedua fenomena ini dapat terjadi pada komponen \emph{softmax} dan solusi LWLS.

\begin{teorema}
    \label{thm:vanishing_softmax}
    Gradien melalui transformasi \emph{softmax} mengalami \emph{vanishing} ketika distribusi bobot mendekati distribusi degenerasi. Secara kuantitatif, jika $w_{i^\star} \to 1$ untuk suatu indeks $i^\star$ (sehingga $w_j \to 0$ untuk semua $j \neq i^\star$), maka
    \begin{equation}
        \left\| \frac{\partial \mathbf{w}}{\partial \mathbf{s}} \right\|_F \to 0.
    \end{equation}
\end{teorema}

\begin{proof}
    Norma Frobenius dari Jacobian \emph{softmax} dapat dihitung sebagai
    \begin{align}
        \left\| \frac{\partial \mathbf{w}}{\partial \mathbf{s}} \right\|_F^2 &= \sum_i \left(\frac{\partial w_i}{\partial s_i}\right)^2 + \sum_{i \neq j} \left(\frac{\partial w_i}{\partial s_j}\right)^2 \nonumber \\
        &= \sum_i w_i^2(1-w_i)^2 + \sum_{i \neq j} w_i^2 w_j^2.
    \end{align}
    Suku kedua dapat disederhanakan sebagai
    \begin{equation}
        \sum_{i \neq j} w_i^2 w_j^2 = \left(\sum_i w_i^2\right)^2 - \sum_i w_i^4.
    \end{equation}
    Ketika $w_{i^\star} \to 1$ dan $w_j \to 0$ untuk $j \neq i^\star$, diperoleh $\sum_i w_i^2 \to 1$ dan setiap suku $w_i(1-w_i) \to 0$ karena:
    \begin{itemize}
        \item Untuk $i = i^\star$: $w_{i^\star}(1-w_{i^\star}) \to 1 \cdot 0 = 0$.
        \item Untuk $i \neq i^\star$: $w_i(1-w_i) \to 0 \cdot 1 = 0$.
    \end{itemize}
    Dengan demikian, $\left\| \frac{\partial \mathbf{w}}{\partial \mathbf{s}} \right\|_F \to 0$.
\end{proof}

Fenomena \emph{vanishing gradient} pada \emph{softmax} terjadi ketika fungsi skor GNN menghasilkan nilai yang sangat berbeda antarobservasi, menyebabkan satu observasi mendominasi bobot. Dalam konteks spasial, ini terjadi jika GNN terlalu yakin bahwa satu tetangga jauh lebih relevan dibandingkan yang lain.

\begin{teorema}
    \label{thm:exploding_lwls}
    Gradien melalui solusi LWLS mengalami \emph{exploding} ketika matriks informasi lokal $\mathbf{M} = \mathbf{X}^\top \mathbf{W} \mathbf{X}$ mendekati singularitas. Secara kuantitatif, jika $\lambda_{\min}(\mathbf{M}) \to 0$ (nilai eigen terkecil mendekati nol), maka
    \begin{equation}
        \left\| \frac{\partial \widehat{\boldsymbol{\beta}}}{\partial w_i} \right\| = \mathcal{O}\left( \frac{1}{\lambda_{\min}(\mathbf{M})} \right) \to \infty.
    \end{equation}
\end{teorema}

\begin{proof}
    Berdasarkan Proposisi sebelumnya, gradien solusi LWLS terhadap bobot adalah $\frac{\partial \widehat{\boldsymbol{\beta}}}{\partial w_i} = \mathbf{M}^{-1} \mathbf{x}_i (y_i - \mathbf{x}_i^\top \widehat{\boldsymbol{\beta}})$. Norma gradien dapat dibatasi oleh
    \begin{align}
        \left\| \frac{\partial \widehat{\boldsymbol{\beta}}}{\partial w_i} \right\| &\leq \|\mathbf{M}^{-1}\|_2 \cdot \|\mathbf{x}_i\| \cdot |y_i - \mathbf{x}_i^\top \widehat{\boldsymbol{\beta}}|.
    \end{align}
    Karena norma spektral matriks invers adalah $\|\mathbf{M}^{-1}\|_2 = 1/\lambda_{\min}(\mathbf{M})$, maka
    \begin{equation}
        \left\| \frac{\partial \widehat{\boldsymbol{\beta}}}{\partial w_i} \right\| \leq \frac{\|\mathbf{x}_i\| \cdot |r_i|}{\lambda_{\min}(\mathbf{M})},
    \end{equation}
    dengan $r_i = y_i - \mathbf{x}_i^\top \widehat{\boldsymbol{\beta}}$ adalah residual. Ketika $\lambda_{\min}(\mathbf{M}) \to 0$, norma gradien divergen menuju tak hingga.
\end{proof}

Matriks $\mathbf{M}$ akan mendekati singularitas dalam situasi-situasi berikut.
\begin{enumerate}[label=(\roman*)]
    \item Jika bobot terkonsentrasi pada kurang dari $p$ observasi, maka $\mathrm{rank}(\mathbf{M}) < p$.
    \item Jika kovariat pada observasi berbobot tinggi bersifat kolinear, maka $\mathbf{M}$ mendekati singular.
    \item Jika lokasi target memiliki sedikit tetangga dalam $\mathcal{N}_h(\mathbf{u}_0)$, ukuran sampel efektif menjadi kecil.
\end{enumerate}

\subsection{Konstanta Lipschitz dan Stabilitas \emph{Pipeline}}

Untuk mengkarakterisasi propagasi perturbasi melalui \emph{pipeline}, diperlukan analisis konstanta Lipschitz dari setiap komponen. Konstanta Lipschitz mengukur sensitivitas \emph{output} terhadap perubahan \emph{input} dan merupakan kunci untuk memahami stabilitas numerik.

\begin{definisi}[\textbf{Konstanta Lipschitz}]
    Suatu fungsi $f: \mathcal{X} \to \mathcal{Y}$ dikatakan Lipschitz kontinu dengan konstanta $L \geq 0$ jika untuk semua $\mathbf{x}_1, \mathbf{x}_2 \in \mathcal{X}$ berlaku
    \begin{equation}
        \|f(\mathbf{x}_1) - f(\mathbf{x}_2)\| \leq L \|\mathbf{x}_1 - \mathbf{x}_2\|.
    \end{equation}
    Konstanta Lipschitz terkecil yang memenuhi pertidaksamaan di atas disebut konstanta Lipschitz dari $f$ dan dinotasikan $\mathrm{Lip}(f)$.
\end{definisi}

Konstanta Lipschitz penting dalam analisis GA-GWR karena alasan berikut.
\begin{enumerate}[label=(\roman*)]
    \item Konstanta Lipschitz kecil menjamin bahwa perturbasi kecil pada \emph{input} tidak menyebabkan perubahan besar pada \emph{output}.
    \item Untuk fungsi dengan konstanta Lipschitz terbatas, algoritma \emph{gradient descent} dijamin konvergen dengan laju tertentu.
    \item Batas Lipschitz berkontribusi pada batas generalisasi model melalui teori kompleksitas Rademacher.
\end{enumerate}

\begin{teorema}
    Misalkan $L_{\mathrm{GNN}}$, $L_{\mathrm{softmax}}$, $L_{\mathrm{LWLS}}$, dan $L_{\mathcal{L}}$ adalah konstanta Lipschitz dari masing-masing komponen \emph{pipeline}. Konstanta Lipschitz keseluruhan memenuhi
    \begin{equation}
        L_{\mathrm{total}} \leq L_{\mathcal{L}} \cdot L_{\mathrm{LWLS}} \cdot L_{\mathrm{softmax}} \cdot L_{\mathrm{GNN}}.
    \end{equation}
    Berdasarkan hasil sebelumnya, $L_{\mathrm{softmax}} \leq 2$ dari sifat Lipschitz transformasi \emph{softmax}. Nilai $L_{\mathrm{LWLS}}$ bergantung pada kondisioning matriks $\mathbf{M}$:
    \begin{equation}
        L_{\mathrm{LWLS}} = \mathcal{O}\left( \frac{\kappa(\mathbf{M})}{\lambda_{\min}(\mathbf{M})} \right),
    \end{equation}
    dengan $\kappa(\mathbf{M}) = \lambda_{\max}(\mathbf{M}) / \lambda_{\min}(\mathbf{M})$ adalah \emph{condition number} matriks $\mathbf{M}$.
\end{teorema}

\begin{proof}
    Untuk komposisi fungsi $f = f_L \circ \cdots \circ f_1$, konstanta Lipschitz memenuhi $\mathrm{Lip}(f) \leq \prod_{i=1}^L \mathrm{Lip}(f_i)$. Hal ini dapat dibuktikan secara induktif: untuk $\mathbf{x}, \mathbf{y}$ sembarang,
    \begin{align}
        \|f(\mathbf{x}) - f(\mathbf{y})\| &= \|f_L(f_{L-1}(\cdots f_1(\mathbf{x}))) - f_L(f_{L-1}(\cdots f_1(\mathbf{y})))\| \nonumber \\
        &\leq L_L \|f_{L-1}(\cdots f_1(\mathbf{x})) - f_{L-1}(\cdots f_1(\mathbf{y}))\| \nonumber \\
        &\leq \cdots \leq \prod_{i=1}^L L_i \|\mathbf{x} - \mathbf{y}\|.
    \end{align}
    
    Untuk komponen LWLS, perhatikan bahwa $\widehat{\boldsymbol{\beta}} = \mathbf{M}^{-1} \mathbf{X}^\top \mathbf{W} \mathbf{y}$. Sensitivitas terhadap perubahan bobot $\mathbf{W}$ melibatkan invers matriks $\mathbf{M}$, yang norma spektralnya adalah $1/\lambda_{\min}(\mathbf{M})$. Dengan analisis perturbasi matriks, diperoleh batas $L_{\mathrm{LWLS}} = \mathcal{O}(\kappa(\mathbf{M})/\lambda_{\min}(\mathbf{M}))$.
\end{proof}

Teorema di atas menunjukkan bahwa stabilitas \emph{pipeline} secara keseluruhan bergantung kritis pada matriks informasi lokal $\mathbf{M}$. Matriks dengan \emph{condition number} tinggi (misalnya $\kappa(\mathbf{M}) > 10^{6}$) menyebabkan amplifikasi perturbasi dan ketidakstabilan numerik, yang mengindikasikan perlunya regularisasi atau peningkatan ukuran sampel lokal.

\subsection{Algoritma Pelatihan dan Kompleksitas Komputasional}

Bagian ini menyajikan algoritma komputasional untuk pelatihan GNN dan estimasi koefisien lokal dengan skema \emph{cross-fitting}, serta analisis kompleksitas algoritmiknya.

\begin{definisi}[\textbf{Fungsi kerugian pelatihan}]
    Untuk himpunan pelatihan $\mathcal{D}_{\mathrm{latih}}$, fungsi kerugian didefinisikan sebagai
    \begin{equation}
        \mathcal{L}(\boldsymbol{\theta}) = \frac{1}{|\mathcal{D}_{\mathrm{latih}}|} \sum_{i \in \mathcal{D}_{\mathrm{latih}}} \bigl(y_i - \mathbf{x}_i^\top \widehat{\boldsymbol{\beta}}_{\boldsymbol{\theta}}(\mathbf{u}_i)\bigr)^2,
    \end{equation}
    dengan $\widehat{\boldsymbol{\beta}}_{\boldsymbol{\theta}}(\mathbf{u}_i)$ adalah estimator koefisien lokal yang bergantung pada parameter GNN $\boldsymbol{\theta}$ melalui bobot \emph{kernel} $\widehat{\kappa}_{\boldsymbol{\theta}}(\cdot \mid \mathbf{u}_i)$.
\end{definisi}

Untuk menjamin stabilitas pelatihan berdasarkan analisis gradien pada subbagian sebelumnya, ditambahkan regularisasi entropi yang mencegah \emph{vanishing gradient}.

\begin{definisi}[\textbf{Fungsi kerugian teregularisasi}]
    Fungsi kerugian teregularisasi didefinisikan sebagai
    \begin{equation}
        \mathcal{L}_{\mathrm{reg}}(\boldsymbol{\theta}) = \mathcal{L}(\boldsymbol{\theta}) - \lambda_{\mathrm{ent}} \cdot \frac{1}{n} \sum_{i=1}^n H(\mathbf{w}_i),
    \end{equation}
    dengan $H(\mathbf{w}_i) = -\sum_j w_{ij} \log w_{ij}$ adalah entropi distribusi bobot untuk lokasi $i$, dan $\lambda_{\mathrm{ent}} > 0$ adalah parameter regularisasi. Suku entropi mendorong distribusi bobot yang lebih merata, mencegah konsentrasi bobot yang menyebabkan \emph{vanishing gradient}.
\end{definisi}

Algoritma berikut mengimplementasikan skema \emph{cross-fitting} yang telah diuraikan sebelumnya.

\begin{algorithm}[H]
    \caption{Estimasi Koefisien Lokal GA-GWR dengan \emph{Cross-Fitting}}
    \begin{algorithmic}[1]
    \Require Data $\{(\mathbf{x}_i, y_i, \mathbf{u}_i)\}_{i=1}^n$, jumlah \emph{fold} $K$, lokasi target $\mathbf{u}_0$
    \Ensure Estimator koefisien lokal $\widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0)$
    \State \textbf{Tahap 1: Partisi Data}
    \State Partisi $\{1, \ldots, n\}$ menjadi $K$ \emph{fold} saling lepas: $\mathcal{I}_1, \mathcal{I}_2, \ldots, \mathcal{I}_K$
    \State
    \State \textbf{Tahap 2: Pelatihan GNN per \emph{Fold}}
    \For{$k = 1$ \textbf{to} $K$}
        \State Definisikan data pelatihan: $\mathcal{D}^{(-k)} = \{i : i \notin \mathcal{I}_k\}$
        \State Latih GNN: $\widehat{\boldsymbol{\theta}}^{(-k)} = \argmin_{\boldsymbol{\theta}} \mathcal{L}_{\mathrm{reg}}(\boldsymbol{\theta}; \mathcal{D}^{(-k)})$
    \EndFor
    \State
    \State \textbf{Tahap 3: Konstruksi Graf Lokal}
    \State Bangun $\mathcal{N}_h(\mathbf{u}_0) = \{i : d(\mathbf{u}_i, \mathbf{u}_0) \leq h\}$
    \State Untuk setiap $i \in \mathcal{N}_h(\mathbf{u}_0)$: hitung $\mathbf{z}_i(\mathbf{u}_0) = (\mathbf{x}_i^\top, (\mathbf{u}_i - \mathbf{u}_0)^\top)^\top$
    \State
    \State \textbf{Tahap 4: Komputasi Bobot \emph{Cross-Fitted}}
    \For{$k = 1$ \textbf{to} $K$}
        \For{$i \in \mathcal{I}_k \cap \mathcal{N}_h(\mathbf{u}_0)$}
            \State Hitung skor: $s_i = \mathrm{GNN}_{\widehat{\boldsymbol{\theta}}^{(-k)}}(\mathcal{G}_h(\mathbf{u}_0), \mathbf{z}_i(\mathbf{u}_0))$
        \EndFor
        \State Normalisasi \emph{softmax}: $w^{(-k)}_i = \exp(s_i) / \sum_{j \in \mathcal{I}_k \cap \mathcal{N}_h} \exp(s_j)$
    \EndFor
    \State
    \State \textbf{Tahap 5: Agregasi Estimator \emph{Cross-Fitted}}
    \For{$k = 1$ \textbf{to} $K$}
        \State $\mathbf{H}_k \gets \sum_{i \in \mathcal{I}_k} w^{(-k)}_i \mathbf{x}_i \mathbf{x}_i^\top$
        \State $\mathbf{g}_k \gets \sum_{i \in \mathcal{I}_k} w^{(-k)}_i \mathbf{x}_i y_i$
    \EndFor
    \State $\widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0) \gets \bigl(\sum_{k=1}^K \mathbf{H}_k\bigr)^{-1} \bigl(\sum_{k=1}^K \mathbf{g}_k\bigr)$
    \State \Return $\widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_0)$
    \end{algorithmic}
\end{algorithm}


Analisis kompleksitas komputasional penting untuk memahami skalabilitas algoritma GA-GWR. Notasi $\mathcal{O}(\cdot)$ menyatakan batas atas asimtotik, dengan kompleksitas yang lebih rendah mengindikasikan algoritma yang lebih efisien.

\begin{teorema}[\textbf{Kompleksitas GA-GWR}]
    Misalkan $n$ adalah jumlah observasi, $p$ adalah dimensi kovariat, $m = |\mathcal{N}_h(\mathbf{u}_0)|$ adalah ukuran rata-rata tetangga lokal, $K$ adalah jumlah \emph{fold}, $T$ adalah jumlah iterasi pelatihan, dan $|\boldsymbol{\theta}|$ adalah jumlah parameter GNN. Kompleksitas waktu untuk setiap tahap adalah sebagai berikut.
    \begin{enumerate}[label=(\roman*)]
        \item \textbf{Pelatihan GNN} (Tahap 2): $\mathcal{O}(K \cdot T \cdot n \cdot m \cdot |\boldsymbol{\theta}|)$ --- kompleksitas linear terhadap ukuran data dan parameter (baik).
        \item \textbf{Konstruksi graf lokal} (Tahap 3): $\mathcal{O}(n \log n)$ dengan struktur data spasial seperti \emph{k-d tree} (baik), atau $\mathcal{O}(n^2)$ dengan pencarian naif (moderat).
        \item \textbf{Komputasi bobot} (Tahap 4): $\mathcal{O}(K \cdot m \cdot |\boldsymbol{\theta}|)$ --- linear terhadap jumlah tetangga (baik).
        \item \textbf{Solusi LWLS} (Tahap 5): $\mathcal{O}(m \cdot p^2 + p^3)$ untuk setiap lokasi target --- kubik terhadap $p$ namun linear terhadap $m$ (baik untuk $p$ kecil).
    \end{enumerate}
    
    Kompleksitas total untuk estimasi di $n_0$ lokasi target adalah
    \begin{equation}
        \mathcal{O}\bigl(K \cdot T \cdot n \cdot m \cdot |\boldsymbol{\theta}| + n_0 \cdot (m \cdot p^2 + p^3)\bigr).
    \end{equation}
\end{teorema}

\begin{proof}
    Untuk Tahap 2, setiap iterasi pelatihan memproses $n(1-1/K)$ observasi dengan $K$ model. Setiap observasi memerlukan \emph{forward pass} melalui GNN dengan kompleksitas $\mathcal{O}(m \cdot |\boldsymbol{\theta}|)$ untuk agregasi tetangga dan transformasi parameter.
    
    Untuk Tahap 3, pencarian tetangga dalam radius $h$ dapat dilakukan dalam $\mathcal{O}(\log n + m)$ per query menggunakan \emph{k-d tree}, atau $\mathcal{O}(n)$ dengan pencarian linear.
    
    Untuk Tahap 4, komputasi skor untuk setiap observasi memerlukan satu \emph{forward pass} GNN.
    
    Untuk Tahap 5, pembentukan matriks $\mathbf{H}_k$ memerlukan $\mathcal{O}(m \cdot p^2)$ operasi (produk luar $\mathbf{x}_i \mathbf{x}_i^\top$ untuk $m$ observasi), dan inversi matriks $p \times p$ memerlukan $\mathcal{O}(p^3)$ operasi.
\end{proof}

Kompleksitas GA-GWR bersifat baik untuk aplikasi praktis karena:
\begin{enumerate}[label=(\roman*)]
    \item Linear terhadap ukuran data $n$ (skalabel untuk dataset besar).
    \item Kubik terhadap dimensi $p$, namun $p$ biasanya kecil dalam aplikasi GWR ($p < 20$).
    \item Pelatihan GNN hanya dilakukan sekali, sedangkan estimasi di lokasi baru hanya memerlukan $\mathcal{O}(m \cdot p^2 + p^3)$.
\end{enumerate}
Sebagai perbandingan, GWR klasik memerlukan $\mathcal{O}(n \cdot (n \cdot p^2 + p^3))$ untuk seluruh estimasi (kuadratik terhadap $n$), sehingga GA-GWR lebih efisien untuk dataset besar.


\subsection{Diagnostik Model dan Validasi Inferensi}

Diagnostik model diperlukan untuk mengevaluasi kualitas estimasi dan memvalidasi prosedur inferensi yang telah diuraikan pada bagian sebelumnya. Subbagian ini menyajikan metrik diagnostik dalam kerangka definisi-teorema yang sesuai dengan struktur teoretis GA-GWR.

Metrik evaluasi prediksi digunakan untuk mengukur kemampuan model dalam memprediksi nilai respons.

\begin{definisi}[\textbf{\emph{Root Mean Squared Error} (RMSE)}]
    RMSE adalah akar kuadrat dari rata-rata kuadrat galat prediksi, didefinisikan sebagai
    \begin{equation}
        \mathrm{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^n \widehat{\varepsilon}_i^2} = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \mathbf{x}_i^\top \widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_i))^2}.
    \end{equation}
    RMSE memiliki satuan yang sama dengan variabel respons $y$. Nilai RMSE yang lebih rendah mengindikasikan prediksi yang lebih akurat.
\end{definisi}

\begin{definisi}[\textbf{\emph{Mean Absolute Error} (MAE)}]
    MAE adalah rata-rata nilai absolut galat prediksi, didefinisikan sebagai
    \begin{equation}
        \mathrm{MAE} = \frac{1}{n} \sum_{i=1}^n |\widehat{\varepsilon}_i| = \frac{1}{n} \sum_{i=1}^n |y_i - \mathbf{x}_i^\top \widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_i)|.
    \end{equation}
    MAE lebih robust terhadap \emph{outlier} dibandingkan RMSE karena tidak mengkuadratkan galat. Nilai MAE yang lebih rendah mengindikasikan prediksi yang lebih akurat.
\end{definisi}

\begin{definisi}[\textbf{Koefisien determinasi ($R^2$)}]
    Koefisien determinasi mengukur proporsi variansi dalam variabel respons yang dapat dijelaskan oleh model, didefinisikan sebagai
    \begin{equation}
        R^2 = 1 - \frac{\mathrm{SS}_{\mathrm{res}}}{\mathrm{SS}_{\mathrm{tot}}} = 1 - \frac{\sum_{i=1}^n (y_i - \mathbf{x}_i^\top \widehat{\boldsymbol{\beta}}_{\mathrm{CF}}(\mathbf{u}_i))^2}{\sum_{i=1}^n (y_i - \bar{y})^2},
    \end{equation}
    dengan $\bar{y} = n^{-1} \sum_{i=1}^n y_i$ adalah rata-rata sampel, $\mathrm{SS}_{\mathrm{res}}$ adalah jumlah kuadrat residual, dan $\mathrm{SS}_{\mathrm{tot}}$ adalah jumlah kuadrat total. Nilai $R^2 \in [0, 1]$, di mana $R^2 = 1$ menunjukkan prediksi sempurna dan $R^2 = 0$ menunjukkan model tidak lebih baik dari prediksi menggunakan rata-rata.
\end{definisi}

Diagnostik residual diperlukan untuk memvalidasi asumsi model, khususnya Asumsi~\ref{asumsi:gagwr_independensi} dan normalitas galat.

\begin{definisi}[\textbf{Uji Shapiro-Wilk}]
    Uji Shapiro-Wilk digunakan untuk menguji normalitas distribusi residual dengan hipotesis sebagai berikut.
    \begin{align*}
        H_0 &: \text{Residual berdistribusi normal, lawan}  \\
        H_1 &: \text{residual tidak berdistribusi normal}.
    \end{align*}
    Statistik uji Shapiro-Wilk didefinisikan sebagai
    \begin{equation}
        W = \frac{\left(\sum_{i=1}^n a_i \widehat{\varepsilon}_{(i)}\right)^2}{\sum_{i=1}^n (\widehat{\varepsilon}_i - \bar{\varepsilon})^2},
    \end{equation}
    dengan $\widehat{\varepsilon}_{(i)}$ adalah statistik urutan ke-$i$ dari residual (residual yang diurutkan dari terkecil ke terbesar), $\bar{\varepsilon} = n^{-1} \sum_{i=1}^n \widehat{\varepsilon}_i$ adalah rata-rata residual, dan $a_i$ adalah koefisien yang diturunkan dari momen statistik urutan distribusi normal standar.
\end{definisi}

Untuk tingkat signifikansi $\alpha$, hipotesis nol $H_0$ ditolak jika $W < W_{\alpha,n}$, dengan $W_{\alpha,n}$ adalah nilai kritis dari tabel distribusi Shapiro-Wilk untuk ukuran sampel $n$. Secara ekuivalen, jika $p$-value $< \alpha$, maka terdapat cukup bukti untuk menolak $H_0$.

\begin{definisi}[\textbf{Indeks Moran}]
    Indeks Moran digunakan untuk mendeteksi autokorelasi spasial pada residual dengan hipotesis sebagai berikut.
    \begin{align*}
        H_0 &: \text{Tidak terdapat autokorelasi spasial pada residual} \; (I = \mathbb{E}[I]), \; \text{lawan} \\
        H_1 &: \text{terdapat autokorelasi spasial pada residual} \; (I \neq \mathbb{E}[I]).
    \end{align*}
    Statistik Indeks Moran didefinisikan sebagai
    \begin{equation}
        I = \frac{n}{S_0} \cdot \frac{\sum_{i=1}^n \sum_{j=1}^n w^*_{ij} (\widehat{\varepsilon}_i - \bar{\varepsilon})(\widehat{\varepsilon}_j - \bar{\varepsilon})}{\sum_{i=1}^n (\widehat{\varepsilon}_i - \bar{\varepsilon})^2},
    \end{equation}
    dengan $w^*_{ij}$ adalah elemen matriks bobot spasial (biasanya biner: $w^*_{ij} = 1$ jika lokasi $i$ dan $j$ bertetangga, 0 sebaliknya), dan $S_0 = \sum_{i=1}^n \sum_{j=1}^n w^*_{ij}$ adalah total bobot.
\end{definisi}

Di bawah $H_0$, nilai harapan dan variansi Indeks Moran adalah
\begin{align}
    \mathbb{E}[I] &= -\frac{1}{n-1}, \\
    \mathrm{Var}(I) &= \frac{n^2 S_1 - n S_2 + 3 S_0^2}{S_0^2 (n^2 - 1)} - \mathbb{E}[I]^2,
\end{align}
dengan $S_1 = \frac{1}{2} \sum_{i \neq j} (w^*_{ij} + w^*_{ji})^2$ dan $S_2 = \sum_i (\sum_j w^*_{ij} + \sum_j w^*_{ji})^2$.

Statistik-$z$ untuk pengujian adalah
\begin{equation}
    z_I = \frac{I - \mathbb{E}[I]}{\sqrt{\mathrm{Var}(I)}},
\end{equation}
yang berdistribusi asimtotik $\mathcal{N}(0,1)$ di bawah $H_0$. Untuk uji dua sisi dengan tingkat signifikansi $\alpha$, $H_0$ ditolak jika $|z_I| > z_{\alpha/2}$. Secara ekuivalen, jika $p$-value $< \alpha$, maka terdapat cukup bukti untuk menolak $H_0$.

Untuk memvalidasi bahwa bobot \emph{kernel} yang dipelajari GNN memenuhi kondisi regularitas pada Asumsi~\ref{asumsi:gagwr_ukuran_sampel_lokal} dan \ref{asumsi:gagwr_keterbatasan_bobot}, dilakukan evaluasi berikut.

\begin{definisi}[\textbf{Entropi bobot \emph{kernel}}]
    Entropi distribusi bobot untuk lokasi target $\mathbf{u}_0$ didefinisikan sebagai
    \begin{equation}
        H(\mathbf{u}_0) = -\sum_{i \in \mathcal{N}_h(\mathbf{u}_0)} \widehat{\kappa}_{\boldsymbol{\theta}}(i \mid \mathbf{u}_0) \log \widehat{\kappa}_{\boldsymbol{\theta}}(i \mid \mathbf{u}_0).
    \end{equation}
    Entropi mengukur ketidakpastian atau keragaman distribusi bobot.
\end{definisi}

\noindent Entropi tinggi berarti bobot terdistribusi merata di antara banyak observasi sehingga baik untuk stabilitas, tetapi dapat menyebabkan \emph{oversmoothing}. Entropi rendah berarti bobot terkonsentrasi pada sedikit observasi sehingga dapat menyebabkan variansi tinggi dan fenomena \emph{vanishing gradient}.

\begin{definisi}[\textbf{\emph{Effective number of neighbors}}]
    Jumlah tetangga efektif untuk lokasi target $\mathbf{u}_0$ didefinisikan sebagai
    \begin{equation}
        n_{\mathrm{eff}}(\mathbf{u}_0) = \exp(H(\mathbf{u}_0)).
    \end{equation}
    Metrik ini mengukur berapa banyak observasi yang secara efektif berkontribusi pada estimasi lokal, dengan interpretasi sebagai berikut.
    \begin{enumerate}[label=(\roman*)]
        \item Jika bobot seragam, maka $n_{\mathrm{eff}} = |\mathcal{N}_h(\mathbf{u}_0)|$ (maksimum).
        \item Jika bobot terkonsentrasi pada satu observasi, maka $n_{\mathrm{eff}} \to 1$.
    \end{enumerate}
\end{definisi}

\begin{teorema}[\textbf{Kondisi kecukupan sampel lokal}]
    Untuk menjamin identifikasi parameter lokal (Asumsi~\ref{asumsi:gagwr_ukuran_sampel_lokal}), jumlah tetangga efektif harus memenuhi
    \begin{equation}
        n_{\mathrm{eff}}(\mathbf{u}_0) \geq p + 1,
    \end{equation}
    dengan $p$ adalah dimensi kovariat. Jika kondisi ini tidak terpenuhi, matriks $\mathbf{X}^\top \mathbf{W} \mathbf{X}$ dapat menjadi singular atau \emph{ill-conditioned}.
\end{teorema}

\begin{proof}
    Untuk solusi LWLS, matriks Hessian $\mathbf{X}^\top \mathbf{W} \mathbf{X}$ berukuran $p \times p$. Agar matriks ini memiliki \emph{full rank}, diperlukan minimal $p$ observasi dengan bobot tak nol yang kovariat-nya \emph{linearly independent}.
\end{proof}

Untuk memastikan validitas inferensi statistik, dilakukan validasi berikut.

\begin{definisi}[\textbf{Kondisi \emph{undersmoothing}}]
    Kondisi \emph{undersmoothing} terpenuhi jika \emph{bandwidth} $h$ memenuhi
    \begin{equation}
        \sqrt{nh^d} \cdot h^2 \to 0 \quad \text{saat } n \to \infty,
    \end{equation}
    dengan $d$ adalah dimensi ruang koordinat spasial. Kondisi ini menjamin bahwa bias asimtotik dapat diabaikan relatif terhadap galat baku, sehingga interval kepercayaan memiliki \emph{coverage} yang valid.
\end{definisi}

Dalam praktik, validitas inferensi dapat diverifikasi dengan cara sebagai berikut.
\begin{enumerate}[label=(\roman*)]
    \item Membandingkan distribusi empiris statistik-$t$ dengan distribusi $\mathcal{N}(0,1)$ menggunakan Q-Q plot.
    \item Mengevaluasi sensitivitas hasil terhadap variasi jumlah \emph{fold} $K$ dalam skema \emph{cross-fitting}.
    \item Memastikan \emph{coverage} empiris interval kepercayaan mendekati level nominal melalui simulasi atau \emph{bootstrap}.
\end{enumerate}